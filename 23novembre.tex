%chktex-file 36
%chktex-file 8
%chktex-file 24
%chktex-file 35
%chktex-file 9
%chktex-file 10
%chktex-file 17
\documentclass[computational_mathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{23rd of November 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\subsection{Gaussian elimination on symmetric matrices}
\recap{
In Gaussian elimination we had $A$ and we multiplied it by $L_1$ in order to get a big chunk of $0$s in the first column
\[
\begin{pmatrix}
    1 & \\
    \ast & 1 & \\
    \ast &  & 1  \\
    \ast &  & & 1  \\
    \ast &  &  & & 1 
\end{pmatrix}
\begin{pmatrix}
  \circled{\ast} & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
\end{pmatrix}
=
\begin{pmatrix}
  \circled{\ast} & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
\end{pmatrix},
\]}

Let us consider an upgrade of Gaussian elimination in the case of $A \in S(m, \R)$.

Let us see what happens if we multiply $L_1A$ on the right by the transpose of $L_1$:

\begin{description}
  \item[{\sc Step 1:}]
    \[
\begin{pmatrix}
    1\\
    \ast & 1\\
    \ast & & 1\\
    \ast  & & & 1\\
    \ast & & & & 1
\end{pmatrix}
\begin{pmatrix}
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
    \ast & \ast & \ast & \ast & \ast\\
\end{pmatrix}
\begin{pmatrix}
    1 & \ast & \ast & \ast & \ast\\
    & 1\\
    & & 1\\
    & & & 1\\
    & & & & 1
\end{pmatrix}
=
\begin{pmatrix}
    \ast & \Cr{0} & \Cr{0} & \Cr{0} & \Cr{0}\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
\end{pmatrix}
\]
  \item[{\sc Step 2:}]
\[
\begin{pmatrix}
    1 & \\
    & 1\\
    & \ast & 1\\
    & \ast & & 1\\
    & \ast & & & 1
\end{pmatrix}
\begin{pmatrix}
    \ast & \Cr{0} & \Cr{0} & \Cr{0} & \Cr{0}\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
    \Cr{0} & \ast & \ast & \ast & \ast\\
\end{pmatrix}
\begin{pmatrix}
    1\\
    & 1 & \ast & \ast & \ast\\
    & & 1\\
    & & & 1\\
    & & & &1\\
\end{pmatrix}
=
\begin{pmatrix}
	\ast & 0 & 0 & 0 & 0\\
	0 & \ast & \Cr{0} & \Cr{0} & \Cr{0}\\
	0 & \Cr{0} & \ast & \ast & \ast\\
	0 & \Cr{0} & \ast & \ast & \ast\\
	0 & \Cr{0} & \ast & \ast & \ast\\
\end{pmatrix}
\]

\item[{\sc Step $m$:}] 
  \[
L_{m-1}L_{m-2}\dots L_1 A L1^T \dots L_{m-2}^T L_{m-1}^T = D,
\]
where $D$ is diagonal, or
\[
A = L_1 L_2 \dots L_{m-1} D L_{m-1}^T \dots L_2^T L_1^T = LDL^T.
\]
\end{description}


\begin{obs}[Stroke of luck]
  Notice that the stroke of luck of \Cref{obs:21novstroke1} holds in this case too, hence we pay nothing to compute matrix $L$.
  
\begin{gather*}
\begin{bsmallmatrix}
    1 & \\
    -a_2 & 1 & \\
    -a_3 &  & 1  \\
    -a_4 &  & & 1  \\
    -a_5 &  &  & & 1 
\end{bsmallmatrix}^{-1} \cdot
\begin{bsmallmatrix}
    1 & \\
     & 1 & \\
     & -b_3 & 1  \\
     & -b_4 & & 1  \\
     & -b_5 &  & & 1 
\end{bsmallmatrix}^{-1} \cdot
\begin{bsmallmatrix}
    1 & \\
     & 1 & \\
     &  & 1  \\
     &  & -c_4& 1  \\
     &  & -c_5 & & 1 
\end{bsmallmatrix}^{-1} \cdot 
\begin{bsmallmatrix}
    1 & \\
     & 1 & \\
     &  & 1  \\
     &  & & 1  \\
     &  &  & -d_5& 1 
\end{bsmallmatrix}^{-1}
= \begin{bsmallmatrix}
    1 & \\
    a_2 & 1 & \\
    a_3 & b_3 & 1  \\
    a_4 & b_4 & c_4& 1  \\
    a_5 & b_5 & c_5 &d_5 & 1     
\end{bsmallmatrix}
\end{gather*}
\end{obs}

\begin{theorem}[Symmetric Gaussian elimination]
Let $A \in S(m, \R)$ such that during Gaussian elimination we don't encounter any $0$ pivot. $A$ admits a factorization $A=LDL^T$, where $L$ is lower triangular with ones on its diagonal, and $D$ is diagonal.
\end{theorem}

A Matlab implementation of symmetric Gaussian elimination is shown in \Cref{algo:23novsymgauss}.

\begin{center}
\begin{minipage}{.9\linewidth}
  \begin{algorithm}[H]
    \caption{Symmetric Gaussian factorization, Matlab implementation.}\label{algo:23novsymgauss}
    \begin{minted}[linenos=true]{matlab}
function [L, D] = ldl_factorization(A)
  m = size(A, 1);
  L = eye(m); D = zeros(m);
  for k = 1:m-1
    D(k, k) = A(k, k);
	  L(k+1:end, k) = A(k+1:end, k) / A(k, k);
	  A(k+1:end, k+1:end) = A(k+1:end, k+1:end) ...
        - L(k+1:end, k) * A(k, k+1:end);
  end
  D(m, m) = A(m, m);
    \end{minted}
  \end{algorithm}
\end{minipage}
\end{center}
\vspace{0.5cm}

It is possible to make an optimization of this algorithm: since $A$ is supposed to be symmetric, we only need to update the lower triangular part of $A$, since the rest is mirrored by symmetry, hence the computational complexity is half that of Gaussian elimination.

This algorithm is not backward stable, exactly like the one on non symmetric matrices. Pivoting may be performed in order to improve stability. It comes without saying that the row swap should be done consistently on the columns to preserve symmetry.

Of course there are some matrices (like the ones with all $0$s on the diagonal) that cannot be ``pivoted''. There are workarounds, though. As an example, Matlab's \texttt{[L, D, P] = ldl(A)} produces matrices such that $\tr{P}AP=LD\tr{L}$, where $D$ may have $2\times 2$ diagonal blocks.

\recap{
  We recall the characterization of \textbf{positive definite matrix} $A \in M(m, \R)$: all its eigenvalues are strictly positive. In other words, $A$ is positive definite if $\forall z\neq 0 \in {\R}^m ~ \tr{z}Az >0$.
}

\begin{lemma}\label{lemma:23nov1}
  In the context of positive definite matrices the following holds:
\begin{enumerate}
  \item Let $A$ be a symmetric matrix. $A$ is positive definite if and only if $MAM^T$ is so, for some invertible $M \in M(m, \R)$.
    Formally, $\forall A \in S(m, \R) \text{ s.t. } A{\succ} 0 \Leftrightarrow \exists M \in GL(m, \R) \text{ s.t. } MA\tr{M} {\succ} 0$;
	\item Let $A$ a symmetric positive definite matrix such that $A = \begin{bsmallmatrix}
	    A_{11} & A_{12}\\
	    A_{21} & A_{22}
  \end{bsmallmatrix}$, then $A_{11}$ and $A_{22}$ are, too.
    Formally, $\forall A \in S(m, \R) \text{ s.t. } A {\succ} 0 \text{ and } A = \begin{bsmallmatrix}
	    A_{11} & A_{12}\\
	    A_{21} & A_{22}
    \end{bsmallmatrix} \Rightarrow A_{11} {\succ} 0$ and $A_{22} {\succ} 0$.
\end{enumerate}
\end{lemma}


\begin{proof}~\\
   \begin{enumerate}
    \item~\\
      \begin{description}
        \item[$\Rightarrow)$]
          $A \in S(m, \R)$ and $A {\succ} 0  \Longrightarrow MA\tr{M} \in S(m, \R)$ and $MA\tr{M} {\succ} 0$.

          Take $z \in {\R}^m$, $z\neq 0$ $\tr{z} MA\tr{M} z = \tr{y} Ay > 0$, where we performed a variable change $y = \tr{M} z$. Notice that $y \neq 0$ because $M$ is invertible (and $ker(M) = \{ 0 \}$). The symmetry of the matrix $MA\tr{M}$ follows from $\tr{(MA\tr{M})} = \tr{\tr{M}} \tr{A} \tr{M} = MA\tr{M}$;
        \item[$\Leftarrow)$]
          $MA\tr{M} \in S(m, \R)$ and $MA\tr{M} {\succ} 0  \Longrightarrow A \in S(m, \R)$ and $A {\succ} 0$.
          
          This proof follows from the previous arrow, where the substitution is $z = \inv{M}y$.
            \end{description}
     \item $A = \begin{bsmallmatrix}
	    A_{11} & A_{12}\\
	    A_{21} & A_{22}
     \end{bsmallmatrix}$ positive definite $\Longrightarrow A_{11}$ and $A_{22}$ are positive definite too.

      Since $A$ is positive definite, its scalar product is greater than zero with all the vectors in $\R^m$. 
  \begin{description}
    \item[$A_{11})$]
      Let us take $\mathbf{z}=
        \begin{bsmallmatrix}
          \mathbf{z}_1\\0
        \end{bsmallmatrix}$.

        $\begin{bsmallmatrix}
          \tr{\mathbf{z}_1} &0\\
        \end{bsmallmatrix} 
        \cdot 
        \begin{bsmallmatrix}
	        A_{11} & A_{12}\\
	        A_{21} & A_{22}
        \end{bsmallmatrix}
        \cdot 
        \begin{bsmallmatrix}
          \mathbf{z}_1\\0
        \end{bsmallmatrix}
        = \tr{\mathbf{z}_1} A_{11} \mathbf{z_1} > 0$, $\forall \mathbf{z}_1 \in \R^{size of A_{11}}$
      \item[$A_{22})$]
      Let us take $\mathbf{z}=
      \begin{bsmallmatrix}
        0\\\mathbf{z}_2
      \end{bsmallmatrix}$.

      $\begin{bsmallmatrix}
        0& \tr{\mathbf{z}_2}\\
      \end{bsmallmatrix} 
      \cdot 
      \begin{bsmallmatrix}
	      A_{11} & A_{12}\\
	      A_{21} & A_{22}
      \end{bsmallmatrix}
      \cdot 
      \begin{bsmallmatrix}
        0\\\mathbf{z}_2
      \end{bsmallmatrix}
      = \tr{\mathbf{z}_2} A_{22} \mathbf{z_2} > 0$, $\forall \mathbf{z}_2 \in \R^{size of A_{22}}$
 
  \end{description}
   \end{enumerate}
\end{proof}

\begin{corollary}
  Let $A \in M(n, \R)$ such that $A$ is positive definite. When computing the $LDL^T$ factorization of $A$, at each step we have $D_{kk}>0$, hence we need no pivoting technique.
\end{corollary}

\begin{proof}
  From the first point of \Cref{lemma:23nov1} we have that, since $A {\succ} 0$, $L_1 A \tr{L_1}$ is positive definite. Thanks to the second point of the same lemma we have $L_1 A \tr{L_1} = 
\begin{pmatrix}
    \ast & 0 & 0 & 0 & 0\\
    0 & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
\end{pmatrix}$ and so the first and the second diagonal blocks are positive definite ($D_{11} > 0$ and $D_{22} {\succ} 0$).

  Notice that, since $A$ is positive definite $A_{11} {\succ} 0$, but it's a scalar, hence $A_{11} > 0$ and this implies no breakdown case.
\end{proof}

We may introduce another kind of factorization.

\subsection{Cholesky factorization}
The key idea is to write the diagonal matrix of the Gaussian elimination $D$ as product of $D^{1/2}$ times itself:

$D = \begin{pmatrix}
  d_{11}&&\\
  &d_{22} &\\
  && \ddots\\
  &&& d_{mm}\\
\end{pmatrix} = \begin{pmatrix}
  \sqrt{d_{11}}&&\\
  &\sqrt{d_{22}} &\\
  && \ddots\\
  &&& \sqrt{d_{mm}}\\
\end{pmatrix} \cdot
\begin{pmatrix}
  \sqrt{d_{11}}&&\\
  &\sqrt{d_{22}} &\\
  && \ddots\\
  &&& \sqrt{d_{mm}}\\
\end{pmatrix}
$

The LDL factorization may be rewritten as follows

\[
A = LDL^T = LD^{1/2} ({D^{1/2}}^T L^T) = C C^T,
\]
where $D^{1/2} = \operatorname{diag}(D_{11}^{1/2}, D_{22}^{1/2},\dots,D_{mm}^{1/2})$, and $C$ is lower triangular (but not anymore with ones on the diagonal).

In Matlab the Cholesky factorization of a positive definite matrix is performed by the function \texttt{chol(A);} and returns $\tr{C}$.

\begin{obs}
We will not discuss stability further, but Cholesky is always backward stable even without pivoting ($\norm{C}=\norm{A}^{1/2}$).
\end{obs}


\begin{obs}
In a sparse matrix, we can choose the (symmetric) permutation with the only goal of reducing fill-in. The same considerations about LU factorization hold in this case too.
\end{obs}

\subsection{Krylov subspace methods}

In this part we will discuss some different techniques to solved linear systems, inspired from optimization algorithms.

\begin{example}
  Let us consider the optimization problem $\min \frac{1}{2}x^T A x + b^T x $, where $A \succ 0$. We know that the solution to this problem is $x=A^{-1}b$. Assume we solve this problem via a gradient descent-type method, starting from $x_0=0$.
  \begin{description}
    \item[{\sc Step 1:}] $x_0 = 0$, $\nabla f(x_0) = -b$;
    \item[{\sc Step 2:}] $x_1 = $some multiple of $b$ $\in Span(b)$
      
      $f(x_1) = Ax_1 - b$

      $\nabla f(x_1) = $ some multpiple of $Ab$ + some multiple of $b$ $\in span(b)$;
    \item[{\sc Step 3:}] $x_2 = $ mult.~of $x_1$ + mul.~of $\nabla f(x_1)$ + mult.~of $x_0 + \cdots $ $\in span(Ab_1, b)$;
    \item[{\sc Step 4:}] $x_3 = mult. of x_2 + \cdots \nabla f(x_2) + $ previous iterates $\cdots$

      $Ax_2 - b = A \cdot (sAb + tb) - b= s A^2 b + tAb -b \in span(b, Ab, A^2b)$, where $s$ and $t$ are scalars;
    \item[{\sc Step 5:}] $x_4 \in span(b, Ab, A^2b, A^3b)$.
  \end{description}
  Notice that we can make some linear combinations of the vectors we have available and $A(A(sb + tAb) +uAb + vb) + e(sb + tAb) + fAb+gb \in span(b, Ab, A^2b, A^3b)$.
\end{example}

  \textbf{Idea:} first compute the basis of $span(b, Ab, A^2b, A^3b)$, then look for the best solution inside this subspace.

The following family of algorithms ``uses'' the matrix $A$ only by computing matrix-vector products.

\begin{obs}
  The cost of multiplying a sparse matrix $A$ with a vector $z$ is $O(\text{nnz}(A))$, where $\text{nnz}(A)$ is the number of non zero entries of $A$.
\end{obs}

\begin{proof}
  Let us assume the matrix $A$ is stored as a vector, whose entries are $(i, j, A_{ij})$.

  The matrix-vector product would then be
  \begin{enumerate}
    \item \texttt{x=zeros}
    \item \texttt{for (i, j, }$A_{ij}$ \texttt{) such that} $A_{ij} \neq 0$
    \item $x_i = x_i + A_{ij} * z_j$
    \item \texttt{end}
  \end{enumerate}
\end{proof}

  From now on we will consider to have a function \texttt{compute\_product\_with\_A} that we use to compute matrix-vector products with $A$. In particular, $x=$\texttt{compute\_product\_with\_A(z)} computes $x =Az$, given $z$. This function will be the only way in which the matrix $A$ appears in our algorithm. If $A$ is sparse, hence, these algorithms become particularly fast. Moreover, if we somehow have matrices that are not really sparse, but for which there exists a clever implementation of the matrix-vector product, this class of algorithms will give good results.

\begin{definition}[Krylof subspace]
  Let $A \in M(m, \R)$ and let $b \in \R^m$. The \textbf{Krilov subspace} of index $n$ is $\mathbf{K_n(A, b)} = span(b, Ab, A^2b, \ldots, A^{n-1}b)$.
  
  Equivalently, $k \in K_n(A, b) \iff ~ \exists \alpha_1, \ldots, \alpha_{n-1} \in \R^m$ s.t. $v= \alpha_0 b + \alpha_1 Ab + \alpha_2 A^2b + \cdots + \alpha_{n-1} A^{n-1}b$.
  
  Equivalently, $(\alpha_0 + \alpha_1A + \alpha_2A^2 + \cdots + \alpha_{n-1} A^{n-1}) b = p(A) b$ for a polinomial $p$ of degree such that $deg(p) \le n-1$.
\end{definition}

\begin{obs}[Properties]~\\
  \begin{enumerate}
    \item $v, w \in K_n(A, b) \Rightarrow \alpha + \beta W \in K_n(A, b)$;
    \item $v \in {K}_{n}(A, b) \Rightarrow Av \in {K}_{n}(A, b)$. Proof. Let us take $v = {\alpha}_{0} b + \cdots + {\alpha}_{n-1} b$, then $Av = A({\alpha}_{0} b + \cdots + {\alpha}_{n-1}b) = {\alpha}_0 Ab + \cdots + {\alpha}_{n-1} {A}^{n}b$;
    \item $\dim(K_n(A, b)) \le n$. It is exactly $n$ if $b, Ab, A^2 b, \ldots, A^{n-1} b$ are linearly independent.
    \item Let us assume $dim(K_n(A, b)) \le n$. In the second point, if $A^{n-1}$ was really necessary ${\alpha}_{n-1} \neq 0$ or $v \in {K}_{n}(A, b)$, $v \notin {K}_{n-1}(A, b)$, equivalently then ${A}^{n}b$ is really necessary to write $Av$, i.e. $Av \in {K}_{n+1}(A, b)$ but $Av not \in {K}_{n}(A, b)$.
    \item We may observe that $\dim({K}_{1} (A, b)) < \dim({K}_{2}(A, b)) < \cdots < \dim({K}_{n_{max}}(A, b)) = \dim({K}_{n_{max} + 1}(A, b)) = \cdots$.
  \end{enumerate}
  \end{obs}
\end{document}
