%chktex-file 36
%chktex-file 8
%chktex-file 24
\documentclass[ComputationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{16th of November 2018 --- A. Frangioni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
What happens when the Hessian isn't strictly positive definite? If there are some negative eigenvaues, I can desume that there are some directions in which the function goes to $-\infty$. The model has no minimum, unless we restrict to a \textbf{compact set}.

In particular, we may decide to restrict to a part of the space where we can trust our model, which is called \textbf{trust region}.

Finding such a region is a NP-hard problem, if we don't restrict to a ball.

\begin{definition}[Karush-Khun-Tucker conditions]
  Any optimal solution of the problem $x^{i+1}$ must satisfy that $\equiv$ $\exists \, \lambda \geq 0$ s.t.

  \begin{description}
    \item[{\sc Karush:}]$[H^i +\lambda I ] x^{i+1} = - \nabla f(x^i)$ [linear];
    \item[{\sc Kuhn:}] $H^i + \lambda I~\succeq~ 0$ [semidefinite];
    \item[{\sc Tucker:}] $\lambda (r - \norm{x^{i+1}}) = 0$ [nonlinear].
  \end{description}
Where the last condition means that $\lambda$ has to be $0$, unless the solution we get is exaclty on the border of the ball.
\end{definition}

What's the difference between this approach and what we used to do before? 
We have two different cases:
  \begin{itemize}
    \item $\norm{x^{i+1}} < r \Longrightarrow \lambda = 0 \Longrightarrow$ normal Newton step ($\mathcal{T}$ has no effect);
    \item $\lambda > 0 \text{(= small radius r)} \Longrightarrow$ like in line search with $\varepsilon^i = \lambda$.
  \end{itemize}

The problem is computing these systems of equations taking less than $O(n^3)$.

\begin{myframe}{\bf Key idea}
We don't need to compute the Hessian, we can use the first order information to infer things on the second order matrix, although we don't really need to compute the Hessian.
\end{myframe}

\subsection{Quasi-newton methods}
At a step we have:
$m^i(x) = \nabla f(x^i) (x - x^i) + \frac{1}{2} \tr{(x - x^i)} H^i (x - x^i)$, $x^{i+1} = x^i + \alpha^i d^i$

At the next step we recompute the gradient in $x^{i+1}$ and the matrix $H^{i+1}$:
 $m^{i+1}(x) =\nabla f(x^{i+1}) (x - x^{i+1}) + \frac{1}{2} \tr{(x - x^{i+1})} H^{i+1} (x - x^{i+1})$

How should $H^i$ be chosen? It should satisfy the following:
\begin{enumerate}
  \item positive definite ($H^{i+1} \succ 0$);
  \item we know the gradient in the previous point, we know the gradient in the current point, we construct $H^{i+1}$ such that the model works: $\nabla m^{i+1}(x^i) = \nabla f(x^i)$;
  \item $\norm{H^{i+1} - H^i}$ is ``small''.
\end{enumerate}

This new model isn't too different from the previous one, because of the third preerty. Or equivalently $H^{i+1} ( \, x^{i+1} - x^i \, ) = \nabla f(x^{i+1}) - \nabla f(x^i)$, which we call \textbf{secant equation} and we denote (S).

To ease notation we define $s^i$ such that:
$s^i = x^{i+1} - x^i = \alpha^i d^i$ and $y^i = \nabla f(x^{i+1}) - \nabla f(x^i)$.

$s^i$ is chosen, while $y^i$ is decided by the function.

In order to have a matrix $H^i$ that satifies the first two condition we could check that $H^{i+1} s^i = y^i$, because this implies $s^i y^i = \tr{(s^i)} H^{i+1} s^i$ and this implies $1$.~and $2$., hence we obtain the \textbf{curvature condition} $s^i y^i > 0$.

\begin{theorem}
  Wolf condition implies $s^i y^i >0$, using the notation we inroduced: (W) $\Longrightarrow$ (C).
\end{theorem}

\begin{proof}
  \[
    \varphi'(\alpha^i) = \nabla f(x^{i+1}) d^i \geq m_3 \varphi'(0) = m_3 \nabla f(x^i) d^i
  \]
  \[
    \Downarrow
  \]
  \[
    (\nabla f(x^{i+1}) - \nabla f(x^i)) d^i \geq (m_3 - 1) \varphi'(0) > 0
  \]
\end{proof}

\begin{obs}
  We may observe that this theorem implies that if we perform Armijo Wolf exact line search condition (C) can always be satisfied.
\end{obs}

\subsubsection{Davidson-Fletcher-Powell}
How can we choose a $H^i$ that satisfies the three conditions enumerated above? Taking $H^{i+1} =argmin \{\norm{H - H^i}~:~H \in$(S), $ ~ H \succeq 0\}$ is a good idea and for this minimum problem holds the following:

\begin{theorem}[Davidson-Feltcher-Powell]
  The new matrix is obtained at each step constructing a rank two matrix, obtained from $H^i$ as a rank to correction, as follows:
  $H^{i+1} = (I - \rho^i y^i \tr{(s^i)}) H^i(I - \rho^i s^i \tr{(y^i)}) + \rho^i y^i \tr{(y^i)}$
\end{theorem}

Let us denote $B^i = \inv{H^i}$.
At any step we need to compute $B^{i+1} = \inv{(H^{i+1})}$, because we need to solve the system. We have some fomulas that give us a way to compute $\inv{(H^{i+1})}$ from $\inv{H^i}$.

\begin{theorem}[Sherman-Morrison-Woodbury]\label{theo:16novsmw}
  The inverse of a matrix of the form $A + a \tr{b}$ has the following shape:
  $\inv{(A + a \tr{b})} = \frac{\inv{A} - \inv{A} a \tr{b} \inv{A}}{1 - \tr{b} \inv{A} a}$.
\end{theorem}

\begin{obs}
  From \Cref{theo:16novsmw} we can conclude that $B^{i+1} = \frac{B^i + \rho^i s^i \tr{(s^i)} - B^i y^i \tr{(y^i)}B^i}{\tr{(y^i)} B^i y^i}$.
\end{obs}
It's important to notice that this operation has a cost of $O(n^2)$.

We can do better, in terms of computational complexity.

\subsubsection{Broyden-Fletcher-Goldfarb-Shanno}

We can use directly $B^i$, the inverse of $H^i$.
Write (S) for $B^{i+1}$: $s^i = B^{i+1} y^i \Longrightarrow B^{i+1} =$ argmin $\{\norm{B - B^i}~:~\ldots~ \}$.

$B^{i+1} = B^i + \rho^i [(1 + \rho^i \tr{(y^i)} B^i y^i) s^i \tr{(s^i)} - (B^i y^i \tr{(s^i)} + s^i \tr{(y^i)} B^i)]$

This formula proves to be more stable than the other one.

This method takes $O(n^2)$.

The two $B^i$s, obtained from DFP and BFGS, are different although both sensible, but we can use a convex combination of the two.

\begin{obs}
  How can we choose $H^1$? The value we choose will make a differencein the results, at least for the first steps.
\end{obs}

Let us see a couple of choices for $B^1$:

\begin{itemize}
  \item Scalar multiples of identity, but how to choose the scalar?
  \item Compute the gradient in every direction and approximate $H$. This will cost $O(n^3)$, but it should be done only once.
\end{itemize}

Let us compute the space needed to store the $B^i$s: order of $n^2$ is still a lot. What happens if we restrict to working with information of the last $k$ operations?

\subsubsection{Poorman's approach}

At each step we only consider $B^{i-k}$ and $k$ rank one operations. This operations cost $n$ each, and we have $k$ lines. The problem is that $B^{i-k}$ takes $O(n^2)$ space. We can optimize if we choose $B^{i-k}$ to be simpler, say a multiple of the identity, or finite difference of the gradient. Then the space complexity is $O(kn)$.

I need to tune the algorithm to find the right $k$ which gives me enough precision and also keeps the computational cost low.


\begin{myframe}{\bf Final observation of quasi Newton methods}
We may notice that this variation of Newton method doesn't get trapped in local minima, as Newton method did. In the end, the fact that quasi Newton isn't that precise at the beginning may be a good feature.
\end{myframe}


\subsection{Conjugate gradient method}

\recap{In the gradint method, the angle between two consecutive directions is exactly $\ang{90}$, as can be seen in \Cref{fig:16nov1}.}

\addpic{0.5}{pics/16nov/1.png}{Geometric idea on how the new direction is chosen.}{fig:16nov1}

We would like to take into account not only the subspace spanned by $d^{i+1}$ but we would like to optimize over larger and larger subspaces (spanned by $d^i$ and $d^{i+1}$).

\begin{definition}[Q-conjugate]
Let $v$ and $w$ be vectors in $\R$. We say that $v$ ad $w$ are \textbf{$Q$-conjugate} if $\tr{(v)} Q w = 0$.
\end{definition}

We would like pick a direction to be $Q$-conjugate with all the previous iterations. The point is that we can't take into account all the previous directions, but we will see that we only need the previous direction to obtain all the information we need.

\algo{alg:CGQ1}{Pseudocode for conjugate gradient method for quadratic functions.}{
    \Procedure{\bf CGQ}{$Q, q, x, \varepsilon$}
      \State~$d^- \gets 0$;
      \While{($\norm{\nabla f(x)} > \varepsilon$)}
        \If{($d^- = 0$)}
          \State~$d \gets - \nabla f(x)$;
        \Else%
          \State~$\beta = (\nabla \tr{f(x)} Q d^-) / (\tr{(d^-)} Q d^-)$;
          \State~$d \gets - \nabla f(x) + \beta d^-$;
        \EndIf%
        \State~$\alpha \gets (\nabla \tr{f(x)} d) / (\tr{d} Q d)$;
        \State~$x \gets x + \alpha d$; 
        \State~$d^- \gets d$;
      \EndWhile%
    \EndProcedure%
}

The number of iterations needed to converge is proportional to the clusterization of the eigenvalues of the matrix $Q$.

The algorithm that was presented is for quadratic functions, but the same algorithm works for non quadratic function as well, as long as we change the fomula for $\beta$.

The pseudocode of \Cref{alg:16novCGA} is referred to Fletcher-Reeves definition of $\beta^i = \norm{\nabla f(x^i)} / \sqrnorm{\nabla f(x^{i-1})}$.

This algorithm converges in at most $n$ iterations.

\algo{alg:16novCGA}{Pseudocode for conjugate gradient method for arbitrary functions}{
    \Procedure{\bf CGA}{$Q, q, x, \varepsilon$}
      \State~$\nabla f^- = 0$; 
      \While{($\norm{\nabla f(x)} > \varepsilon$)}
        \If{($\nabla f^- = 0$)}
          \State~$d \gets - \nabla f(x)$;
        \Else%
          \State~$\beta = \sqrnorm{\nabla f(x^i)} / \sqrnorm{\nabla f^- }$;
          \State~$d \gets - \nabla f(x) + \beta d^-$;
        \EndIf%
        \State~$\alpha \gets$ AWLS($f(x + \alpha d)$);
        \State~$x \gets x + \alpha d$;
        \State~$d^- \gets d$;
        \State~$\nabla f^- \gets \nabla f(x)$;
      \EndWhile%
    \EndProcedure%
}

We have three different formulas for $\beta^i$, which coincide in the quadratic case.

\begin{enumerate}
  \item Polak-Ribi\`ere: $\beta^i = \frac{\nabla \tr{f(x^i)} (\nabla f(x^i) - \nabla f(x^{i-1}))}{\sqrnorm{\nabla f(x^{i-1})}}$  
  \item Hestenes-Stiefel: $\beta^i = \frac{\tr{\nabla f(x^i)}(\nabla f(x^i) - \nabla f(x^{i-1}))}{\tr{(\nabla f(x^i) - \nabla f(x^{i-1}))}d^{i-1}}$
    
  \item Dai-Yuan: $\beta^i = \frac{\sqrnorm{\nabla f(x^i)}}{\tr{(\nabla f(x^i) - \nabla f(x^{i-1}))} d^{i-1}}$
\end{enumerate}

Some of these algorithms require some hypothesis on the function in order for the conjugate method to converge.

\begin{enumerate}
  \item Fletcher-Reeves requires $m_1 < m_2 < \, \frac{1}{2}$ for (A) $\cap$ (W') to work;
  \item (A) $\cap$ (W') $\not\Longrightarrow$ $d^i$ of Polak-Ribière is of descent, unless $\beta^i_{PR} = \max \{\beta^i, 0\}$.
\end{enumerate}

Sometimes it's important to restart from scratches if the algorithm isn't converging, because many bad choices may lead to a bad result.

The idea of taking the gradient and modify it instead of multiplying by a factor, adding the previous direction.

It's possibile to design hybrids between quasi-Newton and conjugate method.
\end{document}
