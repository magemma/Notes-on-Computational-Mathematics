%chktex-file 36
%chktex-file 8
%chktex-file 24
%chktex-file 35
%chktex-file 44
\documentclass[ComputationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{29th of November 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

In this lecture we address the problem of designing methods that use Krylov spaces to solve linear systems.

We need alternative methods to Gaussian elimination because matrices are too large.

\subsubsection{Naive idea}
Find first a ``good'' search subspace, then look for best approximation of the solution of $Ax=b$ in that space.

Let us assume that our space is the image of a matrix $V$ $Im(V)$. $\forall~x \in Im(V)$ such $x$ may be written as $x=V^1 y_1 + V^2 y_2 + \cdots + V^n y_n$, where $V^i$ are the columns of the matrix $V$.

  The idea is too find a vector $y$ that satisfies $\min_{y \in \R} \norm{AVy - b}$, which is a least squares problem.

\subsubsection{Improvement}
A good search space is $Im(V) = K_n(A, b) = \operatorname{span}(b, Ab, A^2b, \ldots, A^{n-1}b)$, where $K_n(A, b)$ is the Krylov space.

  The issue here is that $V=\begin{pmatrix} b & Ab & \cdots & A^{n-1}b\end{pmatrix}$ is a bad basis, because it is ill conditioned.

  Working with that $V$ is problematic: its columns ``tend'' to be aligned with the dominant eigenvector and this means that $V$ is close to a rank $1$ matrix.
  
We need a better basis for $K_n(A,b)$, in particular an orthogonal basis.

\recap{An orthogonal basis is a basis in which each couple of vectors are orthogonal}

\subsection{Arnoldi algorithm}

The idea behind this algorithm is to build an orthogonal basis of $K_n(A, b)$ incrementally.

The algorithm at a generic step, takes an orthogonal basis for $K_{n}(A, b)$ and adds a vector to produce one of $K_{n+1}(A, b)$.

Let us assume that we start with $\{q_1, q_2, \ldots, q_{n}\}$, orthogonal basis of $K_n(A, b)$.

We also assume that $q_n = \alpha_0 b + \alpha_1 Ab + \cdots + \alpha_{n-1}A^{n-1}b = p(A)b$, where we impose $\alpha_{n-1} \neq 0$.

$p(A) = \alpha_0 I + \alpha_1 A + \cdots + \alpha_{n-1} A^{n-1}$ has degree exactly $1$.

\begin{description}
  \item[{\sc Step 1:}] $K_1(A, b) = span(b)$, $q_1 = \frac{q_1}{\norm{q_1}}$
  \item[{\sc Generic step:}]~\\
    \begin{enumerate}
      \item produce a vector $K_{n+1}(A, b) - K_n(A, b): w =Aq_n$
      \item $w \in K_{n+1}(A, b) = 0$ $w = q_1 \beta_1 + q_1 \beta_2 + \cdots + q_n \beta_n + q_{n+1} \beta_{n+1}$, where $q_1, q_2, \ldots, q_n, q_{n+1}$ is an orthogonal basis of $K_{n+1}(A, b)$. In this context $q_1, q_2, \ldots, q_n$ are know, while $q_{n+1}$ is still to be determined.

        Notice that $\forall i = 1, \ldots, n$ holds the following $\tr{q_i} w = \tr{q_i} q_1 \beta_1 + \cdots + \tr{q_i} q_n \beta_n + \tr{q_i} q_{n+1}\beta_{n+1} = \tr{q_i} q_i \beta_i = \beta_i$, because of the orthonormality of the basis.
        Now we can compute $q_{n+1} \beta_{n+1} = w - q_1\beta_1 - q_2 \beta_2 + \cdots + q_n \beta_n = z$.
      If we choose $\beta_{n+1} =\norm{z}$ we have that $q_{n+1} = \frac{z}{\norm{z}}$ and this produces a valid choice of $q_{n+1}$. We still need to prove that it has norm $1$ and it's orthogonal to all the other vectors in the basis.
  \end{enumerate}
\end{description}

An implementation of this algorithm is shown in \Cref{alg:29novarnoldi}.
\begin{center}
\begin{minipage}{.9\linewidth}
  \begin{algorithm}[H]
    \caption{Arnoldi algorithm Matlab implementation.}\label{alg:29novarnoldi}
    \begin{minted}[linenos=true]{matlab}
function Q = arnoldi(A, b, n)
Q = zeros(length(b), n); %will be filled in
H = zeros(n+1, m);
Q(:, 1) = b / norm(b);
for j = 1 : n
  w = A * Q(:, j);
  for i = 1:j
  % not what we showed earlier here, but stabler
    betai = Q(:, i)' * w;
    w = w - betai * Q(:, i);
    H(i, j) = betai;
  end
  nrm = norm(w);
  H(j+1, j) = nrm;
  Q(:, j+1) = w / nrm;
end
    \end{minted}
  \end{algorithm}
\end{minipage}
\end{center}
\vspace{0.5cm}

Notice that we presented an algorithm where $\beta_i = \tr{q_i} w$ for $i=1, \ldots, n$, then $w \gets w - \beta_1 q_1 - \beta_2 q_2 - \cdots - \beta_n q_n$.

In the implementation we compute $\beta_i = \tr{q_1} w$, $w \gets w - \beta_1 q_1$, $\beta_2 = \tr{q_2} w$, $w \gets w - \beta_2 q_2$. Why? It is more stable.

At step $j$ \[
A q_j = \beta_{1,j}q_1 + \beta_{2,j}q_2 + \cdots + \beta_{j,j}q_j + \beta_{j+1,j}q_{j+1} = Q \begin{bsmallmatrix}
    \beta_{1,j}\\
    \beta_{2,j}\\
    \vdots\\
    \beta_{j+1,j}\\
    0\\
    0\\
    \vdots\\
    0
\end{bsmallmatrix}.
\vspace{-0.5cm}
\]

$Q = \begin{bmatrix}
  \rule[-10mm]{0pt}{20mm} q_1 & q_2 & \cdots & q_n & q_{n+1}
\end{bmatrix}$

$AQ = A \begin{bmatrix}
    \rule[-10mm]{0pt}{20mm} q_1 & q_2 & \cdots & q_n & q_{n}
\end{bmatrix} = Q \begin{bsmallmatrix}
  \beta_{1, 1} & \beta_{1, 2} & \beta_{1, 3} & \cdots & \beta_{1,n}\\
  \beta_{2, 1} & \beta_{2, 2} & \beta_{2, 3} & \cdots & \beta_{2,n}\\
    0 & \beta_{3,2} & \beta{3,3} & \cdots & \beta_{3,n}\\
    0 & 0 & \beta_{4,3} & \cdots & \beta_{4,n}\\
    \vdots & & \ddots & \ddots & \vdots\\
    0 & 0 & \cdots & 0 & \beta_{n+1, n}
\end{bsmallmatrix}$ 

 Hence the values $q_i$, $\beta_{i,j}$ computed by Arnoldi satisfy $AQ_n = Q_{n+1} H$, where $H \in M(n+1, n, \R)$ and looks like a triangular matrix plus a diagonal below, namely $H=
\begin{bsmallmatrix}
  \beta_{1, 1} & \beta_{1, 2} & \beta_{1, 3} & \cdots & \beta_{1,n}\\
  \beta_{2, 1} & \beta_{2, 2} & \beta_{2, 3} & \cdots & \beta_{2,n}\\
  0 & \beta_{3,2} & \beta{3,3} & \cdots & \beta_{3,n}\\
  0 & 0 & \beta_{4,3} & \cdots & \beta_{4,n}\\
  \vdots & & \ddots & \ddots & \vdots\\
  0 & 0 & \cdots & 0 & \beta_{n+1, n}
\end{bsmallmatrix}$ 

 Notation: $Q_{n+1} = \begin{bmatrix}
  \rule[-10mm]{0pt}{20mm} q_1 & q_2 & \cdots & q_n & q_{n+1}
 \end{bmatrix}$, $Q_{n} = \begin{bmatrix}
  \rule[-10mm]{0pt}{20mm} q_1 & q_2 & \cdots & q_n
\end{bmatrix}$

such that $Q_{n} \in M(m, n, \R)$, while $Q_{n+1} \in M(m, n+1, \R)$, $A \in M(m, \R)$ and $b \in M(m, n)$.

\addpic{0.5}{pics/29nov/1.png}{We started form a matirx $A$ which has many entries and it gets factorized by the product of two smaller matrices}{fig:29nov1}

For every matrix $A$ there exist and Arnoldi factorization.

We don't like that we multiply $A$ by two different matrices on the left and on the right, but we may improve the algorithm by using the same matrix.

$AQ_n  = (Q_n | q_{n+1}) \cdot matriToDraw = Q_n H + q_{n+1} \cdot (0, \ldots, 0, \ast) = Q_n H + q_{n+1} \cdot tr{e_{n+1}} \beta_{j+1, j}$, where $e_i = 0 \ldots 0 1 0 \ldots 0$.

Last remark: $AQ_n = Q_{n+1}H$ doesn't allow a factorization of the matrix $A$, because $Q_n$ isn't invertible, because $Q_n$ is tall, thin and it doesn't have an inverse.

Once we see how Arnoldi works, we would like to see how to use it.

At some point this assumption must become false. For instance, assume we arrive at step $m=n$: $q_1,q_2,\ldots,q_m$ are a basis of $\mathbb{R}^m$, so
\[
Aq_m = \beta_1 q_1 + \cdots + \beta_m q_m + 0
\]
(without an additional term $\beta_{m+1} q_{m+1}$.)

Arnoldi factorization for $m=n$ (assuming nothing broke down before):

$AQ_m = Q_m H$ is a factorization into square matrices, formally $Q_m, H, A, \in M(m, \R)$, so we can write something that we couldn't write before:

$A = A_m H \tr{Q_m}$ we recall that $H$ is upper triangular plus another diagonal before.

\begin{definition}
  Let $H \in M(m, \R)$ such that $H_{ij} = 0 \forall i > j+1$. $H$ is called \textbf{Hessemberg matrix}.
\end{definition}

\begin{proposition}
  The QR factorization an Hessemberg matrix $H \in M(m, \R)$ can be computed in $O(m^2)$ operations.
\end{proposition}


This approach may be used, but in practice, since $A$ is large and sparse we do not want to go until the end.

Let us analyze what happens if at some point $K_{n+1}(A, b) = K_{n}(A, b)$?
\[
Aq_n = \beta_1 q_1 + \cdots + \beta_n q_n + 0
\]

for instance, if $b$ is an eigenvector of $A$, it happens already at $n=1$.


In the implementation \Cref{alg:29novarnoldi} if we have a breakdown at step $n+1$ this means that $w = A_q n$ has already enough $q_i$s, so $q_{n+1} \beta_{n+1} = 0$.

Problem: $q_{n+1} = \frac{z}{\norm{z}}$ division by $0$. We need to change the definition $\beta_{n+1} = \norm{z} = 0$. At this point we don't get a basis of the Krylov space, but we can still go on as ``nothing happened'', as long as these vectors are orthonormal. We go on until the end we get

\[
A Q_m = H_m Q_m,
\]
\[
H_m = \left[
\begin{array}{cccc|ccccc}
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
0 & \ddots & \ast & \ast  & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\hline
&&& 0 & \ast & \cdots & \ast & \ast\\
&&&& \ast & \cdots & \ast & \ast\\
&&&& 0 & \ddots & \ast & \ast \\
&&&& 0 & 0 & \ast & \ast\\
\end{array}
\right].
\]

The blocks are square.

This is good news, because it allows us to make a lot of manipulations.
\[
A = Q_m H_m Q_m^T = 
\begin{bmatrix}
\rule[-4mm]{0pt}{10mm}  Q_n & \widehat{Q}
\end{bmatrix}
\begin{bmatrix}
    H_n & L\\
    0 & M
\end{bmatrix}
\begin{bmatrix}
\rule[-4mm]{0pt}{10mm}    Q_n & \widehat{Q}
\end{bmatrix}^T
\]

\recap{
We said two things about block triangular matrices:
\begin{enumerate}
  \item The eigenvalues of the matrix are the eigenvalues of the diagonal blocks, more formally, $eigh(A) = eigh(H) = eigh(H_n) \cup eigh(M)$
  \item We can solve linear systems on block matrices more easily. In this case,
 \begin{equation}
  \begin{aligned}
    x &= \inv{A}b = Q_m \inv{H} \tr{Q_m}b =  Q_m \inv{H} \begin{bmatrix} \tr{q_1}\\
    \tr{q_2}\\
    \vdots\\
    \tr{q_m}\\
    \end{bmatrix}b \numeq{(1)} Q_m \inv{H} \begin{bmatrix} \norm{b}\\
    0\\
    \vdots\\
    0\\
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
      \rule[-4mm]{0pt}{10mm}  Q_n & \widehat{Q}
    \end{bmatrix}
    \begin{bmatrix}
      \inv{H_n} & -\inv{H_n}L\inv{M}\\
      0 & \inv{M}
    \end{bmatrix}
    \begin{bmatrix}
      \norm{b} \mathbf{e}_1\\
      0
    \end{bmatrix}\\
    &=
    \begin{bmatrix}
\rule[-4mm]{0pt}{10mm}  Q_n & \widehat{Q}
\end{bmatrix}
\begin{bmatrix}
    \norm{b}H_n^{-1}\mathbf{e}_1\\
    0
\end{bmatrix}
    =
    Q_n \norm{b} H_n^{-1} \mathbf{e}_1.
  \end{aligned}
  \end{equation}
    where $\numeq{(1)}$ follows from the fact that $b = \norm{b} q_1$ and $q_1$ is orthogonal to all the other $q_i$.
\end{enumerate}
}

An attentive reader may notice that at step $j$, when encountering $B_{j+1, j} = 0$, we know the following:
\begin{itemize}
  \item some eigenvalues of $A$ (those of $H_n$)
  \item Given an eigencouple $v, \lambda$ such that $H_n v = \lambda v$ then $Q_n v$ is an eigenvector of $A$ with eigenvalue $\lambda$
  \item the solution $x = Q_n\inv{H_n}\norm{b}e_1$ of $Ax=b$ and this is called lucky breakdown.
\end{itemize}

The point is that we have what we need to compute the solution at last step before the breakdown.

\begin{theorem}

\textbf{`Lucky breakdown'}: if it happens at an early step, we can solve linear systems (or compute some eigenvalues) cheaply: costs $n$ matrix-vector products + $O(mn^2)$.
\end{theorem}

What happens when there is no breakdown? After $n$ steps of Arnoldi,
\begin{enumerate}
  \item if $H_n v = \lambda v$ is an eigenpair of $H_n$. $Q_n v$, $\lambda$ is an approximation of an eigenpair of $A$.
  \item $\widetilde{x}=Q_n \inv{H_n} \norm{b} e_1$ is an approximantion of the solution $x$ of $Ax=b$
\end{enumerate}
\end{document}
