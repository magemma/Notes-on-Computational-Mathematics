%chktex-file 36
%chktex-file 8
%chktex-file 24
\documentclass[computational_mathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{9th of November 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\subsection{Conditioning}
Two lectures ago we introduced the QR factorization to solve least squares problems and we noticed that it has a computational complexity which is much worse thant the normal equations method.

Why did we introduce the QR factorization to solve the least squares problem, then? Altough it's more complex computationally speaking, it's much better than the normal equation for what concerns accuracy. Let's see why through an example:

 \begin{example}\label{ex:9nov1}
   Let $A \in \mathcal{M}(4, 3, \R)$ s.t.
   \[
  A = \begin{pmatrix}
    1&1&2\\
    1&2&3\\
    3&1&4\\
    1&2& 3 + 10^{-8}\\
  \end{pmatrix}
    \]

    In this case the normal equations method doesn't even find the order of magnitude correctly.
 \end{example}

    At this point we may indroduce the problem of \textbf{sensitivity}:

\begin{definition}[Sensitivity]
  We call \textbf{sensitivity} the measure of how much the output of a problem changes when we perturb its input.

  As an example, let $f(x, y) = x + 2y$. If we perturb the second parameter of $f$ as follows $\widetilde{y} = y + \delta$, we can compute the variation in the output value of the function \textbf{v} as
  
  \[
    \mathbf{v} = f(x, \widetilde{y}) - f(x, y) =  x + 2(y + \delta) - (x + 2y) = 2\delta.
  \]

\end{definition}

A good example of this behaviour is the temperature of water coming from the shower: in particular, when we rotate little the knob the water becomes too cold or too hot very fast. This function is plotted in \Cref{fig:9nov1}.
   \addpic{0.5}{pics/9nov/1.png}{Geometric idea of temperature of the water in the shower}{fig:9nov1}

\begin{definition}[Absolute condition measure]
  The \textbf{absolute condition number} of a function $f$ is the \textbf{maximum} possible output change / input change ratio in the limit for a \textbf{small} change of the input.
\[
\kappa_{abs}(f,x) = \lim_{\varepsilon\to 0} \sup_{\abs{\tilde{x}-x} \leq \varepsilon} \frac{\abs{f(\tilde{x})-f(x)}}{\abs{\tilde{x}-x}}.
\]
\end{definition}

We would like to focus on what this definition means.

Why are we interested in the limit of a very small change?

If we zoom-in a continuous function is gets basically linear (key idea of derivative) and then the ratio between the difference on the outputs and the one of the inputs is approximatively the derivative, as shown in \Cref{fig:9nov2}.

\addpic{0.8}{pics/9nov/2.png}{Geometric idea behind the derivative, as ``zoom'' of the function in a certain point.}{fig:9nov2}

We take a point $x$ and we cosider a ball of radius $\varepsilon$ and we compute the change in the output over the change in the input, then we take the maximum.


\addpic{0.9}{pics/9nov/3.png}{Geometric idea behind absolute condition number.}{fig:9nov3}

\begin{example}
  Let $f(x) = x^2$.
  
  If we perturb the input $x$ to $x+\delta$, then $f(\widetilde{x}) = {(x+\delta)}^{2} = x^2 + 2x\delta + \delta^2$, then we obtain the ratio
  \[
    r = \frac{\abs{f(\widetilde{x}) - f(x)}}{\abs{\widetilde{x}-x}}
    = \frac{\abs{2x\delta + \delta^2}}{\abs{\delta}} = \abs{2x+\delta}
  \]

  If we denote $\varepsilon$ the radius of the ball, we obtain the following

  \[
    \max\limits_{\substack{\abs{\delta} < \abs{\varepsilon}\\ \widetilde{x} \in B(x, \varepsilon)}} r = \abs{2x} + \abs{\varepsilon}
  \]

  then 

  \[
    \lim\limits_{\varepsilon \rightarrow 0} \max\limits_{\abs{\delta} < \abs{\epsilon}} r = \lim\limits_{\varepsilon \rightarrow 0} ~ \abs{2x} + \abs{\varepsilon} = \abs{2x}
  \]
\end{example}

\begin{example}
It's more interesting to see a multivariate function:

Let $f(x) = \tr{x} Q x$, for instance for $x \in \mathbb{R}^2$ so that we can plot its graph in $\mathbb{R}^3$.

\addpic{0.5}{pics/9nov/4.png}{Paraboloid}{fig:9nov4}

We shall take a general example where the cross-section are ellipses, so that there is a direction of faster and slower ascent; this is not just a circular ``cup'' seen in perspective. Note that these directions of faster ascent and lower ascent correspond to the eigenvectors of the matrix $Q$.

In this case the absolute condition number is $\lim\limits_{\varepsilon \rightarrow 0}\max\limits_{\widetilde{x} \in B(x, \varepsilon)} \frac{\norm{f(\widetilde{x}) - f(x)}}{\norm{\widetilde{x} - x}}$,
and one can see that the output/input ratio varies with the direction in which $\tilde{x}$ is, so we have to take a maximum in the whole ball $B(x,\varepsilon)$.

  \addpic{0.6}{pics/9nov/5.png}{Level curves of a quadratic function (``seen from above'').}{fig:9nov4}
\end{example}

At this point, an observation is mandatory: \textbf{any} absolute measure doesn't take into account the values of the function in other points, so we want to define the following
\begin{definition}[Relative error]
	The \textbf{relative error} of an approximation $\widetilde{x}$ to a quantity $x$ is $\frac{\norm{\widetilde{x} - x}}{\norm{x}}$.
\end{definition}

Here are some examples of good and bad accuracy:
\begin{itemize}
  \item $\frac{\abs{\tilde{x}-x}}{\abs{x}} \approx 1$: \textbf{very bad} accuracy; it's just a number with the same order of magnitude.
  \item $\frac{\abs{\tilde{x}-x}}{\abs{x}} \approx 10^{-3}$: about 3 correct significant digits.
  \item $\frac{\abs{\tilde{x}-x}}{\abs{x}} \approx 10^{-16}$: about 16 correct digits; we \textbf{can't do better} typically (with \texttt{double} precision arithmetic).
\end{itemize}


\begin{definition}[Relative condition number]
  The \textbf{relative condition number} of a function $f$ is defined as
\[
\kappa_{rel}(f,\x) = \lim_{\delta\to 0} \sup_{\frac{\norm{\tilde{\x}-\x}}{\norm{\x}} \leq \delta} \frac{\frac{\norm{f(\tilde{\x})-f(\x)}}{\norm{f(\x)}} } {\frac{\norm{\tilde{\x}-\x}}{\norm{\x}}} = \kappa_{abs}(f,\x)\frac{\norm{\x}}{\norm{f(\x)}},
\]
i.e., we replace the absolute error $\norm{\tilde{\x}-\x}$ with the relative error.
\end{definition}


\subsubsection{Conditioning of linear systems}
At this point we would like to compute the condition number of solving a linear system, i.e., the condition number of the function $f(A,b)=A^{-1}b$, perturbing the inputs $A$ and $b$, one at a time.

\begin{description}
  \item[{\sc Perturbing} $b$] We want to compute the limit of the relative error $\frac{\norm{f(A, \widetilde{b}) - f(A, b)}}{\norm{f(A, b)}}$, so we set $x=A^{-1}b$ and $\tilde{x}=A^{-1}\tilde{b}$, and we estimate $output ~ error = \frac{\norm{\widetilde{x}-x}}{\norm{x}} =$?
    \begin{enumerate}
        \item \begin{equation}
          \begin{aligned}
            \norm{\widetilde{x} - x} &= \norm{\inv{A} \widetilde{b} - \inv{A} b}\\
            &= \norm{\inv{A} (\widetilde{b} - b)}\\
            &\le \norm{\inv{A}} \cdot \norm{\widetilde{b} - b}\\
          \end{aligned}
          \end{equation}
        \item Since $\norm{b} = \norm{Ax} \le \norm{A} \cdot \norm{x}$ we have
          $\frac{\norm{\widetilde{x} - x}}{\norm{x}} \le \norm{\inv{A}} \cdot \norm{A} \cdot \frac{\norm{\widetilde{b} - b}}{\norm{b}}$
    \end{enumerate}
    In the end, since $input ~ error = \frac{\norm{\widetilde{b} - b}}{\norm{b}}$ we obtain 
    \[
      \kappa_{rel} (f, x) = \lim\limits_{\varepsilon \rightarrow 0} \frac{output ~ error}{input ~ error} \le \lim\limits_{\varepsilon \rightarrow 0} \norm{\inv{A}} \cdot \norm{A} = \norm{\inv{A}} \cdot \norm{A}
    \]
    
    We denote $\kappa(A) = \norm{\inv{A}} \cdot \norm{A}$ the \textbf{condition number of $A$};

  \item[{\sc Perturbing} $A$]~%
    Given $Ax=b$ we obtain $(A+\Delta_A) \cdot (x + \Delta_x)= b$, where $\widetilde{A} = A + \Delta_A$ and $\widetilde{x} = x + \Delta_x$. Then we can expand as follows

    $\cancel{Ax} + \Delta_A \cdot x + A \cdot \Delta_x + \Delta_A \cdot \Delta_x = \cancel{b}$

We can stop taking into account $\Delta_A \cdot \Delta_x$, since it's a sort of second order term ($ \Delta_A \cdot \Delta_x = o (\norm{\Delta_A} \cdot \norm{\Delta_x})$), so we get the following
  \[
      \Delta_A \cdot x + A \cdot \Delta_x = 0
  \]
  \[
      \Delta_x = -\inv{A} \cdot \Delta_A \cdot x
    \]

then $\norm{\Delta_x} \le \norm{\inv{A}} \cdot \norm{\Delta_A} \cdot \norm{x}$, which implies $\frac{\norm{\Delta_x}}{\norm{x}} \le \norm{\inv{A}} \cdot \frac{\norm{\Delta_A}}{\norm{A}}$.

We obtain that $relative ~ output ~ error \le \kappa(A) \cdot relative ~ input ~ error$.
\end{description}
We only proved an inequality, but it turns out that it is tight: for every $A$ and $b$ there is a possible choice of the perturbation $\tilde{x}$ that attains equality.

In the end, in both cases, the error in the output is the error in the input (namely $b$ or $A$) times the condition number.

\subsection{Condition number, SVD, and distance to singularity}

\begin{proposition}
$\kappa(A) = \frac{\sigma_1}{\sigma_n}$, i.e., $\kappa(A)$ is the ratio between the smallest and the largest singular value.
\end{proposition}

So we can say that if a matrix is close to a singular matrix, then its condition number is going to be large.

\begin{proof}
Let $A = US\tr{V}$, then $\norm{A} = \norm{U \cdot S \cdot \tr{V}} = \norm{S} = \sigma_1$, since 

$S =  \begin{pmatrix}
    \sigma_1 &&&&&\\
    & \sigma_2&&&&\\
    && \cdot&&&\\
    &&& \cdot&&\\
    &&&& \cdot&\\
    &&&&& \sigma_n\\
  \end{pmatrix}$ and $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n$. 

  It's also true that $\norm{\inv{A}} = \norm{\inv{(USV)}} = \norm{V\inv{S} \tr{U}} = \norm{\inv{S}} = \sigma_n$, since 

$S =  \begin{pmatrix}
  \frac{1}{\sigma_1} &&&&&\\
    & \frac{1}{\sigma_2}&&&&\\
    && \cdot&&&\\
    &&& \cdot&&\\
    &&&& \cdot&\\
    &&&&& \frac{1}{\sigma_n}\\
  \end{pmatrix}$ and $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n$. 

In the end $\kappa(A) = \frac{\norm{A}}{\norm{\inv{A}}} = \frac{\sigma_1}{\sigma_n}$
\end{proof}

\begin{proposition}
The relative distance between $A$ and the closest singular matrix is $\frac{1}{\kappa(A)}$.
\end{proposition}
\begin{proof}

\recap{\textbf{Eckart-Young theorem:} the closest matrix to $A$ that has rank $\le n-1$ is 

\[
  \hat{A} = U \begin{pmatrix}
    \sigma_1 &&&&&\\
    & \sigma_2&&&&\\
    && \cdot&&&\\
    &&& \cdot&&\\
    &&&& \cdot&\\
    &&&&& \sigma_n\\
    &&&&&& 0\\
  \end{pmatrix} \cdot \tr{V}
\]}

\begin{equation}
  \begin{aligned}
bau
  \end{aligned}
\end{equation}

\begin{equation}
  \begin{aligned}
    \norm{A - \hat{A}} &= \norm{U \cdot 
    \Bigl(
    \begin{pmatrix}
      \sigma_1 &&&&&\\
      & \sigma_2&&&&\\
      && \cdot&&&\\
      &&& \cdot&&\\
      &&&& \cdot&\\
      &&&&& \sigma_n\\
    \end{pmatrix} -
    \begin{pmatrix}
      \sigma_1 &&&&&\\
      & \sigma_2&&&&\\
      && \cdot&&&\\
      &&& \cdot&&\\
      &&&& \cdot&\\
      &&&&& \sigma_{n-1}\\
      &&&&&& 0\\
    \end{pmatrix} \Bigr) \cdot \tr{V}}\\
    &= \norm{U \cdot 
    \begin{pmatrix}
      0 &&&&&\\
      & \cdot &&&&\\
      && \cdot&&&\\
      &&& \cdot&&\\
      &&&& 0&\\
      &&&&& \sigma_n\\
    \end{pmatrix} \cdot \tr{V}}\\
    &= \sigma_n\\
  \end{aligned}
\end{equation}

Thus, $\norm{A - \hat{A}} = \sigma_n$. We know already that $\norm{A}=\sigma_1$, so we just need to take the ratio.
\end{proof}


We analyzed the conditioning of linear systems, but the main problem we want to study in this course is least squares problem.

\subsection{Conditioning of least squares problem}

We need two quantities to be able to measure the conditioning of least squares problem:

\begin{description}
  \item[\ding{78} $\kappa(A)$]
Let $A \in \mathcal{M}(m, n, \R)$, with $m > n$ (tall, thin $A$). We \emph{define} $\kappa(A) = \frac{\sigma_1}{\sigma_n}$. We take this as a definition of $\kappa(A)$. Note that we cannot use the other definition $\kappa(A)=\norm{A}\norm{\inv{A}}$, since $\inv{A}$ does not exist for a non-square $A$. However, one can verify that $\norm{A} \cdot \norm{A^{+}} = \frac{\sigma_1}{\sigma_n}=\kappa(A)$, where $A^+$ is the pseudoinverse.

\begin{obs}
  Note that $\frac{1}{\kappa(A)}$ is the relative distance to the closest $\hat{A}$ without full column rank.
\end{obs}

    \addpic{0.8}{pics/9nov/6.png}{The triangle in the figure (the one whose cathets are $Ax$ and $b-Ax$) is a square triangle.}{fig:9nov6}
  \item[\ding{78} $\theta$]
    The second quantity needed is the angle between $Ax$ and $b$, see \Cref{fig:9nov6}.
$\theta = \arccos \frac{\norm{Ax}}{\norm{b}}$
\end{description}

Now we can express the theorem:

\begin{theorem}[Trefethen, Bau, Theorem 18.1]
Consider the linear least squares problem $\min \norm{Ax-b}$, with $A\in\mathbb{R}^{m\times n}$ with full column rank. Its relative condition number with respect to the input $b$ is
\[
\kappa_{rel, b \to x} \leq \frac{\kappa(A)}{\cos\theta},
\]
and with respect to $A$ it is
\[
  \kappa_{rel, A \to x} \leq \kappa(A) + {\kappa(A)}^2 \tan \theta,
\]
where $\theta$ is the angle such that $\cos \theta = \frac{\norm{Ax}}{\norm{b}}$.
\end{theorem}

At this point we have two condition numbers and they both depend on $\kappa(A)$ and $\theta$.

\begin{obs}~%
  \begin{description}
    \item[{\sc Special case 1: $\theta \approx 90^\circ$}]~%
      We can see from the figure that a big change of $b$ induces a small perturbation of $Ax$. No matter what the conditioning of $A$ is, a small (relative) perturbation in $b$ can change a large (relative) perturbation in $x$ and $Ax$, see \Cref{fig:9nov7}.
     \addpic{0.5}{pics/9nov/7.png}{Special case 1}{fig:9nov7}
    \item[{\sc Special case 2: $\theta \approx 0^\circ$}]~%
      When $b$ is almost in plane with $Im(x)$. In this case $cond \approx \kappa(A)$, see \Cref{fig:9nov8}.
      \addpic{0.5}{pics/9nov/8.png}{Special case 2.}{fig:9nov8}
    \item[{\sc General case: $\theta$ far from $0^\circ$ and $90^\circ$}]
      In the more general case, $cond \approx \kappa(A)^2$.
  \end{description}
\end{obs}
\end{document}
