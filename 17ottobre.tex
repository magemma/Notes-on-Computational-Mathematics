%chktex-file 36
%chktex-file 23
%chktex-file 10
%chktex-file 17
%chktex-file 9
\documentclass[computational_mathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{17th of October 2018 --- A. Frangioni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\subsection{Optimization algorithms}

\recap{We are interested in finding the minimum of a function through an iterative procedure, such that we start form an initial guess $x_0$ and go on $x^i \rightsquigarrow x^{i+1}$. We want to move towards the otimum.}

How to be sure we are in an optimum?
\begin{itemize}
  \item (strong) $\{x^i\} \to x_*$: the whole sequence converges to an optimal solution;
  \item (weaker) all accumulation points of $\{x^i\}$ are optimal solutions;
  \item (weakest) at least one accumulation point of $\{x^i\}$ is optimal.
\end{itemize}

Such iterative process, can be held in two different forms:
\begin{description}
  \item[{\sc line search}:] first choose $d^i \in \R^n$ (direction), then choose $\alpha^i \in \R$ (that we term stepsize or equivalently ``learning rate'') s.t. $x^{i+1} \gets x^i + \alpha^i d^i$;
  \item[{\sc trust region}:] first choose $\alpha^i$ (trust radius), then choose $d^i$.
\end{description}

In both these alternatives, it is crucial to choose a proper model to approximate function $f$.

\subsubsection{Gradient method for quadratic functions}
The simplest model we can build is a linear one, namely $L^i(x) = L_{x^i}(x) = f(x^i) + \nabla f(x^i) (x - x^i)$ and find the direction according to the model.

We should not move too far from $x^i$, so we want $\alpha_i \to 0$ and then $d_i = argmin \{\lim\limits_{t \to 0} \frac{f(x + td)}{t}\} = - \nabla f(x^i)$, aka the steepest descent direction.

Notice that a too short step is bad either, because the gain in the value of the function is very little.

At each step we want $\alpha^i \in \mbox{argmin} \{f(x^i + \alpha d^i)~:~\alpha \geq 0\}$.

Linear functions are unbounded below, so we would like to use a different family of functions. The easiest family of functions which are still more complex than linear ones are quadratic functions.

\[
  f(x) = \frac{1}{2} x^T Q x + q x
\]
where $Q \succeq 0$ otherwise $f$ is unbounded below.

The minimum here is the point in which the gradient ($\nabla f(x) = Qx + q$) is $0$.

\algo{alg:17ottquad}{Pseudocode for quadratic functions local minimum detection.}{
    \Procedure{\bf SDQ}{$f, x, \varepsilon$}
      \While{($\norm{\nabla f(x)} > \varepsilon$)}
        \State~$d \gets - \nabla f(x)$;
        \State~$\alpha \gets \frac{\sqrnorm{d}}{(\tr{d} Q d)}$;
        \State~$x \gets x + \alpha d$;
      \EndWhile%
    \EndProcedure%
}

For the time being we do not go into detail of how to chose the $\eps$ such that we can stop when the norm of the gradient is smallert than such a constant.

Let us see how to obtain the formula for $\alpha$.

We are interested in computing the minimum of $\{ f(x^i + \alpha d^i) : \alpha \ge 0\}$.

Let us do come algebra to describe better such an $f$:

\begin{equation}
  \begin{split}
    f(x^i + \alpha d^i) &= \frac{1}{2} \tr{(x^i + \alpha d^i)} Q (x^i + \alpha d^i) + q (x^i + \alpha d^i)\\
    &= \cancel{\frac{1}{2} \tr{(x^i)}Q x^i}  + \tr{(x^i)} Q (\alpha d^i) + \frac{1}{2} \tr{(d^i)} Q d^i + \cancel{q x^i} + \alpha (q d^i)\\
    &= [\frac{1}{2} \tr{(d^i)} Q d^i] \alpha^2  + \alpha [(x^i Q + q) d^i]\\
    &\numeq{(1)} [\frac{1}{2} \tr{(d^i)} Q d^i] \alpha  - \norm{d^i}\\
  \end{split}
\end{equation}

What if $\tr{(d^i)} Q d^i = 0$? If $Q$ is strictly positive definite this cannot happen, so the algorithm never breaks down.

Can we prove that the sequence of iterates is moving towards the optimum?

For this proof let us assume that $\eps = 0$, hence the procedure will neve stop. We want to prove that the sequence $\{x^i\}$ is (or contains) a minimizing sequence.

First of all we can state that the sequence is monotone, so it has a limit for sure.

What we can prove is that the the point where the sequence is converging is a stationary point.
$\lim\limits_{i \to \infty} \ps{\nabla f(x^i)}{\nabla f(x^{i+1})} = 0 \numeq{(*)} \ps{\nabla f(x)}{\nabla f(x)}$ and this holds because of the fundamental relationship $\ps{\nabla f(x^i)}{\nabla f(x^{i+1})} = 0$.

Notice that $\numeq{(*)}$ follows from the fact that the gradient is continuous.

How fast is the convergence?

In general, showing how fast $\| \, x^i - x_* \, \|$ decreases is more involved that showing how fast $f(x^i) - f_*$ decreases, but we do not know $f_*$.

We concentrate on computing $\lim\limits_{i \to \infty} \frac{f(x^{i+1}) - f_*}{{f(x^i) - f_*}^p}=R$.

According to the values of $p$ and $R$ we get the following alternatives:
\begin{description}
  \item[{\sc Sublinear:}] $p = 1, R = 1$;
  \item[{\sc Linear:}] $p = 1, R < 1$;
  \item[{\sc Superlinear:}] $p = 1, R = 0$; 
  \item[{\sc Quadratic:}] $p = 2, R > 0$.
\end{description}

Since the optimum is in $x^* = -\inv{Q} q$, we get that $f(x^*) = \frac{1}{2} \tr{q} \inv{Q} Q \inv{Q} q - \tr{q} \inv{Q} q = \frac{1}{2}  \tr{q} \inv{Q} q - \tr{q} \inv{Q} q = -\frac{1}{2} \tr{q} \inv{Q} q$.

Let us use a nifty trick: let us define 
\begin{equation}
  \begin{split}
    \bar{f}(x) &= \frac{1}{2} \tr{(x-x_*)} Q (x-x_*)\\
    &= \frac{1}{2} \tr{x}Qx + \frac{1}{2} \tr{x_*}Qx_* - \tr{x} (Qx)\\
    &\numeq{(2)} \frac{1}{2} \tr{x} Q x + \frac{1}{2} \tr{\inv{Q} q} Q (\inv{Q} q) + qx\\
    &= \frac{1}{2} \tr{x} Q x + \frac{1}{2} \tr{q} \cancel{\inv{Q}} \cancel{Q} \inv{Q} q + qx\\
    &= \frac{1}{2} \tr{x} Q x + \frac{1}{2} \tr{q} \inv{Q} q + qx\\
    &= f(x) - f_*
  \end{split}
\end{equation}


\end{document}
