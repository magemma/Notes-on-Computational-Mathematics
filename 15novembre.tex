%chktex-file 36
%chktex-file 8
%chktex-file 24
%chktex-file 26
\documentclass[computational_mathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{15th of November 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\subsection{Stability of algorithms}

In this lecture we will try to answer the questio: ``Is our algorithm (using floating point) going to compute a good approximation of the answer?''

It is related to sensitivity / conditioning but different. Depends on how we perform the computation.

\recap{
Computers work with IEEE arithmetic and the basic idea is the following.

TL;DR:~floating point numbers are numbers in base-2 scientific (exponential) notation.

\texttt{double} (64-bit numbers):
\[
	\pm 1.\underbrace{01001011101\dots 101}_{\text{52 binary digits}} \cdot 2^{\pm} \underbrace{{}^{101\dots01}}_{\mathclap{\text{10 binary digits}}}.
\]

We use $1$ bit for the sign, $52$ bits for the ``mantissa'' and $11$ bits for the exponent and its sign.

Some of these combinations of bits are reserved for special numbers, e.g. \texttt{Inf} and \texttt{NaN}, \texttt{-0}.
}

This system is subject to approximation errors, exactly like the ``usual'' decimal arithmetic: for example, if we do $\frac{1}{3} = 0.33333 \ldots$ and if we do $\frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 0.99999 \ldots \neq 1$. 

Whenever we store a number on a computer we need to approximate it, using the ``representable numbers, as shown in \Cref{fig:15nov1}.

\addpic{0.5}{pics/15nov/1.png}{We have $2^{52}$ equispaced numbers between $\frac{1}{2}$ and $1$ and between $1$ and $2$, and also $2^{52}$ between $2$ and $4$ and so on and so forth, so we have the same number of integers, although the space is enlarging.}{fig:15nov1}

Not all numbers are exactly representable, take $0.1$ (decimal). It's a periodic number when written in binary, hence we can't represent it exactly as a machine number.

\begin{definition}[Error bound]
For each $x\in \pm [10^{-308}, 10^{308}]$, there is an exactly representable number $\tilde{x}$ such that $\frac{\abs{\tilde{x}-x}}{\abs{x}} \leq \mathsf{u}$, with $\mathsf{u} = 2^{-52} \approx 2\cdot 10^{-16}$.
\end{definition}

Let us assume that we have the best possible algorithm that returns $\tilde{y} = f(\tilde{x})$ which is the best representation of $f(\tilde{x})$, then

\begin{align*}
	\frac{\abs{\tilde{y}-y}}{\abs{y}} &\leq \kappa_{rel}(f,x) \frac{\abs{\tilde{x}-x}}{\abs{x}} +o(\frac{\abs{\tilde{x}-x}}{\abs{x}}) \\
	&\leq \kappa_{rel}(f,x) \mathsf{u} + o(\mathsf{u}).
\end{align*}

In practice we may ignore $o(u)$, since it's small o of $2^{-16}$.

When we are approximating a result, we are approximating it up to a relative error of order of magnitude $10^{-16}$.

\[
a \oplus b = (a+b)(1+\delta), \quad \abs{\delta} \leq \mathsf{u}.
\]

\[
\Updownarrow
\]

\[
  \frac{\abs{(a \oplus b) - (a + b)}}{\abs{a+b}} = \delta
\]

We would like to compute the error on the function $f(a, b) = \tr{a} b$.

\[
  a =\begin{pmatrix}
    a_1\\a_2\\a_3\\
  \end{pmatrix},
  b =\begin{pmatrix}
    b_1\\b_2\\b_3\\
  \end{pmatrix} \text{, so} f(a, b) = a_1 \cdot b_1 + a_2 \cdot b_2 + a_3 \cdot b_3
\]

Denoting $\odot$ the approximated product and $\oplus$ the approximated sum on a computer, we can write the following:

\begin{equation}
  \begin{aligned}
    \text{computer result} &= a_1 \odot b_1 \oplus a_2 \odot b_2 \oplus a_3 \odot b_3\\
    &= \Biggl [ \Bigl [ a_1 b_1 \cdot (1+\delta) + a_2 b_2 \cdot (1+\delta_2) \Bigr ] \cdot (1+\delta_3) + a_3 b_3 \cdot (1+\delta_4) \Biggr] \cdot (1+\delta_5)\\
    &= a_1 b_1 \cdot (1+\delta_1) \cdot (1+ \delta_3) \cdot (1+\delta_5) + a_2 b_2 \cdot (1+\delta_2) \cdot (1+\delta_3) \cdot (1+\delta_5)\\
    &+ a_3 b_3 \cdot (1+\delta_4) \cdot (1+\delta_5)\\
    &= a_1 b_1 (1 + \delta_1 + \delta_3 + \delta_5 + O(u^2))\\
    &+ a_2 b_2 \cdot (1+ \delta_1 + \delta_3 + \delta_5 + O(u^2))\\
    &+ a_3 b_3 \cdot (1 + \delta_4 + \delta_5 + O(u^2))\\
    &\approx a_1 b_1 (1 + \delta_1 + \delta_3 + \delta_5) + a_2 b_2 \cdot (1+ \delta_1 + \delta_3 + \delta_5) + a_3 b_3 \cdot (1 + \delta_4 + \delta_5)
  \end{aligned}
\end{equation}

Where $O(u^2)$ comes from the summation of $\delta_i \delta_j$, for some $i$, $j$ and allows us to do an approximation up to second order terms of precision.

The absolute error then is 

\begin{equation}
  \begin{aligned}
    \text{Err}_a &= \abs{a_1 b_1 (\cancel{1} + \delta_1 + \delta_3 + \delta_5) + a_2 b_2 \cdot (\cancel{1} + \delta_1 + \delta_3 + \delta_5) + a_3 b_3 \cdot (\cancel{1} + \delta_4 + \delta_5) + \cancel{a_1 b_1} + \cancel{a_2 b_2} + \cancel{a_3 b_3}}\\
    &\numineq{(1)}{\le} \abs{a_1b_1} \cdot 3u + \abs{a_2 b_2} \cdot 3u + \abs{a_3 b_3} \cdot 2u\\
    &\le (\abs{a_1 b_1} + \abs{a_2 b_2} + \abs{a_3 b_3}) \cdot 3u
  \end{aligned}
\end{equation}

Where $\numineq{(1)}{\le}$ follows from the observation that $\abs{\delta_i} \le u$.

The result that we can obtain is weaker than we would have expected: \emph{if $a_i b_i \ge 0, \forall i=1,2,3$}, then 
\[
  \frac{\abs{\text{computer result} - (a_1 b_1 + a_2 b_2 + a_3 b_3)}}{a_1b_1 + a_2 b_2 + a_3 b_3} \le 3u,
\]
which means that the algorithm is stable.

However, if $a_1b_1$, $a_2b_2$ and $a_3 b_3$ have different signs, then we can bound the error not with $a_1 b_1 + a_2 b_2 + a_3 b_3$, but only with $\abs{a_1 b_1} + \abs{a_2 b_2} + \abs{a_3 b_3}$. This might be a lot larger than what we want to compute.

\begin{example}
  Take $\varepsilon = 10^{-16}$. Compute $\begin{pmatrix} 1 & -1 & 0\end{pmatrix} \cdot \begin{pmatrix} 1+ \varepsilon\\ 1\\1 \end{pmatrix} = (1+ \varepsilon) -1 + 0 = \varepsilon$.
    
    In this case we have a subtraction between two very close numbers.

    $    \abs{computer~result} \le u \cdot ( i + \abs{1 \cdot (1 + \varepsilon)} + \abs{-1 \cdot 1} + \abs{0 \cdot 1}) = (2 + \varepsilon) \cdot u$, which means that we have $10$ correct digits.

    The problem is in the fact that we have a very small result ($\varepsilon = O(10^{-6})$) and a very small error (of the same order of the result), which implies a large relative error.
\end{example}

An attentive reader might have noticed that it's very long to make this computation. Therefore, since we computed it on easy examples, we have to observe that we can't afford it for SVD or other complicated methods.

Wilkinson trick (from the $60$'s) comes to help us, to simplify this computation for more complicated problems.

\textbf{Idea:} see the computer result as the exact output of an algorithm run on a slightly perturbed input.

\begin{equation}
\begin{aligned}
&\tilde{y} = \dots\\
&= ((a_1b_1 (1+\delta_1) + a_2b_2(1+\delta_2))(1+\delta_4) + a_3b_3(1+\delta_3))(1+\delta_5)\\
&= a_1 \widetilde{b_1} + a_2 \widetilde{b_2} + a_3 \widetilde{b_3}
\end{aligned}
\end{equation}

where

\begin{equation}
\begin{aligned}
\widetilde{b_1} &= b_1(1+\delta_1)(1+\delta_4)(1+\delta_5) = b_1 + 3ub_1 + o(u),\\
\widetilde{b_2} &= b_2(1+\delta_2)(1+\delta_4)(1+\delta_5) = b_2 + 3ub_2 + o(u),\\
\widetilde{b_3} &= b_3(1+\delta_2)(1+\delta_5) = b_3 + 2u b_3 + o(u),\\
  \widetilde{a_i} = a_i \quad i=1,2,3.
\end{aligned}
\end{equation}

And the relative error is $\frac{\norm{\widetilde{b}-b}}{\norm{b}} \leq 3u + o(u)$.

Hence,
\[
  \frac{\norm{\tilde{y}-y}}{\norm{y}} \leq \kappa_{rel, b} \frac{\norm{\widetilde{b}-b}}{\norm{b}} \le k_{rel, b} \cdot 3u
\]

This isn't bad, because it's within a factor $3$ of the optimal error for a perfect algorithm.

Computing the conditioning is much easier than making all the calculations of $\delta$s.

\begin{definition}[Backward stability of an algorithm]
  An algorithm to compute $\mathbf{y}=f(\x)$ is called \textbf{backward stable} if the computed output $\tilde{\mathbf{y}}$ can be written as $\tilde{\mathbf{y}} = f(\tilde{\x})$, where $\tilde{\x} = \x + O(\mathsf{u} \norm{\x})$ (exact function, perturbed input).
\end{definition}

\begin{obs}
In real-life usage, this $O()$ notation often hides polynomial factors in the dimension $n$. Although this may look an illicit simplification, we observe that these factors are much more harmless than the error that we could make otherwise.
\end{obs}

\begin{theorem}
Backward stable algorithms are as accurate as theoretically possible (given the condition number of a problem), up to some factor that depends only on the dimension (e.g., $n$, $2n^2+18n$, \dots).
\end{theorem}

\begin{proof}
\[
\frac{\norm{\tilde{\mathbf{y}} - \mathbf{y}}}{\norm{\mathbf{y}}} \leq \kappa_{rel}(f,\x)\frac{\norm{\tilde{\x}-\x}}{\norm{\x}} = \kappa_{rel}(f,\x) O(\mathsf{u}),
\]
while the best attainable accuracy is $\kappa_{rel}(f,\x)\mathsf{u}$.
\end{proof}


We may ask ourselves if it's possible to perturb the input in order to get $\widetilde{y}$ for every possible algorithm and the answer is no. Let us see a counterexample, where $f(a, b) = \tr{a} b$ (vector-vector product in the order that produces a matrix).

We observe that in general it's not true that the computed approximation of $f(a, b)$ has rank 1.

\[
\begin{bmatrix}
    x_1\\ x_2\\ x_3
\end{bmatrix} \cdot
\begin{bmatrix}
    y_1 & y_2 & y_3
\end{bmatrix}
=
\begin{bmatrix}
    x_1 y_1 & x_1 y_2 & x_1 y_3\\
    x_2 y_1 & x_2 y_2 & x_2 y_3\\
    x_3 y_1 & x_3 y_2 & x_3 y_3
\end{bmatrix}
=
M
\]
While

\[
\begin{bmatrix}
    x_1\\ x_2\\ x_3
\end{bmatrix} \odot
\begin{bmatrix}
    y_1 & y_2 & y_3
\end{bmatrix}
=
\begin{bmatrix}
    x_1 \odot y_1 & x_1  \odot y_2 & x_1  \odot y_3\\
    x_2  \odot y_1 & x_2  \odot y_2 & x_2  \odot y_3\\
    x_3  \odot y_1 & x_3  \odot y_2 & x_3  \odot y_3
\end{bmatrix}
=
\widetilde{M}
\]

And we are looking for $\widetilde{x}$ and $\widetilde{y}$ such that $\widetilde{M} = \tr{\widetilde{x}} \cdot \widetilde{y}$
\begin{example}
Example (with exaggerated errors):

\[
\begin{bmatrix}
    1\\2\\3
\end{bmatrix} \cdot
\begin{bmatrix}
    4&5&6
\end{bmatrix}
=
\begin{bmatrix}
    4 & 5 & 6\\
    8 & 10 & 12\\
    12 & 15 & 18
\end{bmatrix}
\]
While
  \[
\begin{bmatrix}
    1\\2\\3
\end{bmatrix} \odot
\begin{bmatrix}
    4&5&6
\end{bmatrix}
=
\begin{bmatrix}
    4.01 & 4.99 & 6.01\\
    7.99 & 10.01 & 12.02\\
    11.98 & 15.02 & 17.97
\end{bmatrix}
\]
and this second matrix doesn't have rank $1$.
\end{example}

\subsection{Backward stability of QR factorization}

\recap{A generic step of the computation of the QR factorization of a matrix has the following shape:
\[
\begin{pmatrix}
  &&\\
  && \text{\Huge I} &&\\
    &&&& {\text{\huge H {\large \bf{u}}}}_k\\
    &&&&\\
\end{pmatrix}
\begin{pmatrix}
    \ast & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & 0 & \ast & \ast & \ast\\
    0 & 0 & \ast & \ast & \ast\\
    0 & 0 & \ast & \ast & \ast\\
\end{pmatrix}
=
\begin{pmatrix}
    \ast & \ast & \ast & \ast & \ast\\
    0 & \ast & \ast & \ast & \ast\\
    0 & 0 & \ast & \ast & \ast\\
    0 & 0 & 0 & \ast & \ast\\
    0 & 0 & 0 & \ast & \ast\\
\end{pmatrix}
\]
}

Each step of the QR factorization in backward stable.

Let us compute the backward stability of QR factorization:

With some tedious computations like the ones above, one can show that \emph{one} step of the QR factorization is backward stable, i.e., ${\widetilde{R}}_{k-1} = R_{k-1} + \Delta_{R_{k-1}}$, where $\norm{\Delta_{R_{k-1}}} \le O(u) \norm{R_{k-1}}$ and this allows us to write the following

\[
  \frac{\norm{{\widetilde{R}}_{k-1} - R_{k-1}}}{\norm{R_{k-1}}} \le O(u)
\]

At a generic step $k$ we have that

\[
  R_k = Q_k \cdot R_{k-1}
\]

\[
  \Updownarrow
\]

\[
  \widetilde{R_k} = Q_k (R_{k-1} + \Delta_{R_{k-1}})
\]

The idea is doing this procedure one step after the other, starting from step $1$, where $R_0 = A$.

\[
  \widetilde{R_1} = Q_1 \widetilde{R_0} = Q_1 \cdot (R_0 + \Delta_{R_0}) = Q_1 \cdot (A + \Delta_{R_0})
\]

\[
  \widetilde{R_2} = Q_2 \widetilde{R_1} = Q_2 (R_1 + \Delta_{R_1}) = Q_2 Q_1 (A + \Delta_{R_0}) + Q_2 \Delta_{R_1} = Q_2 Q_1 (A + \Delta_{R_0} + \tr{Q_1} \Delta_{R_1})
\]

We go on combining errors like this and we see that all the quantities $(A + \Delta_{R_j})$ are perturbations of the original matrix $A$.

In the end we may observe that the final computed $R_n$ is the exact result obtained from $A$ plus $n$ perturbations.

\begin{obs}
  We should notice that the norm of each perturbation is small with respect to the norm of $A$.
  
  In formal terms, $\norm{\Delta_{R_0}} \le u \cdot \norm{R_0} = u \cdot \norm{A}$ and $\norm{\tr{Q_1} \Delta R_1} \numeq{(1)} \norm{\Delta R_1} \le u \cdot \norm{R_1} \numeq{(2)} u \cdot \norm{A}$.

  Where $\numeq{(1)}$ and $\numeq{(2)}$ hold because $Q_i$ are orthogonal. 
\end{obs}

\begin{myframe}{{\large \ding{74}} \textbf{Mantra}}
Orthogonal transformations are the key for stability. 
\end{myframe}

\subsubsection{Stability of algorithms for least-squares problems}
Let us see how various algorithms to solve LS problems (implemented in Matlab) behave in relation to backward stability.

\textbf{Least squares problem via QR}
\begin{description}
  \item[{\sc Step 1:}] Computing a thin QR (\texttt{qr(A, 0);}) $\rightarrow$ backward stable;
  \item[{\sc Step 2:}] (\texttt{Q1'*b;})$\rightarrow$ backward stable;
  \item[{\sc Step 3:}] (\texttt{R1 \ c;})$\rightarrow$ backward stable;
\end{description}

\todo{Scrivere meglio}

\textbf{Least squares problem via SVD}
\begin{description}
  \item[{\sc Step 1:}] Computing a SVD (\texttt{svd(A, 0);}) $\rightarrow$ backward stable;
  \item[{\sc Step 2:}] (\texttt{U'*b;})$\rightarrow$ backward stable;
  \item[{\sc Step 3:}] (\texttt{c ./ diag(S);})$\rightarrow$ backward stable;
  \item[{\sc Step 4:}] (\texttt{V*d;})$\rightarrow$ backward stable; 
\end{description}

\textbf{Least squares problem via Normal Equations}
\begin{description}
  \item[{\sc Step 1:}] \texttt{C = A' * A;}
  \item[{\sc Step 2:}] \texttt{d = A' * b;}
  \item[{\sc Step 3:}] \texttt{x = C \ d;}
\end{description}

We can also prove that some algorithms aren't backward stable, like normal equations. The issue here is that the same input is needed in more than one operation, so it should satisfy more than one equation and this may lead to a solution set which is empty. Also, in the last equation we solve a linear system with matrix $C$, and this gives an error of the order of ${\kappa(A)}^2$, never of $\kappa(A)$: $\kappa(C) = \kappa(\tr{A} A) = {\kappa(A)}^2$.

Let us take the example of two lectures ago: \Cref{ex:1}.

\begin{example}
   Let $A \in \mathcal{M}(4, 3, \R)$ s.t.
   \[
  A = \begin{pmatrix}
    1&1&2\\
    1&2&3\\
    3&1&4\\
    1&2& 3 + 10^{-8}\\
  \end{pmatrix}
    \]

    We may observe that $A$ is at distance $10^{-8}$ from a matrix without full column rank, hence $\kappa \approx 10^8$.  

  What is the condition number of solving this least squares problem?
  It's about $10^8$, since in that problem we generated $b= A \cdot \begin{pmatrix} 3 & 4 & 5 \end{pmatrix}$, so $b$ lies in the image of $A$ ($Im(A)$). The geometric idea in \Cref{fig:15nov2}.

    \addpic{0.7}{pics/15nov/2.png}{$b$ lies (at least almost, because of numerical error) on the plane of $Im(A)$. In this case $\kappa_{rel}(\text{solving LS}) \approx \kappa(A)$.}{fig:15nov2}
\end{example}

A brief recap may be found in \Cref{tab:15nov1}.

\begin{table}
\begin{center}
\begin{tabular}{cccc}
\toprule
  & \textbf{Normal eqns} & \textbf{QR} & \textbf{SVD}\\
	\midrule
$m \approx n$	& $\frac{4}{3}n^3$ & $\frac{4}{3}n^3$ & $\approx 13n^3$\\
\midrule
$m \gg n$ & $mn^2$ & $2mn^2$ & $2mn^2$\\
\midrule
& \parbox{3cm}{\raggedright~Unstable when $cond\approx~\kappa(A)$} & \parbox{2cm}{\raggedright~Backward stable} & \parbox{3cm}{\raggedright~Backward stable; reveals info on sensitivity, allows regularization}\\
\bottomrule
\end{tabular}
\end{center}\caption{Brief recap of the complexities of the algorithms we studied for solving the least squares problem. The last row takes into account the stability.}\label{tab:15nov1}
\end{table}

\subsubsection{A posteriori checks}

Let's assume that Matlab gives us a solution of a problem. We want to be able to check how good this result is.

\begin{example}
Suppose we have solved a linear system\dots

  \begin{minted}{matlab}
>> A = randn(4, 4); b = randn(4, 1);
>> x = A \ b;
>> A*x - b
ans =
            0
  -1.3878e-17
            0
   2.2204e-16
\end{minted}
\end{example}

\begin{definition}[Residual]
  Let $A\in \mathcal{M}(m, {\R})$ and $\b\in\mathbb{R}^m$, and $x$ be the solution of $Ax=b$. For a given $\widetilde{x}$ we define \textbf{residual} the following $\mathbf{r}=A\widetilde{x}-b$. 
\end{definition}

assume that $\norm{r}$ is small; does this mean that $\widetilde{x}$ is close to the exact solution $x$?

\begin{theorem}
Let $A\in\mathbb{R}^{m\times m}, \b\in\mathbb{R}^m$, and $x$ be the solution of $Ax=b$.

  For a given $\widetilde{x}$, the relative error of $\widetilde{x}$ is bounded by the condition of matrix $A$ times the ratio between the norm of the residual and the norm of $b$.
\[
\frac{\norm{x - \widetilde{x}}}{\norm{x}} \leq \kappa(A) \frac{\norm{\mathbf{r}}}{\norm{b}}.	
\]
\end{theorem}

This theorem tells us that $\widetilde{x}$ is ``close to the solution'' apart from a factor which is the conditioning of matrix $A$.

\begin{proof}
Follows from the perturbation results for linear systems. The idea is that $\widetilde{x}$ is the exact solution of the perturbed system 
\[
  A\widetilde{x} \stareq b + \mathbf{r} = \widetilde{b}
\]

Where $\stareq$ follows from the definition of $r$ and $\frac{\norm{\widetilde{b} - b}}{\norm{b}} = \frac{\norm{r}}{\norm{b}}$.

  A relative perturbation of size $\frac{\mathbf{r}}{b}$ is amplified by $\kappa(A)$.
\end{proof}

It's important to notice that also computing $A \odot x \ominus b$ is an approximated operation. We choose to simplify things and ignore this error.

\subsection{A posteriori check for Least Squares Problems}
Can we make an analogous check for the Least Squares Problem?

  Take a LSP $\min \norm{Ax-b}$, with $A$ tall thin, with full column rank.

  The problem is that $\norm{Ax - b}$ isn't small at all, indeed it could be as large as $b$, as you can see from \Cref{fig:15nov3}.

\addpic{0.5}{pics/15nov/3.png}{$A$ can be as large as $b$ if $b$ is perfectly orthogonal.}{fig:15nov3}

  \begin{obs}
    If you solve LSP via QR $\norm{Ax - b} = \norm{\begin{pmatrix} 
    R_1 x - \tr{Q_1}b\\ -\tr{Q_2} b \end{pmatrix}}$. We said that the entries in the second block are fixed irrespective of $x$, but we could make the entries in the first block zero, by choosing $x=R_1^{-1}\tr{Q}b$. This information let us infer something about the values of the vectors in \Cref{fig:15nov3}, in particular the minimum of the value that we can get is $\norm{\tr{Q_2} b} = \norm{Ax - b}$.

    With some algebra we may also check that $\norm{\tr{Q_1} b} = \norm{Ax}$.
  \end{obs}

  Since this is a minimum problem, we know that the gradient of the function is small near the optimum value: $\min \sqrnorm{Ax - b} = \min \tr{x} \tr{A} A x  - 2 \tr{b} A x + \tr{b} b$.

  $\nabla_{\widetilde{x}} f = 2 (\tr{A} A x - \tr{A} b) \to 0$

  \begin{theorem}
    $\frac{\norm{\widetilde{x} - x}}{x} \le {(\kappa(A))}^2 \cdot \frac{\norm{\tr{A} A x - \tr{A} b}}{\norm{\tr{A} b}}$. Although we might have wanted to have the condition number of the problem, instead of the condition number of $A$ and this could lead to underestimating the error.
  \end{theorem}

Another idea could be using as error the first entry of the vector obtained via QR (namely $\tr{R_1} x - \tr{Q_1} b$), by imposing $R_1 x = \tr{Q_1} b$

We may observe that this is a truly backward stable measure:

given $r = \norm{\tr{R_1} \widetilde{x} - \tr{Q_1} b}$, there exists $\widetilde{b}$ with $\norm{\widetilde{b} - b} = \norm{r}$ such that $\widetilde{x}$ is the exact solution of $\min \norm{Ax - \widetilde{b}}$.

We have proved the following

\begin{proposition}
  $\frac{\norm{\widetilde{x} - x}}{\norm{x}} \le \kappa_{rel, LS} \cdot \frac{\norm{\widetilde{b} - b}}{\norm{b}} = \kappa_{rel, LS} \cdot \frac{\norm{r}}{\norm{b}}$.
\end{proposition}


\begin{theorem}
  Let $A=Q_1R_1$ be a thin QR factorization. Let $\mathbf{r}_1 = Q_1^T(A\widetilde{\x}-\mathbf{b})$. Then, $\widetilde{\x}$ is the exact solution of the LS problem
\[
\min \norm{A\x - (\mathbf{b}+Q_1\mathbf{r}_1)},
\]
so the backward error of $\widetilde{\x}$ is $\norm{Q_1\mathbf{r}_1} = \norm{\mathbf{r}_1}$.
\end{theorem}

\begin{proof}
  Idea: replay the solution of a LS problem with QR factorization, and use $\tr{Q_1}TQ_1=I$. You will get in the first block $R_1 x = \tr{Q_1} b + \mathbf{r}_1$, i.e., $\tr{Q_1}(Ax-b)=\mathbf{r}_1$, which is verified by $\widetilde{x}$.
\end{proof}
\end{document}
