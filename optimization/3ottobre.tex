\documentclass[computationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\chapter{3rd of October 2018}
\chapterauthor{A. Frangioni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\begin{example}[On derivatives]
Let $f:\R^2 \to \R$ such that $f(x, y) = x^2 e^y$.
Compute the partial derivatives and the directional derivatives in the two directions $\vect{d_1} = \tr{(0,1)}$ and $\vect{d_2} = \tr{(1,0)}$.

\begin{itemize}
	\item $\frac{\partial f}{\partial x} = 2x e^y $
	\item $\frac{\partial f}{\partial y} = x^2 e^y $
	\item $\parder{f}{\vect{d_1}} = \lim\limits_{t \to 0} \frac{f(x + t \cdot 0, y + t \cdot 1)}{t} =  \lim\limits_{t \to 0} \frac{f(x, y+ t)}{t}$.
	An attentive reader may notice that the directional derivative in direction $\vect{d_1}$ is equivalent to the derivative on the second component.
	\item $\parder{f}{\vect{d_2}} = \lim\limits_{t \to 0} \frac{f(x + t \cdot 1, y + t \cdot 0)}{t} =  \lim\limits_{t \to 0} \frac{f(x + t, y)}{t}$. Conversely, with respect to what stated before, the directional derivative of $f$ along the direction $\vect{d_2}$ is equivalent to the partial derivative w.r.t the first component.
\end{itemize}

Let us compute the scalar product between the gradient and the direction $\vect{d_1}$:

\[
\frac{\partial f}{\partial \vect{d_1}} = \ps{\begin{pmatrix}2x e^y\\ x^2 e^y\end{pmatrix}}{\begin{pmatrix} 0\\ 1 \end{pmatrix}} =  \begin{pmatrix}
2x e^y \cdot 0\\ x^2 e^y \cdot 1 \end{pmatrix} = \begin{pmatrix}
0\\ x^2 e^y \end{pmatrix}
\]

\end{example}

\noindent The intuition of this example is formalized in the following

\begin{proposition}
Let $f: \R^n \to \R$. 
The directional derivative along a certain direction $\vect{d} \in \R^n$ can be computed as the scalar product between the gradient of the function and the direction, formally
\[
\frac{\partial f}{\partial \vect{d}} = \ps{\nabla f}{ \vect{d}}
\]
\end{proposition}

\begin{definition}[Vector-valued function]
  A function which codomain is multi-dimensional is called \textbf{vector-valued function}.
  Formally, $f: \R^n \to \R^m$, where $f(\vect{x}) =  \begin{pmatrix} f_1(\vect{x})\\f_2(\vect{x})\\ \vdots\\ f_m(\vect{x})\end{pmatrix} \in \R^m$.
\end{definition}

For such functions the computation of the derivative requires to specify not only the component with respect to the one the derivation should be performed, but also the index of the function.

\begin{definition}[Partial derivative for vector-valued functions]
Let $f: \R^n \to \R^m$, the partial derivative of the $j$-th function with respect to the $i$-th component is
\[
   \frac{\partial f_{j}}{\partial x_i}(\vect{x}) =
   \lim_{t \to 0} \frac{f_{j}(x_1, x_2, \ldots, x_{i-1}, \ldots, x_i + t, x_{i+1}, \ldots, x_n) - f_{j}(\vect{x})}{t}
\]
where $t \in \R$.
\end{definition}

\begin{definition}[Jacobian]
  Let $f: \R^n \to \R^m$ be a vector-valued function. 
  We call \textbf{Jacobian} the matrix of all its first-order partial derivatives.
\[
  Jf(\vect{x}) =
  \begin{pmatrix}
    \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial f_m}{\partial x_1} & \cdots & \frac{\partial f_m}{\partial x_n}\\
  \end{pmatrix}
  =
  \begin{pmatrix}
    \nabla f_1(\vect{x})\\
    \nabla f_2(\vect{x})\\
    \vdots\\
    \nabla f_m(\vect{x})
  \end{pmatrix}
\]
\end{definition}


\begin{example}
Let $f:\R^2 \to \R$ such that $f(x, y) = x^2 e^y$ as before, where the gradient was
\[
  \nabla f(\vect{x})
  =
  \begin{pmatrix}
    \frac{\partial f}{\partial x}\\
    \frac{\partial f}{\partial y} 
  \end{pmatrix}
  =
  \begin{pmatrix}
    2x e^y\\
    x^2 e^y
  \end{pmatrix}
\]

Let us compute the second derivative of this function:
\[
  \nabla^2 f(\vect{x})
  =
  \begin{pmatrix}
    \frac{\partial^2 f}{\partial x \partial x} & \frac{\partial^2 f}{\partial y \partial x}\\

    \frac{\partial^2 f}{\partial x \partial  y} & \frac{\partial^2 f}{\partial y \partial y}\\

  \end{pmatrix}
  =
  \begin{pmatrix}
    2 e^y & 2x e^y\\
    2x e^y & x^2 e^y\\
  \end{pmatrix}
\]
\end{example}

\noindent The second order derivative of a vector function is a matrix, defined as 

\begin{definition}[Hessian]
  Let $f: \R^n \to \R$, we call \textbf{Hessian} of $f$
  \[
  \nabla^2 f(\vect{x}) := J \nabla f(\vect{x}) =
  \begin{pmatrix}
   \frac{\partial^2 f}{\partial x_1^2}(\vect{x}) &
   \frac{\partial^2 f}{\partial x_2 \partial x_1}(\vect{x}) & \cdots &
   \frac{\partial^2 f}{\partial x_n \partial x_1}(\vect{x})\\
   %
   \frac{\partial^2 f}{\partial x_1 \partial x_2}(\vect{x}) &
   \frac{\partial^2 f}{\partial x_2^2}(\vect{x}) & \cdots &
   \frac{\partial^2 f}{\partial x_n \partial x_2}(\vect{x}) \\
   %
   \vdots & \vdots & \ddots & \vdots \\
   %
   \frac{\partial^2 f}{\partial x_1 \partial x_n}(\vect{x}) &
   \frac{\partial^2 f}{\partial x_2 \partial x_n}(\vect{x}) & \cdots &
   \frac{\partial^2 f}{\partial x_n^2}(\vect{x})
  \end{pmatrix}
\]
\end{definition}

\noindent The size of the matrix grows very rapidly with the number of derivatives that we make.

In optimization, handling Hessians is a crucial task, because such matrices are very large hence unfeasible most of the times.

In most cases, the Hessian is symmetric, meaning that the order in which we derive is not relevant and this happens when $\exists \delta > 0$ s.t. $\forall \vect{x}' \in \mathcal{B}(x, \delta)$ $\frac{\partial^2 f}{\partial x_j \partial x_i}(\vect{x}')$ and  $\frac{\partial^2 f}{\partial x_i \partial x_j}(\vect{x}')$ exist and $\frac{\partial^2 f}{\partial x_j \partial x_i}(\vect{x}')$ and  $\frac{\partial^2 f}{\partial x_i \partial x_j}(\vect{x}')$ are continuous at $\vect{x}$.


\begin{definition}[$\C{2}$ functions]
  Let $f: \R^n \to \R^m$. We say that $f$ \textbf{belongs to $\C{2}$ class} iff $\nabla^2 f(x)$ is continuous.
\end{definition}

\begin{property}
Let $f: \R^n \to \R^m$ such that $f \in \C{2}$, then the Hessian is \emph{symmetric} and the gradient is \emph{continuous}.
\end{property}

\noindent We are now interested in providing some approximations to a generic function $f$.
Such approximations are useful tools to have ah hint on where the function moves along a decreasing direction, but the information is accurate only locally.

\begin{definition}[First order Taylor model]
Let $f : \R^n \to \R \in C^1$, we define the \textbf{first-order Taylor's formula} as a linear approximation of the function $f$ in a neighbourhood of $\vect{x}$.

Formally,  for a given ball $\mathcal{B}(\vect{x}, \delta)$, $\forall \vect{x}' \in \mathcal{B}(\vect{x}, \delta)~\exists ~ \alpha \in (0, 1)$ s.t.
\[
  f(\vect{x}') = \ps{\nabla f(\alpha \vect{x}+ (1 - \alpha) \vect{x}')} {\vect{x}' - \vect{x}} + f(\vect{x})
\]

Equivalently, we can write the so-called \textbf{remainder version of first-order Taylor formula} as

\[
  f(\vect{x}') = \ps{\nabla f(\vect{x})}{\vect{x}' - \vect{x}} + f(\vect{x})+ R(\vect{x}' - \vect{x})
\]
  where $\lim\limits_{h \to 0} \frac{R(h)}{\norm{h}} = 0$, in other words the error that we make is at most quadratic.
\end{definition}

\noindent Notice that the Taylor's approximation works \emph{only locally}: the furthest we get from $\vect{x}$ the more distant the function and the model are.

\begin{definition}[Second order Taylor model]
Let $f : \R^n \to \R \in C^1$, we define the \textbf{second-order Taylor's formula}  as a quadratic approximation of the function $f$ in a neighbourhood of $\vect{x}$.
Formally, given a ball $\mathcal{B}(\vect{x}, \delta)$, $\forall \vect{x}' \in \mathcal{B}(\vect{x}, \delta)$
\[
  f(\vect{x}') = L_x(\vect{x}') + \frac{1}{2} \tr{(\vect{x}' - \vect{x})} \nabla^2 f(\vect{x}) (\vect{x}' - \vect{x}) + R(\vect{x}' - \vect{x})
\]
  with $\lim\limits_{h \to 0} R(h) \norm{h}^2 = 0$ or, equivalently, the remainder vanishes at least cubically, or the error is $O(\norm{\vect{x}' - \vect{x}}^3)$ .
\end{definition}

\begin{example}
	Let $f:\R^2 \to \R$ such that $f(x, y) = x^2 e^y$, as above and $\nabla f(\vect{x})=\begin{pmatrix}
	2x e^y\\
	x^2 e^y
	\end{pmatrix}$ and $\nabla^2 f(\vect{x}) =	\begin{pmatrix}
	2 e^y & 2x e^y\\
	2x e^y & x^2 e^y\\
	\end{pmatrix}$.
	In $\vect{x} = \begin{pmatrix}1\\0\end{pmatrix} \in \R^2$ we have $f(\vect{x}) = 1$ and $\nabla f(\vect{x}) = \begin{pmatrix}2\\1\end{pmatrix}$.
	
	The linear model has the following shape 
	\[
	L_{(1, 0)} (x, y) = 1\ps{\begin{pmatrix}2\\ 1\end{pmatrix}}{\begin{pmatrix} x-1\\ y-0\end{pmatrix}} + 1= 2x -  2 + y + 1 = 2x + y -1
	\]
	
	And the quadratic model is 
	\begin{equation}
	\begin{split}
	Q_{[1, 0]} (x, y) &= 2x + y -1 + \frac{1}{2} \cdot \begin{pmatrix}x-1, & y-0\end{pmatrix} \cdot \begin{pmatrix} 2 & 2\\ 2 & 1 \end{pmatrix} \cdot \begin{pmatrix} x-1 \\ y-0 \end{pmatrix}\\ 
	&= 2x + y -1 + \frac{1}{2} \cdot \begin{pmatrix} 2x -2 +2y, & 2x -2 +y \end{pmatrix} \cdot \begin{pmatrix} x-1\\ y-0\end{pmatrix}\\
	&= 2x + y -1 + \frac{1}{2} \cdot \biggl ( (2x -2 + 2y) \cdot (x-1)  + (2x -2 +y) \cdot y \biggr )\\
	&= 2x + y -1 + \frac{1}{2} \cdot (2 x^2 - 2x + 2xy -2x +2 -2y +2xy -2y +y^2)\\
	&= 2x +y -1 + \frac{1}{2} \cdot (2 x^2 -4x + 4xy +2 -4y + y^2)\\
	&= 2x +y -1 + x^2 -2x + 2xy +1 -2y + \frac{1}{2}y^2)\\
	&= x^2 + 2xy -y + \frac{1}{2}y^2\\	
	\end{split}
	\end{equation}
\end{example}

The Taylor model can be extended to the more general $k$-th order.
Such an expansion requires the computation of the  $k$-th order derivatives, but $\nabla^k f(\vect{x})$ is a tensor of order $k \equiv n^k$ numbers.
For $k > 2$ this approach is practically unfeasible.

As it happens often in computer science, it is important to find a good trade-off between the complexity of the model and the accuracy of the approximation: if the model is too simple (but efficient) the approximation is not very good; conversely, complex and accurate approximations are computationally heavy.

\begin{proposition}
  Let $f : \R^n \to \R \in \C{1}$.
  If $f$ is Lipschitz continuous on $S \in \R^n$, then in such set the norm of the gradient is bounded by the Lipshitz constant.
  
  Formally, $\sup \{\norm{\nabla f(\vect{x})}~:~\vect{x} \in S\} \leq L$.
  If $S$ convex  $\sup \{\norm{\nabla f(\vect{x})}~:~\vect{x} \in S\} = L$.
\end{proposition}

Moreover, we can prove

\begin{proposition}
Let $f : \R^n \to \R \in \C{1}$. $f$ is Lipschitz continuous on $S \in \R^n$ iff its Hessian in bounded on $S$.
Formally, $\sup \{\norm{\nabla^2 f(\vect{x})}~:~\vect{x} \in S\} \leq L$.
\end{proposition}

\begin{proposition}
  Let $f : \R^n \to \R \in \C{1}$.
  If the gradient of $f$ is Lipschitz continuous in all points $\vect{x}' \in \R^n$ that are close to $\vect{x} \in \R^n$, then $f(\vect{x}') \leq L_x(\vect{x}') + \frac{L}{2} \norm{\vect{x}' - \vect{x}}^2$.
\end{proposition}

\section{Simple functions}
In the rest of the course we will deal mostly with some linear and quadratic approximations of the objective function.
In this section, we introduce a couple of examples and considerations on such functions.

\subsection{Linear functions}
In this scenario, $f: \R^n \to \R$ has the following shape: $f(\vect{x}) = \tr{\vect{c}}\vect{x}$, for a fixed $\vect{c} \in \R^n$.

It holds that $\nabla f(\vect{x}) = \vect{c}$, $\nabla^2 f(\vect{x}) = 0$ and that level sets are parallel hyperplanes orthogonal to $\vect{c}$.

\begin{figure}[h]
  \centering
  
  \subfigure[Linear function]{\includegraphics[width=0.35\textwidth]{pics/3ott/simplef1.png}\label{subfloat:3ott_1}}
  \hspace{0.5cm}
  \subfigure[Level sets]{\includegraphics[width=0.29\textwidth]{pics/3ott/simplef2.png}\label{subfloat:3ott_2}}\\
  \caption{Graphical example of linear function.}\label{fig:3ott_1}
\end{figure}

\subsection{Quadratic functions}
A quadratic function $f:\R^n \to \R$ is formalized as $f(\vect{x}) = \frac{1}{2} \tr{\vect{x}} Q \vect{x} + \vect{q}\vect{x}$, where $Q \in \mathcal{M}(n, \R)$, $\tr{\vect{q}} \in \R^n$.

In \Cref{fig:3ott_2} we can see the plot of the quadratic function where 
\[
  Q = \left[\begin{array}{rr} 6 & -2 \\ -2 & 6 \end{array}\right]
  q = \left[\begin{array}{rr} 0 \\ 0 \end{array}\right]
\]

\begin{figure}[h]
  \centering
  \subfigure[Quadratic function]{\includegraphics[width=0.35\textwidth]{pics/3ott/simplef3.png}\label{subfloat:3ott_3}}
  \hspace{0.5cm}
  \subfigure[Level sets]{\includegraphics[width=0.29\textwidth]{pics/3ott/simplef4.png}\label{subfloat:3ott_4}}\\
  \caption{Graphical example of a quadratic function.}\label{fig:3ott_2}
\end{figure}

In quadratic functions the gradient is a linear function on $\vect{x}$ $\nabla f(\vect{x}) = Q \vect{x} + \vect{q}$, while the Hessian is just $Q$ and the level sets are ellipsoids.
Sometimes such ellipses can degenerate to lines, in the case that one of the axis has become $+\infty$.

\begin{proposition}
  Let $f:\R^n \to \R$ be a quadratic function with parameters $Q \in \mathcal{M}(n, \R)$ and $\tr{\vect{q}} \in \R^n$.
  If $Q$ is symmetric, then it has spectral decomposition.
\end{proposition}

\begin{proof}
$\vect{x}^T Q \vect{x} = \frac{[(\vect{x}^T Q \vect{x}) + {(\vect{x}^T Q \vect{x})}^T]}{2} = \vect{x}^T [\frac{(Q + Q^T )}{2}] \vect{x} = H \Lambda H^T$
\end{proof}

\noindent In the rest of the course we will assume for simplicity that $Q$ is symmetric.
Another ``lucky case'' is when $Q$ is non singular (i.e. all its eigenvalues are not $0$): in this case,  $\bar{\vect{x}} = - \inv{Q} \vect{q}$, where $\bar{\vect{x}}$ is called the center of the ellipsoid.
Let us assume that we moved the origin in such $\bar{\vect{x}}$, so $\vect{x}' = \vect{x} - \bar{\vect{x}}$ and $f_{\bar{\vect{x}}}(\vect{x}') = \frac{1}{2} \tr{\vect{x}'} Q \vect{x}'$.

\begin{proposition}
Along $H_i$: $f(\alpha) = f_{\bar{\vect{x}}}( \alpha H_i ) = \alpha^2 \lambda_i$
\end{proposition}

\noindent Moreover, the size of the axes of the level curves is proportional to $\sqrt{1 / \lambda_i}$, where $\lambda_i$ are the eigenvalues.
If the eigenvalues are close to $0$, then the axes get very long, while when they are negative, we no longer have axes.

This is formalized in

\begin{proposition}
	Let $f:\R^n \to \R$ such that $f(\vect{x}) = \frac{1}{2} \tr{\vect{x}} Q \vect{x} + \vect{q} \vect{x}$.
	If $Q$ is positive definite $\bar{\vect{x}} = - \inv{Q} \vect{q}$ is the minimum of $f$, while if $Q$ is \emph{indefinite} (i.e. $\exists \lambda_i <0$) $f$ is unbounded below.
\end{proposition}

\end{document}

