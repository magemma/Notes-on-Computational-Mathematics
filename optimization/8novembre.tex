%chktex-file 36
%chktex-file 8
%chktex-file 24
\documentclass[computationalMathematics.tex]{subfiles}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\chapter{8th of November 2018}
\chapterauthor{A. Frangioni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\section{Good practices for the project's design}
In this lecture some good suggestions for implementing the gradient method in Matlab:
\begin{itemize}
  \item When we have to implement a function the first question we need to ask ourselves is: ``how many parameters should the function take?'';
  \item The interface needs to be designed, i.e.~in the case of the gradient method we would like the user to be able to run it although he might not know what a good starting point could be;
  \item There might be some parameters with the same semantic of hyper-parameters, so they need to be adjusted in order to specify the algorithm;
  \item Another thing that should be taken into consideration is that sometimes, when measuring the number of times the code of the function has been executed we need to count also the calls to functions inside a single step;
  \item It is important to check the conditions that should be satisfied by the input data for the theoretical coherence. In case of not valid input an error message should be returned;
  \item Another important thing to be done is building a set of test functions, possibly limit case for the problem, to see how the algorithm works;
  \item \texttt{diff(f, x)} calculates the gradient of the function $f$ in the variable $x$. If we want to get a simplified version, we may run \texttt{simplify(diff(f,x))};
  \item A very nice tool to compute derivatives for arbitrary functions is \href{https://sourceforge.net/projects/adigator}{\textbf{Adigator}}\footnote{https://sourceforge.net/projects/adigator}. 
\end{itemize}