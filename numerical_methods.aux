\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\bbl@cs{beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\@ifundefined{tof@begin}{\global \let \tof@begin\relax \global \let \tof@finish\@empty\global \let \tof@starttags\@gobble\global \let \tof@stoptags\@gobble\global \let \tof@tagthis\@gobble\global \let \tof@untagthis\@gobble}{}}
\@writefile{toc}{\tof@begin}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}20th of September 2018 --- F. Poloni}{5}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}A warm up}{5}{subsection.0.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.1}{\ignorespaces How a change of basis reflects on the space.\relax }}{7}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:20sett1}{{0.1}{7}{How a change of basis reflects on the space.\relax }{figure.caption.2}{}}
\newlabel{fig:20sett1@cref}{{[figure][1][0]0.1}{[1][7][]7}}
\savepointas {row 1}{pgfid5}
\savepointas {col 1}{pgfid7}
\savepointas {row 2}{pgfid9}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Solving Linear Systems}{8}{subsection.0.1.2}\protected@file@percent }
\savepointas {row 3}{pgfid11}
\savepointas {row 4}{pgfid13}
\savepointas {row 5}{pgfid15}
\savepointas {row 6}{pgfid17}
\savepointas {row 7}{pgfid19}
\savepointas {row 8}{pgfid21}
\savepointas {row 9}{pgfid23}
\pgfsyspdfmark {pgfid4}{7931168}{41105725}
\pgfsyspdfmark {pgfid5}{14318757}{40319293}
\pgfsyspdfmark {pgfid6}{16303045}{40712509}
\pgfsyspdfmark {pgfid7}{19142945}{36727921}
\pgfsyspdfmark {pgfid8}{25366695}{41105725}
\pgfsyspdfmark {pgfid9}{28206595}{40319293}
\pgfsyspdfmark {pgfid10}{7840147}{34095718}
\pgfsyspdfmark {pgfid11}{10680047}{33309286}
\pgfsyspdfmark {pgfid12}{12864578}{34488934}
\pgfsyspdfmark {pgfid13}{15704478}{32916070}
\pgfsyspdfmark {pgfid14}{18216702}{34095718}
\pgfsyspdfmark {pgfid15}{18726428}{33309286}
\pgfsyspdfmark {pgfid16}{20637893}{33702502}
\pgfsyspdfmark {pgfid17}{23477793}{33702502}
\pgfsyspdfmark {pgfid18}{25716951}{34095718}
\pgfsyspdfmark {pgfid19}{27391764}{33309286}
\pgfsyspdfmark {pgfid20}{29485274}{34095718}
\pgfsyspdfmark {pgfid21}{32325174}{33309286}
\pgfsyspdfmark {pgfid22}{34819193}{34095718}
\pgfsyspdfmark {pgfid23}{37659093}{33309286}
\@writefile{lof}{\contentsline {figure}{\numberline {0.2}{\ignorespaces The adjacency matrix of a biparted graph has $0$s in its bottom left part (Matlab syntax \texttt  {A[p+1:n; 1:p]=0}), which means that the edges from a connected component and the other are in one direction only.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:20sett3}{{0.2}{10}{The adjacency matrix of a biparted graph has $0$s in its bottom left part (Matlab syntax \texttt {A[p+1:n; 1:p]=0}), which means that the edges from a connected component and the other are in one direction only.\relax }{figure.caption.3}{}}
\newlabel{fig:20sett3@cref}{{[figure][2][0]0.2}{[1][9][]10}}
\newlabel{exp1:20sett}{{0.1.2}{10}{}{example.0.1.2}{}}
\newlabel{exp1:20sett@cref}{{[example][2][0,1]0.1.2}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Orthogonality}{10}{subsection.0.1.3}\protected@file@percent }
\newlabel{def:20sett_norm}{{0.1.9}{10}{Norms}{definition.0.1.9}{}}
\newlabel{def:20sett_norm@cref}{{[definition][9][0,1]0.1.9}{[1][10][]10}}
\newlabel{fact:20sett}{{0.1.4}{11}{}{theorem.0.1.4}{}}
\newlabel{fact:20sett@cref}{{[proposition][4][0,1]0.1.4}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}26th of September 2018 --- F. Poloni}{12}{section.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Orthogonality (II)}{12}{subsection.0.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.3}{\ignorespaces Matrix $U_1$ represents a rotation, while $U_2$ is a symmetry operation.\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:26sett_ortho}{{0.3}{12}{Matrix $U_1$ represents a rotation, while $U_2$ is a symmetry operation.\relax }{figure.caption.4}{}}
\newlabel{fig:26sett_ortho@cref}{{[figure][3][0]0.3}{[1][12][]12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Eigenvalues / Eigenvector}{13}{subsection.0.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Eigenvector: what could possibly go wrong?}{15}{section*.5}\protected@file@percent }
\newlabel{subfig:quad1}{{0.4(a)}{17}{Subfigure 0 0.4(a)}{subfigure.0.4.1}{}}
\newlabel{subfig:quad1@cref}{{[subfigure][1][0,4]0.4(a)}{[1][17][]17}}
\newlabel{sub@subfig:quad1}{{(a)}{17}{Subfigure 0 0.4(a)\relax }{subfigure.0.4.1}{}}
\newlabel{subfig:quad2}{{0.4(b)}{17}{Subfigure 0 0.4(b)}{subfigure.0.4.2}{}}
\newlabel{subfig:quad2@cref}{{[subfigure][2][0,4]0.4(b)}{[1][17][]17}}
\newlabel{sub@subfig:quad2}{{(b)}{17}{Subfigure 0 0.4(b)\relax }{subfigure.0.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.4}{\ignorespaces The plot of functions.\relax }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:26sett_quadratic}{{0.4}{17}{The plot of functions.\relax }{figure.caption.6}{}}
\newlabel{fig:26sett_quadratic@cref}{{[figure][4][0]0.4}{[1][17][]17}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$f_1(x)$}}}{17}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$f_2(x)$}}}{17}{figure.caption.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.3}28th of September 2018 --- F. Poloni}{20}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Singular value decomposition (SVD)}{20}{subsection.0.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Properties of SVD}{21}{section*.7}\protected@file@percent }
\newlabel{prop:28sett_rank}{{0.3.1}{22}{}{theorem.0.3.1}{}}
\newlabel{prop:28sett_rank@cref}{{[property][1][0,3]0.3.1}{[1][21][]22}}
\pgfsyspdfmark {pgfid30}{22942919}{34703772}
\pgfsyspdfmark {pgfid31}{22942919}{34611839}
\pgfsyspdfmark {pgfid32}{28915426}{34019237}
\pgfsyspdfmark {pgfid33}{10568990}{29973686}
\newlabel{prop:28sett_norms}{{0.3.2}{22}{}{theorem.0.3.2}{}}
\newlabel{prop:28sett_norms@cref}{{[property][2][0,3]0.3.2}{[1][22][]22}}
\newlabel{fact:28sett_orthogonorm}{{0.3.3}{23}{}{theorem.0.3.3}{}}
\newlabel{fact:28sett_orthogonorm@cref}{{[proposition][3][0,3]0.3.3}{[1][22][]23}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}4th of October 2018 --- F. Poloni}{25}{section.0.4}\protected@file@percent }
\newlabel{subfloat:4ott_rk1}{{0.5(a)}{25}{Subfigure 0 0.5(a)}{subfigure.0.5.1}{}}
\newlabel{subfloat:4ott_rk1@cref}{{[subfigure][1][0,5]0.5(a)}{[1][25][]25}}
\newlabel{sub@subfloat:4ott_rk1}{{(a)}{25}{Subfigure 0 0.5(a)\relax }{subfigure.0.5.1}{}}
\newlabel{subfloat:4ott_rk2}{{0.5(b)}{25}{Subfigure 0 0.5(b)}{subfigure.0.5.2}{}}
\newlabel{subfloat:4ott_rk2@cref}{{[subfigure][2][0,5]0.5(b)}{[1][25][]25}}
\newlabel{sub@subfloat:4ott_rk2}{{(b)}{25}{Subfigure 0 0.5(b)\relax }{subfigure.0.5.2}{}}
\newlabel{subfloat:4ott_rk5}{{0.5(c)}{25}{Subfigure 0 0.5(c)}{subfigure.0.5.3}{}}
\newlabel{subfloat:4ott_rk5@cref}{{[subfigure][3][0,5]0.5(c)}{[1][25][]25}}
\newlabel{sub@subfloat:4ott_rk5}{{(c)}{25}{Subfigure 0 0.5(c)\relax }{subfigure.0.5.3}{}}
\newlabel{subfloat:4ott_rkfull}{{0.5(d)}{25}{Subfigure 0 0.5(d)}{subfigure.0.5.4}{}}
\newlabel{subfloat:4ott_rkfull@cref}{{[subfigure][4][0,5]0.5(d)}{[1][25][]25}}
\newlabel{sub@subfloat:4ott_rkfull}{{(d)}{25}{Subfigure 0 0.5(d)\relax }{subfigure.0.5.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.5}{\ignorespaces How the approximation of a matrix changes with respect to the different ranks.\relax }}{25}{figure.caption.8}\protected@file@percent }
\newlabel{fig:4ott_rank}{{0.5}{25}{How the approximation of a matrix changes with respect to the different ranks.\relax }{figure.caption.8}{}}
\newlabel{fig:4ott_rank@cref}{{[figure][5][0]0.5}{[1][25][]25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Rank 1}}}{25}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Rank 2}}}{25}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Rank 5}}}{25}{figure.caption.8}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Full rank}}}{25}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.5}10th of October 2018 --- F. Poloni}{27}{section.0.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.6}{\ignorespaces In this case the image of the matrix $A$ (in red) does not contain $b$ and the best one can do is to obtain a projection of $b$ in the plane $Im(A)$ (drawn in blue).\relax }}{27}{figure.caption.9}\protected@file@percent }
\newlabel{fig:10ott_1}{{0.6}{27}{In this case the image of the matrix $A$ (in red) does not contain $b$ and the best one can do is to obtain a projection of $b$ in the plane $Im(A)$ (drawn in blue).\relax }{figure.caption.9}{}}
\newlabel{fig:10ott_1@cref}{{[figure][6][0]0.6}{[1][27][]27}}
\@writefile{toc}{\contentsline {section}{\numberline {0.6}18th of October 2018 --- F. Poloni}{29}{section.0.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.6.1}Least squares problem}{29}{subsection.0.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Method of normal equations}{30}{section*.10}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.7}{\ignorespaces Geometric idea.\relax }}{30}{figure.caption.11}\protected@file@percent }
\newlabel{fig:18ott1}{{0.7}{30}{Geometric idea.\relax }{figure.caption.11}{}}
\newlabel{fig:18ott1@cref}{{[figure][7][0]0.7}{[1][30][]30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.6.2}QR factorization}{30}{subsection.0.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.8}{\ignorespaces On the left in $\mathds  {R}^{2}$, on the right in $\mathds  {R}^{3}$.\relax }}{32}{figure.caption.12}\protected@file@percent }
\newlabel{fig:18ott2}{{0.8}{32}{On the left in $\mathds {R}^{2}$, on the right in $\mathds {R}^{3}$.\relax }{figure.caption.12}{}}
\newlabel{fig:18ott2@cref}{{[figure][8][0]0.8}{[1][32][]32}}
\@writefile{toc}{\contentsline {section}{\numberline {0.7}26th of October 2018 --- F. Poloni}{34}{section.0.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.7.1}How to construct a QR factorization}{34}{subsection.0.7.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.7.1}{\ignorespaces Householder vector Matlab implementation.\relax }}{34}{algorithm.0.7.1}\protected@file@percent }
\newlabel{alg:26ott1}{{0.7.1}{34}{Householder vector Matlab implementation.\relax }{algorithm.0.7.1}{}}
\newlabel{alg:26ott1@cref}{{[algorithm][1][0,7]0.7.1}{[1][34][]34}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.9}{\ignorespaces If $x$ is oriented as in the plot it's better if we choose $-\Vert x\Vert  e_{1}$ verse, since it's opposite to $x$.\relax }}{34}{figure.caption.13}\protected@file@percent }
\newlabel{fig:26ott1}{{0.9}{34}{If $x$ is oriented as in the plot it's better if we choose $-\|x\| e_{1}$ verse, since it's opposite to $x$.\relax }{figure.caption.13}{}}
\newlabel{fig:26ott1@cref}{{[figure][9][0]0.9}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsubsection}{Matlab implementation}{36}{section*.14}\protected@file@percent }
\newlabel{alg:26ottQR1}{{\caption@xref {alg:26ottQR1}{ on input line 211}}{36}{Matlab implementation}{section*.14}{}}
\newlabel{alg:26ottQR1@cref}{{[subsection][1][0,7]0.7.1}{[1][36][]36}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.7.2}{\ignorespaces First implementation of QR factorization.\relax }}{36}{algorithm.0.7.2}\protected@file@percent }
\newlabel{alg:26ottQR2}{{\caption@xref {alg:26ottQR2}{ on input line 242}}{37}{Matlab implementation}{theorem.0.7.2}{}}
\newlabel{alg:26ottQR2@cref}{{[subsection][1][0,7]0.7.1}{[1][37][]37}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.7.3}{\ignorespaces More efficient implementation of QR factorization.\relax }}{37}{algorithm.0.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.7.2}How to use the thin QR factorization to solve a least squares problem}{38}{subsection.0.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.8}7th of November 2018 --- F.Poloni}{40}{section.0.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.1}Least squares problem with SVD}{40}{subsection.0.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Behaviour in case of zeros as singular values}{43}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.2}Truncated SVD}{44}{subsection.0.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.3}Tikhonov regularization / ridge regression}{44}{subsection.0.8.3}\protected@file@percent }
\newlabel{eq:7novxtik}{{0.8.3}{44}{}{equation.0.8.3}{}}
\newlabel{eq:7novxtik@cref}{{[equation][3][0,8]0.8.3}{[1][44][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.10}{\ignorespaces Here is the shape of the formula for the singular values.\relax }}{46}{figure.caption.17}\protected@file@percent }
\newlabel{fig:7nov1}{{0.10}{46}{Here is the shape of the formula for the singular values.\relax }{figure.caption.17}{}}
\newlabel{fig:7nov1@cref}{{[figure][10][0]0.10}{[1][45][]46}}
\@writefile{toc}{\contentsline {section}{\numberline {0.9}9th of November 2018 --- F. Poloni}{46}{section.0.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.1}Conditioning}{46}{subsection.0.9.1}\protected@file@percent }
\newlabel{ex:9nov1}{{0.9.1}{46}{}{example.0.9.1}{}}
\newlabel{ex:9nov1@cref}{{[example][1][0,9]0.9.1}{[1][46][]46}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.11}{\ignorespaces Geometric idea of temperature of the water in the shower\relax }}{47}{figure.caption.18}\protected@file@percent }
\newlabel{fig:9nov1}{{0.11}{47}{Geometric idea of temperature of the water in the shower\relax }{figure.caption.18}{}}
\newlabel{fig:9nov1@cref}{{[figure][11][0]0.11}{[1][46][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.12}{\ignorespaces Geometric idea behind the derivative, as ``zoom'' of the function in a certain point.\relax }}{47}{figure.caption.19}\protected@file@percent }
\newlabel{fig:9nov2}{{0.12}{47}{Geometric idea behind the derivative, as ``zoom'' of the function in a certain point.\relax }{figure.caption.19}{}}
\newlabel{fig:9nov2@cref}{{[figure][12][0]0.12}{[1][47][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.13}{\ignorespaces Geometric idea behind absolute condition number.\relax }}{48}{figure.caption.20}\protected@file@percent }
\newlabel{fig:9nov3}{{0.13}{48}{Geometric idea behind absolute condition number.\relax }{figure.caption.20}{}}
\newlabel{fig:9nov3@cref}{{[figure][13][0]0.13}{[1][47][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.14}{\ignorespaces Paraboloid\relax }}{48}{figure.caption.21}\protected@file@percent }
\newlabel{fig:9nov4}{{0.14}{48}{Paraboloid\relax }{figure.caption.21}{}}
\newlabel{fig:9nov4@cref}{{[figure][14][0]0.14}{[1][48][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.15}{\ignorespaces Level curves of a quadratic function (``seen from above'').\relax }}{49}{figure.caption.22}\protected@file@percent }
\newlabel{fig:9nov4}{{0.15}{49}{Level curves of a quadratic function (``seen from above'').\relax }{figure.caption.22}{}}
\newlabel{fig:9nov4@cref}{{[figure][15][0]0.15}{[1][48][]49}}
\@writefile{toc}{\contentsline {subsubsection}{Conditioning of linear systems}{49}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.2}Condition number, SVD, and distance to singularity}{50}{subsection.0.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.3}Conditioning of least squares problem}{52}{subsection.0.9.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.16}{\ignorespaces The triangle in the figure (the one whose cathets are $Ax$ and $b-Ax$) is a square triangle.\relax }}{53}{figure.caption.24}\protected@file@percent }
\newlabel{fig:9nov6}{{0.16}{53}{The triangle in the figure (the one whose cathets are $Ax$ and $b-Ax$) is a square triangle.\relax }{figure.caption.24}{}}
\newlabel{fig:9nov6@cref}{{[figure][16][0]0.16}{[1][52][]53}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.17}{\ignorespaces Special case 1\relax }}{53}{figure.caption.25}\protected@file@percent }
\newlabel{fig:9nov7}{{0.17}{53}{Special case 1\relax }{figure.caption.25}{}}
\newlabel{fig:9nov7@cref}{{[figure][17][0]0.17}{[1][53][]53}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.18}{\ignorespaces Special case 2.\relax }}{54}{figure.caption.26}\protected@file@percent }
\newlabel{fig:9nov8}{{0.18}{54}{Special case 2.\relax }{figure.caption.26}{}}
\newlabel{fig:9nov8@cref}{{[figure][18][0]0.18}{[1][53][]54}}
\@writefile{toc}{\contentsline {section}{\numberline {0.10}15th of November 2018 --- F. Poloni}{54}{section.0.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.10.1}Stability of algorithms}{54}{subsection.0.10.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.19}{\ignorespaces We have $2^{52}$ equispaced numbers between $\genfrac  {}{}{}{}{1}{2}$ and $1$ and between $1$ and $2$, and also $2^{52}$ between $2$ and $4$ and so on and so forth, so we have the same number of integers, although the space is enlarging.\relax }}{54}{figure.caption.27}\protected@file@percent }
\newlabel{fig:15nov1}{{0.19}{54}{We have $2^{52}$ equispaced numbers between $\frac {1}{2}$ and $1$ and between $1$ and $2$, and also $2^{52}$ between $2$ and $4$ and so on and so forth, so we have the same number of integers, although the space is enlarging.\relax }{figure.caption.27}{}}
\newlabel{fig:15nov1@cref}{{[figure][19][0]0.19}{[1][54][]54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.10.2}Backward stability of QR factorization}{58}{subsection.0.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stability of algorithms for least-squares problems}{59}{section*.28}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Scrivere meglio}{59}{section*.29}\protected@file@percent }
\pgfsyspdfmark {pgfid49}{7030046}{15284751}
\pgfsyspdfmark {pgfid52}{32209157}{15298858}
\pgfsyspdfmark {pgfid53}{34057452}{15073128}
\@writefile{lof}{\contentsline {figure}{\numberline {0.20}{\ignorespaces $b$ lies (at least almost, because of numerical error) on the plane of $Im(A)$. In this case $\kappa _{rel}(\text  {solving LS}) \approx \kappa (A)$.\relax }}{60}{figure.caption.30}\protected@file@percent }
\newlabel{fig:15nov2}{{0.20}{60}{$b$ lies (at least almost, because of numerical error) on the plane of $Im(A)$. In this case $\kappa _{rel}(\text {solving LS}) \approx \kappa (A)$.\relax }{figure.caption.30}{}}
\newlabel{fig:15nov2@cref}{{[figure][20][0]0.20}{[1][60][]60}}
\@writefile{toc}{\contentsline {subsubsection}{A posteriori checks}{60}{section*.32}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Brief recap of the complexities of the algorithms we studied for solving the least squares problem. The last row takes into account the stability.\relax }}{61}{table.caption.31}\protected@file@percent }
\newlabel{tab:15nov1}{{1}{61}{Brief recap of the complexities of the algorithms we studied for solving the least squares problem. The last row takes into account the stability.\relax }{table.caption.31}{}}
\newlabel{tab:15nov1@cref}{{[table][1][0]1}{[1][60][]61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.10.3}A posteriori check for Least Squares Problems}{62}{subsection.0.10.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.21}{\ignorespaces $A$ can be as large as $b$ if $b$ is perfectly orthogonal.\relax }}{62}{figure.caption.33}\protected@file@percent }
\newlabel{fig:15nov3}{{0.21}{62}{$A$ can be as large as $b$ if $b$ is perfectly orthogonal.\relax }{figure.caption.33}{}}
\newlabel{fig:15nov3@cref}{{[figure][21][0]0.21}{[1][62][]62}}
\@writefile{toc}{\contentsline {section}{\numberline {0.11}21st of November 2018 --- F. Poloni}{64}{section.0.11}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.22}{\ignorespaces A local function on graph.\relax }}{64}{figure.caption.34}\protected@file@percent }
\newlabel{fig:21nov1}{{0.22}{64}{A local function on graph.\relax }{figure.caption.34}{}}
\newlabel{fig:21nov1@cref}{{[figure][22][0]0.22}{[1][64][]64}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.11.1}Gaussian elimination and LU factorization}{64}{subsection.0.11.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.23}{\ignorespaces Graphic idea of a bridge partitioned into small blocks\relax }}{65}{figure.caption.35}\protected@file@percent }
\newlabel{fig:21nov2}{{0.23}{65}{Graphic idea of a bridge partitioned into small blocks\relax }{figure.caption.35}{}}
\newlabel{fig:21nov2@cref}{{[figure][23][0]0.23}{[1][64][]65}}
\newlabel{obs:21novstroke1}{{0.11.1}{66}{Stroke of luck}{obs.0.11.1}{}}
\newlabel{obs:21novstroke1@cref}{{[obs][1][0,11]0.11.1}{[1][66][]66}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.11.1}{\ignorespaces LU factorization, Matlab implementation.\relax }}{66}{algorithm.0.11.1}\protected@file@percent }
\newlabel{algo:21nov1}{{0.11.1}{66}{LU factorization, Matlab implementation.\relax }{algorithm.0.11.1}{}}
\newlabel{algo:21nov1@cref}{{[algorithm][1][0,11]0.11.1}{[1][66][]66}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.24}{\ignorespaces Assuming that we are at step $k$, we have that the $h$-th multiplier is expressed as $\genfrac  {}{}{}{}{A_{hk}}{A_{kk}}$ and this muliplier goes to the right position in $L$.\relax }}{67}{figure.caption.36}\protected@file@percent }
\newlabel{fig:21nov3}{{0.24}{67}{Assuming that we are at step $k$, we have that the $h$-th multiplier is expressed as $\frac {A_{hk}}{A_{kk}}$ and this muliplier goes to the right position in $L$.\relax }{figure.caption.36}{}}
\newlabel{fig:21nov3@cref}{{[figure][24][0]0.24}{[1][67][]67}}
\@writefile{tdo}{\contentsline {todo}{missing}{67}{section*.37}\protected@file@percent }
\pgfsyspdfmark {pgfid64}{7030046}{8056282}
\pgfsyspdfmark {pgfid67}{32209157}{8070389}
\pgfsyspdfmark {pgfid68}{34057452}{7844659}
\@writefile{toc}{\contentsline {subsubsection}{Stability of LU}{68}{section*.38}\protected@file@percent }
\newlabel{21novstability}{{0.11.1}{68}{Stability of LU}{section*.38}{}}
\newlabel{21novstability@cref}{{[subsection][1][0,11]0.11.1}{[1][67][]68}}
\@writefile{toc}{\contentsline {subsubsection}{Gaussian elimination on sparse matrices}{68}{section*.39}\protected@file@percent }
\@writefile{toc}{\tof@stoptags{myfile1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.12}23rd of November 2018 --- F. Poloni}{71}{section.0.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.12.1}Gaussian elimination on symmetric matrices}{71}{subsection.0.12.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.12.1}{\ignorespaces Symmetric Gaussian factorization, Matlab implementation.\relax }}{72}{algorithm.0.12.1}\protected@file@percent }
\newlabel{algo:23novsymgauss}{{0.12.1}{72}{Symmetric Gaussian factorization, Matlab implementation.\relax }{algorithm.0.12.1}{}}
\newlabel{algo:23novsymgauss@cref}{{[algorithm][1][0,12]0.12.1}{[1][72][]72}}
\newlabel{lemma:23nov1}{{0.12.2}{72}{}{theorem.0.12.2}{}}
\newlabel{lemma:23nov1@cref}{{[lemma][2][0,12]0.12.2}{[1][72][]72}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.12.2}Cholesky factorization}{74}{subsection.0.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.12.3}Krylov subspace methods}{74}{subsection.0.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.13}29th of November 2018 --- F. Poloni}{76}{section.0.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Naive idea}{76}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Improvement}{76}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.13.1}Arnoldi algorithm}{76}{subsection.0.13.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.13.1}{\ignorespaces Arnoldi algorithm Matlab implementation.\relax }}{77}{algorithm.0.13.1}\protected@file@percent }
\newlabel{alg:29novarnoldi}{{0.13.1}{77}{Arnoldi algorithm Matlab implementation.\relax }{algorithm.0.13.1}{}}
\newlabel{alg:29novarnoldi@cref}{{[algorithm][1][0,13]0.13.1}{[1][77][]77}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.25}{\ignorespaces We started form a matirx $A$ which has many entries and it gets factorized by the product of two smaller matrices\relax }}{78}{figure.caption.42}\protected@file@percent }
\newlabel{fig:29nov1}{{0.25}{78}{We started form a matirx $A$ which has many entries and it gets factorized by the product of two smaller matrices\relax }{figure.caption.42}{}}
\newlabel{fig:29nov1@cref}{{[figure][25][0]0.25}{[1][78][]78}}
\@writefile{toc}{\contentsline {section}{\numberline {0.14}5th of December 2018 --- F. Poloni}{82}{section.0.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.14.1}Convergence of Arnoldi}{82}{subsection.0.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Better explanation}{82}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.15}7th of December 2018 --- F. Poloni}{85}{section.0.15}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {0.26}{\ignorespaces In this picture the eigenvalues are clustered around $P_1, P_2, P_3$ and $P_4$. We can find a polynomial $q$ such that $q(\text  {red points}) \approx 0$.\relax }}{87}{figure.caption.44}\protected@file@percent }
\newlabel{fig:7diceigen}{{0.26}{87}{In this picture the eigenvalues are clustered around $P_1, P_2, P_3$ and $P_4$. We can find a polynomial $q$ such that $q(\text {red points}) \approx 0$.\relax }{figure.caption.44}{}}
\newlabel{fig:7diceigen@cref}{{[figure][26][0]0.26}{[1][87][]87}}
\@writefile{toc}{\contentsline {section}{\numberline {0.16}13th of December 2018 --- F. Poloni}{88}{section.0.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.1}Lanczos algorithm}{88}{subsection.0.16.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {0.16.1}{\ignorespaces Pseudocode for the conjugate gradient method.\relax }}{88}{algorithm.0.16.1}\protected@file@percent }
\newlabel{alg:7dic_ConjGrad}{{0.16.1}{88}{Pseudocode for the conjugate gradient method.\relax }{algorithm.0.16.1}{}}
\newlabel{alg:7dic_ConjGrad@cref}{{[algorithm][1][0,16]0.16.1}{[1][88][]88}}
\newlabel{subfloat:13dic_arnoldi}{{0.27(a)}{89}{Subfigure 0 0.27(a)}{subfigure.0.27.1}{}}
\newlabel{subfloat:13dic_arnoldi@cref}{{[subfigure][1][0,27]0.27(a)}{[1][89][]89}}
\newlabel{sub@subfloat:13dic_arnoldi}{{(a)}{89}{Subfigure 0 0.27(a)\relax }{subfigure.0.27.1}{}}
\newlabel{subfloat:13dic_cg}{{0.27(b)}{89}{Subfigure 0 0.27(b)}{subfigure.0.27.2}{}}
\newlabel{subfloat:13dic_cg@cref}{{[subfigure][2][0,27]0.27(b)}{[1][89][]89}}
\newlabel{sub@subfloat:13dic_cg}{{(b)}{89}{Subfigure 0 0.27(b)\relax }{subfigure.0.27.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {0.27}{\ignorespaces Traditional orthogonality (Arnoldi) leads to the minimization of the 2-norm, while in the conjugate gradient we impose A-orthogonality and we get a good approximation in several norms.\relax }}{89}{figure.caption.45}\protected@file@percent }
\newlabel{fig:13dic_parallel}{{0.27}{89}{Traditional orthogonality (Arnoldi) leads to the minimization of the 2-norm, while in the conjugate gradient we impose A-orthogonality and we get a good approximation in several norms.\relax }{figure.caption.45}{}}
\newlabel{fig:13dic_parallel@cref}{{[figure][27][0]0.27}{[1][89][]89}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Arnoldi}}}{89}{figure.caption.45}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Conjugate gradient}}}{89}{figure.caption.45}\protected@file@percent }
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  EED6737C5336486CEEE35048F0DD4656EA874F595B2B8948FD4AD3908D478650.pygtex,
  E712477AB9939FB1E350DC42BBF9460AEA874F595B2B8948FD4AD3908D478650.pygtex,
  22342908BBF7FA85E77936BCA1943F3FEA874F595B2B8948FD4AD3908D478650.pygtex,
  506BE4FD22EE5A450C6DCC263C742067EA874F595B2B8948FD4AD3908D478650.pygtex,
  59DABF05D239FC17059D5272C76C11DDEA874F595B2B8948FD4AD3908D478650.pygtex,
  BD99D526AD70755A2045B543225F4CD5EA874F595B2B8948FD4AD3908D478650.pygtex,
  49F3809EE2A24D3923D3316C5317033FEA874F595B2B8948FD4AD3908D478650.pygtex,
  F7F54B3589E7236ADB558887A5B22C2FEA874F595B2B8948FD4AD3908D478650.pygtex}
\@writefile{toc}{\tof@finish}
