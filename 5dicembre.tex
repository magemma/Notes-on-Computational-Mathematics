%chktex-file 36
%chktex-file 8
%chktex-file 24
%chktex-file 35
%chktex-file 44
%chktex-file 1
\documentclass[computational_mathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{5th of December 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\recap{
  \textbf{Arnoldi factorization:} we wanted to build an orthonormal base $\{ q_1, q_2, \ldots, q_n\}$ of the Krylov space $K_n(A, b) = \{ b, Ab, A^2b, \ldots, A^{n-1}b\}$.
  We showed that the matrix $A$ could be more of less factorized as $AQ_n = Q_{n+1} H_n = Q_n H_n + q_{n+1} h_{n+1, n} \tr{e_{n+1}}$.

  Moreover, we concluded that we could approximate the eigenvalues of the matrix $A$ through the eigenvalues of matrix $H_n$, while the eigenvectors are obtained as $Q_n v$, where $v$ is an eigenvector of the matrix $H_n$.
}

In the first part of the lecture we run some experiments on MatLab and we observed that the eigenvalues are approximated better and better starting from the boundaries.
Notice that the approximation is not always good, but it is good enough if we take into account the complexity of the algorithm.

We are interested in explaining the convergence of Arnoldi method.

\subsection{Convergence of Arnoldi}

The eigenvalues of $H_n$ are eigenvalues of a ``nearby matrix'' obtained by taking $\widetilde{H}_m$ (result of the full process) and replacing $\widetilde{H}_{n+1,n}$ with zero.
\[
\scalebox{0.9}{
$
\widetilde{H}_m = \left[
\begin{array}{cccc|ccccc}
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
0 & \ddots & \ast & \ast  & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\hline
  &&& \circled{0} & \ast & \cdots & \ast & \ast\\
&&&& \ast & \cdots & \ast & \ast\\
&&&& 0 & \ddots & \ast & \ast \\
&&&& 0 & 0 & \ast & \ast\\
\end{array}
\right]
$
} \to 
\scalebox{0.9}{
$
H_m = \left[
\begin{array}{cccc|ccccc}
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
0 & \ddots & \ast & \ast  & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\hline
  &&& \circled{\ast} & \ast & \cdots & \ast & \ast\\
&&&& \ast & \cdots & \ast & \ast\\
&&&& 0 & \ddots & \ast & \ast \\
&&&& 0 & 0 & \ast & \ast\\
\end{array}
\right]
$
}
\]

We expect that this change does not lead to a significative change in the eigenvalues, in other words, the eigenvalues of $\widetilde{H}_m$ differ from the eigenvalues of $H_m$ by $\abs{h_{n+1, n}}$.
Formally, $\norm{\widetilde{H}_m - H_m} = h_{n+1, n}$.

\subsubsection{Better explanation}
The space $K_n(A, b) = \operatorname{span}(b,Ab,\dots,A^{n-1}b)$ contains the right ``features'' to represent the eigenvectors of $A$ with largest eigenvalues: if $A = V \Lambda V^{-1}$ is diagonalizable, then
\begin{equation}
  \begin{aligned}
    A^k b &= (V \Lambda \inv{V}) \cdots (V\Lambda \inv{V}) b\\
    &= V \Lambda^k \inv{V} b\\
    &= \begin{pmatrix}
    \\
    V^1 & V^2 & \cdots & V^m\\
    \\
    \end{pmatrix}\cdot \begin{pmatrix}
        {\lambda_1}^n\\
        & {\lambda_2}^n\\
        && \ddots\\
        &&&& {\lambda_m}^n\\
      \end{pmatrix}
      \begin{pmatrix}
        c_1\\
        c_2\\
        \vdots\\
        c_m
      \end{pmatrix}\\
      &= V^1 \lambda_1^k c_1 + V^2 \lambda_2^k c_2 + \cdots + V^m \lambda_m^k c_m 
  \end{aligned}
\end{equation}

where $c = \inv{V} b$.

$A^k b$ is a linear combination of the eigenvectors $V^i$ in which those with largest $\abs{\lambda_i}$ are ``more prominent'', in other words as $n$ increases the components involving the largest $\abs{\lambda_i}$s grow faster.

This also tells us that $\operatorname{span}(V^1, V^2, \ldots, V^m)$ (that are the eigenvectors associated to largest eigenvalues in modulus) ``represent well'' $K_m(A, b) = \operatorname{span}(b, Ab, \ldots, A^{m-1}b)$.

We cannot provide a formal proof of the convergence, because there are some counter examples.

\begin{example}
  Let $A \in M(m, \R)$, such that $A=\begin{pmatrix}
    0 & & & &1\\
    1 & 0\\
    & 1 & \ddots\\
    & & \ddots\\
    & & & 1 & 0
    \end{pmatrix}$

In this case the eigenvalues are $0$ until the last iteration. In the last step the eigenvalues become correct.

Notice that in this case the absolute values of the eigenvalues are the same, hence we are not able to do the trick explained above.
\end{example}

\begin{obs}[Matlab syntax]
  The command \texttt{[V, D] = eigs(A)} does not work on sparse matrices $A$. In this case we may run Arnoldi method and use the best values obtained by Arnoldi: \texttt{[V, D] = eigs(A, n)}, which computes approximations of the top-$n$ (largest in modulus) eigenvalues.

Notice that the Matlab ``implementation'' (the quotes are because the Arnoldi method \textit{de facto} is not implmented in Matlab) of the Arnoldi method uses some tricks to converge fast.

It is possible to use the command \texttt{eigs} and pass to it a $\lambda$-function which computes the matrix-vector product and this is useful when the matrix and the vector have a particular shape.
\end{obs}

We would like to understand how to compute some eigenvalues that are not the biggest.

\begin{lemma}
  Let $A \in M(m, \R)$ and let $(\lambda_1,\mathbf{v}_1),\dots,(\lambda_k,\mathbf{v}_k)$ be the eigenvalues/vectors of $A$. The following holds:

\begin{enumerate}
    \item $(\lambda_i+\alpha, \mathbf{v}_i)$ are eigenvalues/vectors of $A+\alpha I$;
    \item $(\frac{1}{\lambda_i}, \mathbf{v}_i)$ are eigenvalues/vectors of $A^{-1}$;
    \item $(\lambda_i^k, \mathbf{v}_i)$ are eigenvalues/vectors of $A^k$.
\end{enumerate}
\end{lemma}
\begin{proof}
  Let us omit the subscript $i$ to ease notation:~\\
  \begin{enumerate}
    \item $(A + \alpha I)\mathbf{v} = A \mathbf{v} + \alpha \mathbf{v} = \lambda \mathbf{v} + \alpha \mathbf{v} = (\lambda + \alpha) \mathbf{v}$;
    \item $(\inv{\lambda} \mathbf{v})$ is an eigenpair of $\inv{A}$. We need to check that $\inv{A} \mathbf{v} = \inv{\lambda} \mathbf{v}$. If we multiply by $\lambda A$ both sides: $\lambda \cancel{A} \cancel{\inv{A}} \mathbf{v} = \lambda A \inv{\lambda} \mathbf{v} \Leftrightarrow \lambda \mathbf{v} = \cancel{\lambda} \cancel{\inv{\lambda}} A \mathbf{v}$, which is true by definition of eigenvalue/vector of $A$;
    \item (by induction) $A^2 \mathbf{v} = A (A\mathbf{v}) = A \cdot \lambda \mathbf{v} = \lambda A v = \lambda \lambda v = {\lambda}^2 v $.
  \end{enumerate}
\end{proof}

This lemma gives us the chance to use Arnoldi algorithm to compute the eigenvalues of a slightly modified matrix $B = \inv{(A -\mu I)}$.
If $(\lambda, v)$ is an eigenpair of $A$, then $( \inv{\lambda - \mu}, v)$ is an eigenpair of $B$.

When is that $(\lambda - \inv{\mu})$ is large? When $\lambda$ and $\mu$ are close.

If $A$ has eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_m$, then the eigenvalues of $B$ are $\nu_1 = \frac{1}{\lambda_1 - \mu}, \nu_2 = \frac{1}{\lambda_2 - \mu}, \ldots, \nu_m = \frac{1}{\lambda_m - \mu}$.

The problem is that such matrix $B$ is not sparse when $A$ is sparse.

We perform a trick to overcome this issue: we pass the function \texttt{eig} a $\lambda$-function that computes the product $Bz = \inv{A - \mu I} z$ without computing $B$ explicitly.
The idea is to use factorizations: $A - \mu I = LU$, then $Bz = \inv{U} (\inv{L}z)$, that we can compute by back-substitution.

\begin{minted}{matlab}
% computes 5 eigenvalues closest to mu=2
>> fl = eigs(A, 5, mu);
>> fl = eigs(A, 5, 'SM'); %smallest magnitude
>> fl = eigs(A, 5, 'LM'); %largest magnitude
\end{minted}

Notice that \texttt{eigs(A, 6, 1)} needs to factorize $A - 1 \cdot I = LU$, which might induce a lot of fill-in.

As final observation, the equivalents to Matlab's \texttt{eigs} function are \texttt{scipy.linalgs.eigs} for Python and \texttt{arpack} for C/C++ and Fortran.
\end{document}
