%chktex-file 36
%chktex-file 23
%chktex-file 10
%chktex-file 17
%chktex-file 9
%chktex-file 44
\documentclass[computationalMathematics.tex]{subfiles}


\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\chapter{20th of September 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\par In the following chapters we will cover the part of Numerical Analysis of the course of Computational Mathematics for Data Learning and Analytics. This part of the course is held by Professor Federico Poloni. The content of these chapters is based primarily on the material provided by the professor on the e-learning portal. In addition to that material, the content has been written also based on the content of the course Linear Algebra held by Prof. Gilbert Strang from MIT. There is a nice portal called MIT OpenCourseWare where you can find a lot of recorded lectures from MIT. Linear algebra course is just super duper fantastic.
\par During the course, some Matlab code will be covered. We do not plan to include a deep introduction to the Matlab language chiefly for two reasons:
\begin{enumerate}
    \item there is already plenty of stuff online,
    \item it is too time consuming to create one.
\end{enumerate}

\section{A brief journey in Linear Algebra}
\label{section:MAT08/1-introduction}
\par What is linear algebra? Linear algebra solves systems of linear equations. Period. But there is a whole world there in. So better start right now.
\par What is a system of equations? Well, simply put, it is a bunch of linear equations, say $m$, each of which is characterized by at most $n$ unknowns, i.e. variables. Since there are two dimensions involved here, that is $m$ and $n$, there are actually two points of view to look at this system. We can look at it from the row perspective and from the column perspective. Let us go through an example to show this.
\begin{example}
    Suppose we are given the following system of two linear equations with two unknowns:
    \[
        \begin{cases}
            &-4x + 2y = 2 \\
            &3x + 5y = -3    
        \end{cases}
    \]
    We can write this system of linear equations in the following matrix form:
    \[
        \begin{bmatrix}
            -4 & 2 \\
            3 & 5
        \end{bmatrix}
        \begin{bmatrix}
            x \\
            y
        \end{bmatrix}
        =
        \begin{bmatrix}
            2 \\
            -3
        \end{bmatrix}
    \]
    Usually the coefficient matrix is indicated with $A$, the vector of unknowns is indicated with $\vect{x}$, and the vector of coefficients on the right hand side is usually indicated with $\vect{b}$.
    \par The row perspective is the one that we are mostly used to deal with. Each equation in this example represents a line in the Cartesian plane and the problem requires to find the point (if it exists) where the two lines meet (see figure \ref{figure:MAT08/1-introduction/introduction/example_row_perspective}).
    \par The column perspective is a bit different and may be completely new to you. Look at this mind blowing thing:
    \[
        x \begin{bmatrix} -4 \\ 3 \end{bmatrix} + y \begin{bmatrix} 2 \\ 5 \end{bmatrix} = \begin{bmatrix} 2 \\ -3 \end{bmatrix}
    \]
    Why is column perspective interesting? Because now we can look at these columns of $A$ as vectors in a column space. We are eager to find the right amount (unknowns) of each of these column vectors that produce the vector $\vect{b}$ (see figure \ref{figure:MAT08/1-introduction/introduction/example_column_perspective}).
    \par Linear combination is the fundamental operation of the whole course. Besides the question of what is the linear combination of the columns of $A$ that produce $\vect{b}$, we will also ask ourselves what are all the possible vectors $\vect{b}$ that we can obtain with some linear combination of the columns of $A$? Or what are all the possible linear combinations of the columns of $A$ that give a certain $\vect{b}$?
\end{example}

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
		    \begin{axis}[xlabel=$x$, ylabel=$y$, xmin=-10, xmax=10, ymin=-10, ymax=10, axis lines=center, axis equal]
		        \addplot[domain=-10:10, color=blue,]{2*x+1};
		        \addplot[domain=-10:10, color=blue,]{1/5*(-3*x-3)};
		    \end{axis}
		\end{tikzpicture}
	\end{center}
\caption{Linear system}\label{figure:MAT08/1-introduction/introduction/example_row_perspective}
\end{figure}

\begin{figure}
	\begin{center}
	    \begin{tikzpicture}[>=stealth, scale=0.5, line cap=round, bullet/.style={circle,inner sep=1.5pt,fill}]
	        \draw[->] (-9,0) -- (9,0) node[right]{$x$};
	        \draw[->] (0,-7) -- (0,7) node[above]{$y$};
	
	        \draw foreach \X in {2,-4,2}
	        {(\X,0.1) -- ++ (0,-0.2) node[below]{$\X$}};
	
	        \draw foreach \Y in {-3,3,5}
	        {(0.1,\Y) -- ++ (-0.2,0) node[left]{$\Y$}};
	
	        \foreach \X [count=\Y] in {(2,-3),(-4,3),(2,5)} 
	        {\path  \X node(n\Y)[bullet,label=right:{$\X$}]{};
	        \draw[thick,->]  (0,0) -- (n\Y);}
	    \end{tikzpicture}
	\end{center}
\caption{Linear system 2}\label{figure:MAT08/1-introduction/introduction/example_column_perspective}
\end{figure}
    
\par In the previous example we have encountered a bunch of new terms. Let us now define them rigorously. We saw that the generic system of linear equations can be written as:
\[
    A \vect{x} = \vect{b}
\]
Note that we are actually abusing terminology here. Even though it is not that important, bare in mind that these are actually \textbf{affine equations}. The linear equations are the equations that go through the origin, while affine need not to.
\par We have seen that there is a row perspective and a column perspective. These perspectives are actually two different Euclidean spaces. The first is called \emph{row space} and the second one is the \emph{column space}. Note how these correspond directly to the rows and columns of $A$ respectively. When the matrix is a square matrix the dimension of the two spaces is the same (if certain properties hold). But when we have $m \times n$ matrices, then the two spaces have different dimensions (if the same properties hold). Remember all these spaces, because it is important! Especially the column space.
\par Clearly, two equations in two unknowns is a very basic case. Let us see another basic case before moving to definitions.

\begin{example}
    Let us now take three equations in three unknowns.
    \todo[inline]{TODO: I need a tikz magician here...}
\end{example}

\par Now that we have said that the linear system of equations can also be seen as a bunch of vectors in the $n$ dimensional space, we can think of solving a linear system of equations as the problem of finding a linear combination of columns of the matrix $A$ to obtain the vector $\vect{b}$. Thus, given a vector $\vect{b}$, how can we obtain it (if it is possible at all) as a linear combination of the columns of $A$? If you think a bit, the vector $\vect{b}$ is yet another vector in the $n$ dimensional space. Think of the columns of $A$ as directions, i.e. $n$ directions. You are in the origin of the space, and you want to get to the point pointed by $\vect{b}$. You may move in each of $n$ directions only once and in every direction you can choose how much you want to move. You may move backwards (corresponding to negative coefficient in the linear combination) or forward (corresponding to positive coefficient in the linear combination). A zero coefficient means that you do not move in the corresponding direction. The problem is: given these $n$ direction, can you get from the origin to the point $\vect{b}$ whatever $\vect{b}$ you are given? This basically means that these $n$ direction must somehow ``fill'' the whole space, that is for every point $b$ there must be a linear combination of directions that bring you from the origin to $\vect{b}$. So, is there a way to know whether a given set of columns is ``sufficiently powerful'' to be able to generate all the ``treasury maps''? Here comes the concept of linear independence.
\par If the columns of $A$ are \textbf{linearly independent}, then every vector $\vect{b}$ on the right hand side of the equation $A \vect{x} = \vect{b}$ can be generated with some combination of columns of $A$. What does it mean to be linearly independent? It means that when you take one of the $n$ columns, you cannot find a linear combination of the remaining $n-1$ columns of $A$ that can generate the one that you have picked. Geometrically speaking, $n-1$ columns in an $n$ dimensional space generate a so called subspace. In an $n$ dimensional space there are $n$ types of subspaces: a zero subspace, the subspaces of 1 dimension, the subspaces of 2 dimensions, ..., the subspaces of $n-1$ dimensions, and finally a subspace of $n$ dimensions. Note singularity and plurality in the previous sentence. There is only one subspace of zero dimension, namely the origin. There is only one subspace of $n$ dimensions in an $n$ dimensional space, the space it self. The remaining subspaces are all of dimensions $1$ to $n-1$. A subspace of dimension $1$ is a line. A subspace of dimension $2$ is a plane. A subspace of dimension $3$ is the three dimensional space. A subspace of dimension $4$... well you picture it, I'm not able. So $n-1$ vectors in the $n$ dimensional space generate a subspace (think of a plane subspace in the three dimensional space). If the vector that we have picked from $A$ is located on the very same plane generated by the $n-1$ vectors, then there is no way that these $n-1$ vectors can generate a vector that is outside of this plane, because there is no direction that points outside of the plane itself. The vector that we have picked and that is on the plane is then linearly dependent from the $n-1$ remaining ones. Clearly, we may have more than one vectors that are linearly dependent from the rest in $A$. The obvious question now is what is the subspace that the columns of $A$ generate? In other words what is the maximum number of linearly independent columns of $A$? In linear algebra this is the so called rank of the matrix. When the rank is $n$ then the matrix is said to be full rank. Given this definition, the rank of the matrix then satisfy the following box constraint:
\[
    0 \leq \texttt{rank}(A) \leq n
\]
for some $A \in M(n,\R)$. Since we have two dimensions, namely rows and columns, each of which generates its own space, namely row space and column space, you may legitimately wonder if these are the same. Well, in case of square matrices $n \times n$ the two are the same. So the rank of row space is the same of the column space. You want a proof? Search rank of a matrix on Wikipedia for three different proofs.
\par Let us now turn back to the rows. The question for you is: what happens if we do instead:
\[
    \tr{\vect{x}} A = \tr{\vect{b}}
\]
where $\tr{\vect{x}}$ is a row vector, so we have:
\[
    \begin{bmatrix}
        x_1 & x_2 & \dots & x_n
    \end{bmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \dots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix}
    = 
    \begin{bmatrix}
        b_1 & b_2 & \dots & b_n
    \end{bmatrix}
\]
The row vector $b$ is the linear combination of the rows of $A$ where the weights are given by the vector $x$. Namely:
\[
    x_1
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n}
    \end{bmatrix}
    +
    \dots
    +
    x_n
    \begin{bmatrix}
        a_{n1} & a_{n2} & \dots & a_{nn}
    \end{bmatrix}
    =
    \begin{bmatrix}
        b_1 & b_2 & \dots & b_n
    \end{bmatrix}
\]
Everything we said about columns, linear independence, rank, etc, holds for rows and row space of the matrix $A$. Basically, when we do operations from the left of the matrix $A$ then we are operating on rows of the matrix, when instead we are operating from the right then we are performing operations over columns of $A$.
\par Now that we know hat linearly independent columns of $A$ can generate all the vectors in the corresponding $n$ dimensional column space, the question is how do we generate it? In other words, given a vector $\vect{b}$ how do we find the coefficients of the linear combination that generate it? Note the linear combination and not a linear combination. There is only one linear combination that generates it. Why? Picture the row space. Each equation in the linear system generates a subspace of dimension $n-1$. So in three dimensional space the equations represent planes. In case of a system of $n$ linear equations in $n$ unknowns, we have $n$ subspaces of dimension $n-1$ in the $n$ dimensional row space. So if these $n$ subspaces are linearly independent, then it must be the case that there is only one point that touches them all when they intersect. If there is more than one point that is in the intersection of all of them, then the matrix $A$ is not full rank. Suppose then that the columns (or rows) are linearly independent, how do we find the coefficients? Note this basic math:
\[
    a x = b \iff \frac{1}{a} x = \frac{b}{a} \iff x = a^{-1} b
\]
where every term is a simple scalar. So to get $x$ we must multiply $b$ with the inverse of $a$. The same happens with the matrices and vectors. We need something called \textbf{inverse matrix} of $A$, indicated with $A^{-1}$:
\[
    \vect{x} = A^{-1} \vect{b}
\]
The matrix $A$ is invertible only if it has full column rank. This is why the linear independence is so central to the whole linear algebra. Note that only square matrices may have the inverse matrix. Obviously, the inverse matrix is unique. And moreover the inverse of the inverse is the initial matrix, namely:
\[
    (A^{-1})^{-1} = A
\]
\par Cool man. But how the heck are we going to find the inverse? There are many (not \textit{that} many) algorithms that can be used for matrix inversion. But one of the standard one is the Gauss-Jordan Elimination algorithm. Are we going to cover it here? Why not? It is one of the most used algorithms in linear algebra so let us give it some love. But before doing this algorithm, we still need one ingredient: matrix multiplications.

\subsection{Matrix multiplications: four flavors plus one of looking at it}
\par Matrix multiplication is another of the most important operations that you can think of when you think of linear algebra. In here, we are going to cover briefly four different ways of looking at this important operation.
\par We start with the basic definition that you probably have seen if you have ever done some linear algebra course. The classical definition is:
\[
    \begin{bmatrix}
        a_{11} & a_{12} & \dots & a_{1n} \\
        a_{21} & a_{22} & \dots & a_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        a_{n1} & a_{n2} & \dots & a_{nn} \\
    \end{bmatrix}
    \begin{bmatrix}
        b_{11} & b_{12} & \dots & b_{1n} \\
        b_{21} & b_{22} & \dots & b_{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        b_{n1} & b_{n2} & \dots & b_{nn} \\
    \end{bmatrix}
    =
    \begin{bmatrix}
        c_{11} & c_{12} & \dots & c_{1n} \\
        c_{21} & c_{22} & \dots & c_{2n} \\
        \vdots & \vdots & \dots & \vdots \\
        c_{n1} & c_{n2} & \dots & c_{nn} \\
    \end{bmatrix}
\]
where:
\[
    c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}
\]
\par But as we all know, there is more than one of everything. Another way to look at matrix-matrix multiplication is the following. We have seen that matrix vector multiplication is basically a linear operation that send a certain vector $\vect{x}$ into another vector $\vect{y}$. Thus, $A \vect{x} = \vect{y}$ means that $\vect{x}$ is mapped to $\vect{y}$ through $A$. When it comes to the matrix matrix multiplication, i.e. $A B = C$, the same story happens. It is the columns of $B$ that are mapped to columns of $C$. Each column $i$ of $B$ (denoted as $B^i$) is mapped to the corresponding column $i$ of $C$. Similarly, we could think of all of this from the row space perspective. The rows of $A$ are mapped to the corresponding rows of $C$ through $B$. So these are the additional two ways to look at matrix matrix multiplication. What about the fourth one?
\par The fourth way to look at matrix-matrix multiplication is a bit more mind blowing. In the first way, we have computed the dot products between rows of $A$ and columns of $B$. But nobody prevents us from doing the contrary. What if we used the columns of $A$ and the rows of $B$:
\[
  \begin{bmatrix}
    a_{1i} \\
    a_{2i} \\
    \vdots \\
    a_{ni} \\
\end{bmatrix}
\begin{bmatrix}
    b_{i1} & b_{i2} & \dots & b_{in} \\
\end{bmatrix}
\]
What do we get for each $i=1, \ldots, n$? We get an $n \times n$ matrix. So the resulting matrix $C$ is the summation of $n$ different matrices of size $n \times n$. 

\section{Formal definitions}
At this point, we are ready to introduce the formality of linear algebra, that we will use throughout the whole course:
\begin{itemize}
    \item \textbf{Vector-Scalar product}:\\
      Let $\vect{x} \in \R^n$ and $\lambda \in \R$ we call \textbf{multiple} of vector $\vect{x}$ the following vector:
      \[
        \lambda \vect{x} = \vect{x} \lambda =
        \begin{pmatrix}
        \lambda x_1\\
        \vdots\\
        \lambda x_n\\
      \end{pmatrix}
      \]
    
    \item \textbf{Vector-Vector product}:\\
      Let $\vect{x}, \vect{y} \in \R^n$, the dot product between these two vectors is defined as $\tr{\vect{x}} \vect{y} = \sum\limits_{i=1}^n x_i y_i$. Note in particular that $\tr{\vect{x}}\vect{y} \in \R$. In other words, the dot product produces a scalar.
     
     \item \textbf{Scalar-Matrix product}:\\
      Let $A \in M(n, m, R)$ and $\lambda \in \R$ we call the \textbf{scalar-matrix product} the following:
      \[
        \lambda A = A \lambda = 
        \begin{pmatrix}
        \lambda A_{11} & \lambda A_{12} & \cdots & \lambda A_{1m}\\
        \lambda A_{21} & \lambda A_{22} & \cdots & \lambda A_{2m}\\
        \vdots  & \vdots & \ddots & \vdots\\
        \lambda A_{n1} & \lambda A_{n2} & \cdots & \lambda A_{nm}
        \end{pmatrix}
      \]
     
     \item \textbf{Matrix-Vector product}:\\
        Given a matrix $A \in M(n, m, \R)$ and a vector $\vect{v} \in \R^m$ the \textbf{matrix-vector product} $A \vect{v} = \vect{w} \in \R^n$ is computed as follows:
        \[
          \vect{w} = A\vect{v} = \begin{pmatrix}
          A_1 \vect{v}\\
          A_2 \vect{v}\\
          \vdots\\
          A_m \vect{v}\\
          \end{pmatrix}, \, 
          w_i = \sum\limits_{j=1}^m A_{ij} v_j
        \]
        This is the simple way, just a row-by-column vector product, the computational complexity of this operation is $O(n^2)$.\\
        The smart way to compute it: \textbf{linear combinations} of columns of A, e.g.:
        \[
            \begin{pmatrix}
                      A_{11} & A_{12} & A_{13}\\
                      A_{21} & A_{22} & A_{23}\\
                      A_{31} & A_{32} & A_{33}\\
                      A_{41} & A_{42} & A_{43}
            \end{pmatrix}
            \begin{pmatrix}
                      v_1\\
                      v_2\\
                      v_3
            \end{pmatrix} =
            \begin{pmatrix}
                      w_1\\
                      w_2\\
                      w_3
            \end{pmatrix}
          \]
        with \textbf{linear combinations} we have:
        \[
        \begin{pmatrix}
                A_{11}\\
                A_{21}\\
                A_{31}\\
                A_{41}
        \end{pmatrix}v_1 +
        \begin{pmatrix}
                A_{12}\\
                A_{22}\\
                A_{32}\\
                A_{42}
        \end{pmatrix}v_2 + 
        \begin{pmatrix}
                A_{13}\\
                A_{23}\\
                A_{33}\\
                A_{43}
        \end{pmatrix}v_3 =
        \begin{pmatrix}
                  w_1\\
                  w_2\\
                  w_3\\
                  w_4
        \end{pmatrix}
      \]
  \item \textbf{Matrix-Matrix Product}:\\
        Given two matrices $A \in M(n, m, R)$ and $B \in M(m, k, R)$ we call \textbf{matrix-matrix product} the following:
        $C=AB$ such that $C_{ij} = A_i B^j$, where $\tr{A_i} \in \R^m$ is the $i$-th row of $A$, $B^i$ is the $i$-th column of $B$ ($B^i \in \R^m$) and $C \in M(n, k, \R)$.
        Notice that this product is \textbf{not commutative}: $AB \neq BA$ might not even make sense dimension-wise.
        \term{
        	In this notes we refer to the columns to a generic matrix $A$ as $A^1$ for the first column, $A^2$ for the second column and so on and so forth.
        	
        	Conversely, we represent the rows of a matrix $A$ as $A_i$.
        }
		As long as the complexity is concerned, multiplying two matrices $m \times n$ and $n \times k$ requires $O(m n k)$ floating point operations (flops). Forget about fancier algorithms (e.g. Strassen)
		        
		\begin{myframe}{\bf Order of operations}
		  Usual algebra properties hold, e.g.: $A(B + C) = AB + AC, A(BC) = (AB)C, \ldots$.
		  
		  Parenthesization matters a lot: if $A, B \in M(n, \R), \, \vect{v} \in \R^n$, then $(AB)\vect{v}$ costs $O(n^3)$, but $A(B\vect{v})$ costs $O(n^2)$.
		  Programming languages usually do not rearrange parentheses to help.
		\end{myframe}

     \item \textbf{Image} of a matrix $A$ ($\text{Im}(A)$):  the set of vectors that can    be obtained multiplying $A$ by any vector in the domain of $A$.

     \item \textbf{Kernel} of a matrix $A$ ($\ker(A)$):  the set of vectors $\vect{w}$ in its domain such that $A\vect{w}=\vect{0}$.

     \item Given a matrix $A \in M(n, \R)$ we call \textbf{inverse} of $A$ the matrix $\inv{A}$ such that:
       \[  
       \inv{A} A = A \inv{A} = I_{n} =
         \underbrace{\begin{pmatrix}
                1\\
                & 1\\
                & & 1\\
                & & & \ddots\\
                & & & & 1
         \end{pmatrix}}_{n}
         \]
         The \textbf{inverse of a product} (shoe-sock identity) is $\inv{(A B)} = \inv{B} \inv{A}$. Notice that this identity holds only for square matrices.

     \item The \textbf{transpose} of a matrix $A \in M(n, m, \R)$ is $\tr{A}$ such that $\tr{A}_{ij} = A_{ji}$. The \textbf{transpose of a product} (shoe-sock identity) is $\tr{(A B)} = \tr{B} \tr{A}$. (This identity holds for square and rectangular matrices)

\end{itemize} 

\begin{definition}
  \textbf{General linear group (GL)}: the general linear group of degree n is the set of $n\times n$ invertible matrices, together with the operation of ordinary matrix multiplication
\end{definition}

\begin{proposition}
  Let $A \in GL(n, \R)$ (aka $A$ is a real square matrix of size $n$ and invertible), $B, C \in M(n, m, \R)$ and we have the equality $AB = AC$.
  If there is a matrix $M$ such that $MA = I$, the following holds
  \[
    (MA)B = (MA)C \Longleftrightarrow B=C, \,\, M = A^{-1}
  \]
\end{proposition}
\noindent
In general, $AB = AC$ does not imply $B = C$; it holds only when $A$ is invertible.

\term{
  \[
    \vect{v} = \begin{pmatrix}
        4\\
        5\\
        6
    \end{pmatrix}, \vspace{0.5cm}
    \tr{\vect{v}} = \begin{pmatrix}
        4 & 5 & 6
    \end{pmatrix}
  \]
          $\vect{v}$ is a column vector in $\R^3$ (or a matrix in $M(3, 1, \R)$) and $\tr{\vect{v}}$ is a row vector (or a matrix in $M(1, 3, \R)$).
}

\begin{definition}[Basis]
  We call \textbf{basis} a set $\mathcal{B}$ of elements (vectors) in a vector space $V$ if every element of $V$ may be written in a unique way as a (finite) linear combination of elements of $\mathcal{B}$.
  The coefficients of this linear combination are referred to as components or coordinates on $\mathcal{B}$ of the vector.
  The elements of a basis are called \textbf{basis vectors}.
\end{definition}

\begin{definition}[Canonical basis]
  We term \textbf{canonical basis} of a vector space $\R^n$ the basis made of all the column of the $n \times n$ identity matrix $I_n \in M(n, \R)$.
\end{definition}

\begin{example}
  In $\R^4$ the \textbf{canonical basis} is $ \mathcal{B} = \{ \vect{e_1}, \vect{e_2}, \vect{e_3}, \vect{e_4}\}$ such that
  \[
  \vect{e_1} = \begin{pmatrix} 1\\ 0\\ 0\\ 0 \end{pmatrix},~
  \vect{e_2} = \begin{pmatrix} 0\\ 1\\ 0\\ 0 \end{pmatrix},~
  \vect{e_3} = \begin{pmatrix} 0\\ 0\\ 1\\ 0 \end{pmatrix},~
  \vect{e_4} = \begin{pmatrix} 0\\ 0\\ 0\\ 1 \end{pmatrix}
\]
  and each vector $\vect{w} \in R^4$ can be written as $\vect{w} = w_1\vect{e_1} + w_2\vect{e_2} + w_3\vect{e_3} + w_4\vect{e_4}$.
\end{example}

\mantra{
The powerful idea behind linear algebra: many relations are true
regardless of the basis we use. E.g. $\vect{w}$, $\vect{v}$ and $\vect{w} + \vect{v}$ in two different bases.}

\begin{example}
	Let us take two vectors $\vect{v}, \vect{w}$ and let us write those with respect to two different bases $\mathcal{B}_1$ and $\mathcal{B}_2$:
	\[
		\vect{w}_{\mathcal{B}_1} = \begin{pmatrix} 2\\ 4 \end{pmatrix}
		\vect{v}_{\mathcal{B}_1} = \begin{pmatrix} 2\\ 1 \end{pmatrix}
	\]
	and 
	\[
	\vect{w}_{\mathcal{B}_2} = \begin{pmatrix} -1\\ 3 \end{pmatrix}
	\vect{v}_{\mathcal{B}_2} = \begin{pmatrix} 0.5\\ 1.5 \end{pmatrix}
	\]
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.45]{pics/20sett/3.png}
    \caption{How a change of basis reflects on the space.}\label{fig:20sett1}
\end{figure}
\end{example}

\begin{definition}[Triangular matrix]
  Let $A \in M(n, \R)$. We term $A$ \textbf{upper triangular} if ${(A)}_{ij} =0$ for each $i<j$.
  Conversely, we term $A$ \textbf{lower triangular} if ${(A)}_{ij} =0$ for each $j<i$.
  The set of all triangular $n \times n$ real matrices is a group and it is denoted as $T(n, \R)$.
\end{definition}

\begin{definition}[Diagonal matrix]
  Let $A \in M(n, \R)$. We term $A$ \textbf{diagonal} if ${(A)}_{ij} = 0$ for each $i \neq j$.
  The set of all diagonal $n \times n$ real matrices is a group and it is denoted as $D(n, \R)$.
\end{definition}

\begin{definition}[Symmetric matrix]
  Let $A \in M(n, \R)$. We term $A$ \textbf{symmetric} if ${(A)}_{ij} = {(A)}_{ji}$ for each $i,j = 1, \ldots, n$.
  The set of all symmetric $n \times n$ real matrices is a group and it is denoted as $S(n, \R)$.
\end{definition}

\begin{proposition}
	$A \in S(n, \R) \iff \tr{D} = D$
\end{proposition}

\section{Solving Linear Systems}
The objective of this course, for the part concerning numerical methods, is solving linear systems efficiently.

\begin{definition}[Linear system]
  Let $A \in M(n, m, \R)$, $\vect{b} \in \R^n$ and $\vect{x} \in \R^m$. We term \textbf{linear system} the following:

  \[
A\vect{x} = \vect{b}
  \]
\end{definition}

\noindent Our goal is to \emph{approximate} such vector $\vect{x}$, hence resulting in solving a minimum problem:

\[
  \min\limits \norm{A\vect{x} - \vect{b}}
\]

If we have a square and invertible matrix $A \in GL(n, \R)$ solving a linear system means: find those coordinates $x_1, \dots , x_n$ needed to write $\vect{b}$ as a linear combination of the columns of (square) $A$ and in this case, the solution is given by: $\vect{x} = \inv{A}\vect{b}$. \\
\textbf{Warning}: this is not the best way to solve a linear system on a computer!\\

\syntax{Notice that the machine precision is $10^{-16}$, so we should pay attention when making computations, since we may incur in some error (proportional to the size of the operands).

In Matlab a matrix is written as \texttt{A=[1, 2, 3; 4, 5, 6];}, where \texttt{[1, 2, 3]} is the first row of the matrix A.

The transpose of a matrix or a vector is denoted by \texttt{A'}.

The inverse of a square matrix is denoted by \texttt{inv(A)}.

If we are interested in only a part of our matrix \texttt{A} we may write \texttt{A(1:2, 1:3)} and obtain only the rows of \texttt{A} that go from $1$ to $2$ and those columns from $1$ to $3$.

Notice that in Matlab both vector and matrices are $1$-based.
}

\begin{definition}[Block multiplications]
  Let $A \in M(n,m, \R)$ and let $B \in M(m, k, \R)$. We can compute the result of a block of the matrix $AB$ as the product of the two blocks in $A$ and $B$ in the corresponding position.
\end{definition}

\begin{obs}
  When computing a matrix product, we get the same result if we use the row-by-column rule \emph{block-wise}.
  \[
  \left(\begin{array}{ccc|c|cc}
    \tikzmarkin[hor=style orange]{row 1} \times & \times & \times & \times & \times & \times\\
    \times & \times & \times & \times & \times & \times\tikzmarkend{row 1}\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
  \end{array}\right)
  \cdot
 \left(\begin{array}{ccc|cc|c}
   \tikzmarkin[ver=style green]{col 1} \times  & \times & \times & \times & \times & \times\\
    \times & \times & \times & \times & \times & \times\\
    \times & \times & \times & \times & \times & \times\\
    \hline
    \times & \times & \times & \times & \times & \times\\
    \hline
    \times & \times & \times & \times & \times & \times\\
    \times  & \times & \times \tikzmarkend{col 1}& \times & \times & \times\\
  \end{array}\right)
  =
 \left(\begin{array}{ccc|cc|c}
    \tikzmarkin[hor=style cyan]{row 2} \times & \times & \times & \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 2}& \times & \times & \times\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
  \end{array}\right)
\]

\[
  \scalemath{0.8}{ \left(\begin{array}{ccc}
    \tikzmarkin[hor=style orange]{row 3} \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 3}\\
  \end{array}\right)
  \cdot
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style green]{row 4} \times & \times & \times\\
    \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 4}\\
  \end{array}\right)
  +
  \left(\begin{array}{c}
    \tikzmarkin[hor=style orange]{row 5} \times\\
    \times \tikzmarkend{row 5}\\
  \end{array}\right)
  \cdot
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style green]{row 6} \times & \times & \times \tikzmarkend{row 6}\\
  \end{array}\right)
  +
  \left(\begin{array}{cc}
    \tikzmarkin[hor=style orange]{row 7} \times & \times \\
    \times & \times \tikzmarkend{row 7}\\
  \end{array}\right)
  \cdot
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style green]{row 8} \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 8}\\
  \end{array}\right)
  =
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style cyan]{row 9} \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 9}\\
  \end{array}\right)
}\]
\end{obs}

Notice that block operations usually give better performance: one matrix-matrix product performs faster than n matrix-vector products (even if they have the same number of flops).
This is one of the reasons why library calls usually perform better
than hand-coded loops (Blas/Lapack).

\begin{proposition}[Block triangular matrices]
  Let $M \in M(n, m, \R)$ and $B \in M(m, k, \R)$ such that they are \textbf{block triangular}.
  Their product is a block triangular matrix as well. In other words, block triangular matrices are closed under products:
  \[
  M B 
  = \begin{pmatrix}
    A & B\\
    0 & C
  \end{pmatrix}
  \begin{pmatrix}
    D & E\\
    0 & F
  \end{pmatrix}
  = 
  \begin{pmatrix}
    AD & AE + BF\\
    0 & CF
  \end{pmatrix}
\]
\end{proposition}

\begin{proposition}[Properties of block triangular matrices]~\\
    Let M be a block triangular matrix, where all the blocks on the diagonal are square
   \[
     M 
      = \begin{pmatrix}
        A_{11}  & A_{12} & \cdots & A_{1n}\\
        0       & A_{22} & \cdots & A_{2n}\\
        \vdots  &\ddots & \ddots & \vdots\\
        0       & \cdots & 0 & A_{nn}
      \end{pmatrix}
    \]
    \begin{enumerate}
      \item A block triangular matrix is invertible iff all diagonal blocks $A_{ii}$ are invertible;
      \item The eigenvalues\footnote{The concept of eigenvalue is exposed in \Cref{def:eigenvalues}.} of a block triangular matrix are the union of the eigenvalues of each $A_{ii}$ block;
      \item Let $M \in GL(n, \R)$ such that $M= 
        \begin{pmatrix}
          A & B\\
          0 & C\\
        \end{pmatrix}$ 
       the inverse of $M$ is 
        
        $\inv{M} = 
        \begin{pmatrix}
          \inv{A} & -\inv{A}B\inv{C}\\
          0 & \inv{C}
        \end{pmatrix}$.
      \item The product of two block (upper/lower) triangular matrices
(with compatible block sizes) is still block triangular
    \end{enumerate}
  \end{proposition}

Why are we interested in block triangular matrices? They depict a situation as shown in \Cref{fig:20sett3}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{pics/20sett/1.png}
    \caption{The adjacency matrix of a biparted graph has $0$s in its bottom left part (Matlab syntax \texttt{A[p+1:n; 1:p]=0}), which means that the edges from a connected component and the other are in one direction only.}\label{fig:20sett3}
\end{figure}

\mantra{
Matrix structures matter.
Block triangular linear systems have a cheaper solution than general systems as shown in \Cref{exp1:20sett}.}

\begin{example}\label{exp1:20sett}
Let us take a $2\times2$ block triangular linear system\\
  \[
\begin{pmatrix}
    A & B\\
    0 & C
\end{pmatrix}
\cdot
\begin{pmatrix}
    \vect{x} \\
    \vect{y} 
\end{pmatrix} = 
\begin{pmatrix}
    \vect{e} \\
    \vect{f} 
\end{pmatrix}
\]
(Again, diagonal blocks are square and all dimensions are
compatible.)
  \[
\begin{pmatrix}
    A\vect{x} + B\vect{y}\\
    C\vect{y}
\end{pmatrix} = 
\begin{pmatrix}
    \vect{e} \\
    \vect{f} 
\end{pmatrix}\implies \vect{y} = \inv{C}\vect{f}, \vect{x} = \inv{A}(\vect{e} - B\inv{C}\vect{f})
\]
  \[
\begin{pmatrix}
    A & B\\
    0 & C
\end{pmatrix}^{-1} = 
\begin{pmatrix}
          \inv{A} & -\inv{A}B\inv{C}\\
          0 & \inv{C}
        \end{pmatrix}
      \]
Informal idea: we can start solving from the variables multiplied by C.
\end{example}

\section{Orthogonality}

\begin{definition}[Norms]\label{def:20sett_norm}
  Let $\vect{x} \in \R^n$.
  We ``measure'' its magnitude using so-called ``norms''.
  \begin{description}
    \item[{\sc Euclidean:}] $\norm{\vect{x}}_2 = \tr{\vect{x}} \vect{x} = \sqrt{\sum\limits_{i=1}^n {x_i}^2}$;
    \item[{\sc Norm 1:}] $\norm{\vect{x}}_1= \sum\limits_{i=1}^n \abs{x_i}$;
    \item[{\sc $p$-Norm:}] $\abs{\vect{x}}_p = {\bigg(\sum\limits_{i=1}^n \abs{x_i}^p\bigg)}^{1/p}$;
    \item[{\sc $0$-Norm:}] $\norm{\vect{x}}_0 = \abs{\{i~:~\abs{x_i} > 0\}}$, which accounts for $1- \#$of $0$ entries;
    \item[{\sc $\infty$-Norm:}]$\norm{\vect{x}}_\infty = \max\limits_{i=1, \ldots, n} \abs{x_i}$.
  \end{description}
\end{definition}

From now on in this part of the course, if not explicitly specified, we will refer to norm-2 only.

\begin{definition}[Scalar product]
  Let $\vect{v}, \vect{w} \in \R^n$ we term \textbf{standard scalar product} between $\vect{v}$ and $\vect{w}$ the real number $\tr{\vect{v}} \vect{w} = \sum\limits_{i=1}^n v_i w_i$.
\end{definition}

\begin{definition}[Orthogonal vectors]
  Let $\vect{v}, \vect{w} \in \R^n$.
  We say that $\vect{v}$ is \textbf{orthogonal} to $\vect{w}$ (in symbols $\vect{v} \bot \vect{w}$) if their scalar product is zero.
  
  Formally, $\tr{\vect{v}} \vect{w} = 0$.
\end{definition}

\begin{definition}[Orthogonal matrix]
  Let $A \in M(n, \R)$ a square matrix.
  We term $A$ \textbf{orthogonal} if $\tr{A} A = A \tr{A} = I_{n}$ where $I_n$ is the identity matrix of size $n$ ($1$ on the diagonal, $0$ elsewhere) or equivalently if $\inv{A} = \tr{A}$.

  The set of all orthogonal matrices in $M(n, \R)$ is a group and it called orthogonal group and denotes as $O(n, \R)$.
\end{definition}

\begin{proposition}\label{fact:20sett}
  Let $A \in O(n, \R)$, $\forall \vect{x} \in \R^n$ we have that $\norm{A\vect{x}} = \norm{\vect{x}}$. 
\end{proposition}

\begin{proof}
	\begin{enumerate}
		\item 
		\[
			\norm{A\vect{x}} = \sqrt{\tr{(A\vect{x})} A\vect{}} = \sqrt{\tr{\vect{x}}\tr{A} A \vect{x}} = \sqrt{\tr{\vect{x}} I_n \vect{x}} = \sqrt{\tr{\vect{x}} \vect{x}} = \norm{\vect{x}}
		\]
		\item  Instead of proving that $\norm{U\vect{x}} = \norm{\vect{x}}$ we will prove $\sqrnorm{U\vect{x}} = \sqrnorm{\vect{x}}$:
		
		\[
		\sqrnorm{U\vect{x}} = \tr{(U\vect{x})} \cdot (U\vect{x}) \numeq{(1)} \tr{\vect{x}} \tr{U} U \vect{x} = \tr{\vect{x}} I_n x = \tr{\vect{x}} \vect{x} = \norm{\vect{x}}
		\]
		
		where $\numeq{(1)}$ follows from the definition of transpose of a product.
	\end{enumerate}
  
\end{proof}
\end{document}


