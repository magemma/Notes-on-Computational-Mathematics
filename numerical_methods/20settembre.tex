%chktex-file 36
%chktex-file 23
%chktex-file 10
%chktex-file 17
%chktex-file 9
%chktex-file 44
\documentclass[ComputationalMathematics.tex]{subfiles}


\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{20th of September 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\subsection{A warm up}
Before starting here is a small recap
\begin{itemize}
    \item \textbf{Vector-Scalar product}:\\
      Let $x \in \R^n$ and $\lambda \in \R$ we call \textbf{multiple} of vector $x$ the following:
      \[
        \lambda x = x \lambda =
        \begin{pmatrix}
        \lambda x_1\\
        \vdots\\
        \lambda x_n\\
      \end{pmatrix}
      \]
    
    \item \textbf{Vector-Vector product}:\\
     Let $x, y \in \R^n$. The product between those two vectors is computed as follows $\tr{x} y = \sum\limits_{i=1}^n x_i y_i$ and $\tr{x}y \in \R$.
     
     \item \textbf{Scalar-Matrix product}:\\
      Let $A \in M(n, m, R)$ and $\lambda \in \R$ we call the \textbf{scalar-matrix product} the following:
      \[
        \lambda A = A \lambda = 
        \begin{pmatrix}
        \lambda A_{11} & \lambda A_{12} & \cdots & \lambda A_{1m}\\
        \lambda A_{21} & \lambda A_{22} & \cdots & \lambda A_{2m}\\
        \vdots  &\vdots & \ddots\\
        \lambda A_{n1} & \lambda A_{n2} & \cdots & \lambda A_{nm}
        \end{pmatrix}
      \]
     
     \item \textbf{Matrix-Vector product}:\\
        Given a matrix $A \in M(n, m, \R)$ and a vector $v \in \R^m$ the \textbf{matrix-vector product} $A v = w \in \R^n$ is computed as follows:
        \[
          w = Av = \begin{pmatrix}
          A_1 v\\
          A_2 v\\
          \vdots\\
          A_m v\\
          \end{pmatrix}, \, 
          w_i = \sum\limits_{j=1}^m A_{ij} v_j
        \]
        This is the simple way, just a row-by-column vector product, the computational complexity of this operation is $O(n^2)$.\\
        The smart way to compute it: \textbf{linear combinations} of columns of A, e.g.:
        \[
            \begin{pmatrix}
                      A_{11} & A_{12} & A_{13}\\
                      A_{21} & A_{22} & A_{23}\\
                      A_{31} & A_{32} & A_{33}\\
                      A_{41} & A_{42} & A_{43}
            \end{pmatrix}
            \begin{pmatrix}
                      v_1\\
                      v_2\\
                      v_3
            \end{pmatrix} =
            \begin{pmatrix}
                      w_1\\
                      w_2\\
                      w_3
            \end{pmatrix}
          \]
        with \textbf{linear combinations} we have:
        \[
        \begin{pmatrix}
                A_{11}\\
                A_{21}\\
                A_{31}\\
                A_{41}
        \end{pmatrix}v_1 +
        \begin{pmatrix}
                A_{12}\\
                A_{22}\\
                A_{32}\\
                A_{42}
        \end{pmatrix}v_2 + 
        \begin{pmatrix}
                A_{13}\\
                A_{23}\\
                A_{33}\\
                A_{43}
        \end{pmatrix}v_3 =
        \begin{pmatrix}
                  w_1\\
                  w_2\\
                  w_3\\
                  w_4
        \end{pmatrix}
      \]
  \item \textbf{Matrix-Matrix Product}:\\
        Given two matrices $A \in M(n, m, R)$ and $B \in M(m, k, R)$ we call \textbf{matrix-matrix product} the following:
        $C=AB$ such that $C_{ij} = A_i B^j$, where $\tr{A_i} \in \R^m$ is the $i$-th row of $A$, $B^i$ is the $i$-th column of $B$ ($B^i \in \R^m$) and $C \in M(n, k, \R)$. Notice that this product is \textbf{not commutative}: $AB \neq BA$ might not even make sense dimension-wise.
        
As long as the complexity is concerned, multiplying two matrices $m \times n$ and $n \times k$ requires $O(m n k)$ floating point operations (flops). Forget about fancier algorithms (e.g. Strassen)
        
\begin{myframe}{\bf Order of operations}
  Usual algebra properties hold, e.g.: $A(B + C) = AB + AC, A(BC) = (AB)C, \ldots$.
  
  Parenthesization matters a lot: if $A, B \in M(n, \R), \, v \in \R^n$, then $(AB)v$ costs $O(n^3)$, but $A(Bv)$ costs $O(n^2)$.
  Programming languages usually do not rearrange parentheses to help.
\end{myframe}

     \item \textbf{Image} of a matrix $A$ ($\text{Im}(A)$):  the set of vectors that can    be obtained multiplying $A$ by any vector in the domain of $A$.

     \item \textbf{Kernel} of a matrix $A$ ($\ker(A)$):  the set of vectors $w$ in its domain such that $Aw=0$.

     \item Given a matrix $A \in M(n, \R)$ we call \textbf{inverse} of $A$ the matrix $\inv{A}$ such that:
       \[  
       \inv{A} A = A \inv{A} = I_{n} =
         \begin{pmatrix}
                1\\
                & 1\\
                & & 1\\
                & & & \ddots\\
                & & & & 1
         \end{pmatrix}
         \]
         The \textbf{inverse of a product} (shoe-sock identity) is $\inv{(A B)} = \inv{B} \inv{A}$. Notice that this identity holds only for square matrices.

     \item The \textbf{transpose} of a matrix $A \in M(n, m, \R)$ is $\tr{A}$ such that $\tr{A}_{ij} = A_{ji}$. The \textbf{transpose of a product} (shoe-sock identity) is $\tr{(A B)} = \tr{B} \tr{A}$. (This identity holds for square and rectangular matrices)

\end{itemize} 

\begin{definition}
  \textbf{General linear group (GL)}: the general linear group of degree n is the set of $n\times n$ invertible matrices, together with the operation of ordinary matrix multiplication
\end{definition}

\begin{proposition}
  Let $A \in GL(n, \R)$ (aka $A$ is a real square matrix of size $n$ and invertible), $B, C \in M(n, m, \R)$ and we have the equality $AB = AC$.
  If there is a matrix $M$ such that $MA = I$, the following holds
  \[
    (MA)B = (MA)C \Longleftrightarrow B=C, \,\, M = A^{-1}
  \]
\end{proposition}
\noindent
In general, $AB = AC$ does not imply $B = C$; it holds only when $A$ is invertible.

\begin{myframe}{\bf Row and column vectors notation}
  \[
    v = \begin{pmatrix}
        4\\
        5\\
        6
    \end{pmatrix} \vspace{0.5cm}
    \tr{v} = \begin{pmatrix}
        4 & 5 & 6
    \end{pmatrix}
  \]
          $v$ is a column vector in $\R^3$ (or a matrix in $M(3, 1, \R)$) and $v^T$ is a row vector (or a matrix in $M(1, 3, \R)$).
\end{myframe}

\begin{definition}[Basis]
  We call \textbf{basis} a set $B$ of elements (vectors) in a vector space $V$ if every element of $V$ may be written in a unique way as a (finite) linear combination of elements of $B$.
  The coefficients of this linear combination are referred to as components or coordinates on $B$ of the vector.
  The elements of a basis are called basis vectors.
\end{definition}

\begin{definition}[Canonical basis]
  We term \textbf{canonical basis} of a vector space $\R^n$ the basis made of all the column of the $n \times n$ identity matrix $I_n \in M(n, \R)$.
\end{definition}

\begin{example}
  In $\R^4$ the \textbf{Canonical basis} is $\mathcal{B} = \{ e_1, e_2, e_3, e_4\}$ such that:
  \[
  e_1 = \begin{pmatrix} 1\\ 0\\ 0\\ 0 \end{pmatrix},
  e_2 = \begin{pmatrix} 0\\ 1\\ 0\\ 0 \end{pmatrix},
  e_3 = \begin{pmatrix} 0\\ 0\\ 1\\ 0 \end{pmatrix},
  e_4 = \begin{pmatrix} 0\\ 0\\ 0\\ 1 \end{pmatrix}
\]
  and each vector $w \in R^4$ can be written as $w = w_1 e_1 + w_2 e_2 + w_3 e_3 + w_4 e_4$.
\end{example}
The powerful idea behind linear algebra: many relations are true
regardless of the basis we use. E.g. $w$, $v$ and $w + v$ in two different bases.
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.45]{pics/20sett/3.png}
    \caption{How a change of basis reflects on the space.}\label{fig:20sett1}
\end{figure}

\begin{definition}[Triangular matrix]
  Let $A \in M(n, \R)$. We term $A$ \textbf{upper triangular} if ${(A)}_{ij} =0$ for each $i<j$.
  Conversely, we term $A$ \textbf{lower triangular} if ${(A)}_{ij} =0$ for each $j<i$.
  The set of all triangular $n \times n$ real matrices is a group and it is denoted as $T(n, \R)$.
\end{definition}

\begin{definition}[Diagonal matrix]
  Let $A \in M(n, \R)$. We term $A$ \textbf{diagonal} if ${(A)}_{ij} = 0$ for each $i \neq j$.
  The set of all diagonal $n \times n$ real matrices is a group and it is denoted as $D(n, \R)$.
\end{definition}

\begin{definition}[Symmetric matrix]
  Let $A \in M(n, \R)$. We term $A$ \textbf{symmetric} if ${(A)}_{ij} = {(A)}_{ji}$ for each $i,j = 1, \ldots, n$.
  The set of all diagonal $n \times n$ real matrices is a group and it is denoted as $D(n, \R)$.

\end{definition}

\subsection{Solving Linear Systems}
The objective of this course, for the part concerning numerical methods, is solving linear systems efficiently.

\begin{definition}[Linear system]
  Let $A \in M(n, m, \R)$, $b \in \R^n$ and $x \in \R^m$. We term \textbf{linear system} the following:

  \[
Ax = b
  \]
\end{definition}

\noindent Our goal is to \emph{approximate} such vector $x$, hence resulting in solving a minimum problem:

\[
  \min\limits \norm{Ax - b}
\]

If we have a square and invertible mtrix $A \in GL(n, \R)$ solving a linear system means: find those coordinates $x_1, \dots , x_n$ needed to write $b$ as a linear combination of the columns of (square) $A$ and in this case, the solution is given by: $x = A^{-1}b$. \\
\textbf{Warning}: this is not the best way to solve a linear system on a computer!\\

\syntax{Notice that the machine precision is $10^{-16}$, so we should pay attention when making computations, since we may incur in some error (proportional to the size of the operands).

In Matlab a matrix is written as \texttt{A=[1, 2, 3; 4, 5, 6];}, where \texttt{[1, 2, 3]} is the first row of the matrix A.

The transpose of a matrix or a vector is denoted by \texttt{A'}.

The inverse of a square matrix is denoted by \texttt{inv(A)}.

If we are interested in only a part of our matrix \texttt{A} we may write \texttt{A(1:2, 1:3)} and obtain only the rows of \texttt{A} that go from $1$ to $2$ and those columns from $1$ to $3$.

Notice that in Matlab both vector and matrices are $1$-based.
}

\begin{definition}[Block multiplications]
  Let $A \in M(n,m, \R)$ and let $B \in M(m, k, \R)$. We can compute the result of a block of the matrix $AB$ as the product of the two blocks in $A$ and $B$ in the corresponding position.
\end{definition}

\begin{obs}
  When computing a matrix product, we get the same result if we use the row-by-column rule \emph{block-wise}.
  \[
  \left(\begin{array}{ccc|c|cc}
    \tikzmarkin[hor=style orange]{row 1} \times & \times & \times & \times & \times & \times\\
    \times & \times & \times & \times & \times & \times\tikzmarkend{row 1}\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
  \end{array}\right)
  \cdot
 \left(\begin{array}{ccc|cc|c}
   \tikzmarkin[ver=style green]{col 1} \times  & \times & \times & \times & \times & \times\\
    \times & \times & \times & \times & \times & \times\\
    \times & \times & \times & \times & \times & \times\\
    \hline
    \times & \times & \times & \times & \times & \times\\
    \hline
    \times & \times & \times & \times & \times & \times\\
    \times  & \times & \times \tikzmarkend{col 1}& \times & \times & \times\\
  \end{array}\right)
  =
 \left(\begin{array}{ccc|cc|c}
    \tikzmarkin[hor=style cyan]{row 2} \times & \times & \times & \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 2}& \times & \times & \times\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
    \hline
    \times  & \times  & \times & \times & \times & \times\\
    \times  & \times  & \times & \times & \times & \times\\
  \end{array}\right)
\]

\[
  \scalemath{0.8}{ \left(\begin{array}{ccc}
    \tikzmarkin[hor=style orange]{row 3} \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 3}\\
  \end{array}\right)
  \cdot
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style green]{row 4} \times & \times & \times\\
    \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 4}\\
  \end{array}\right)
  +
  \left(\begin{array}{c}
    \tikzmarkin[hor=style orange]{row 5} \times\\
    \times \tikzmarkend{row 5}\\
  \end{array}\right)
  \cdot
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style green]{row 6} \times & \times & \times \tikzmarkend{row 6}\\
  \end{array}\right)
  +
  \left(\begin{array}{cc}
    \tikzmarkin[hor=style orange]{row 7} \times & \times \\
    \times & \times \tikzmarkend{row 7}\\
  \end{array}\right)
  \cdot
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style green]{row 8} \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 8}\\
  \end{array}\right)
  =
  \left(\begin{array}{ccc}
    \tikzmarkin[hor=style cyan]{row 9} \times & \times & \times\\
    \times & \times & \times \tikzmarkend{row 9}\\
  \end{array}\right)
}\]
\end{obs}

Notice that block operations usually give better performance: one matrix-matrix product performs faster than n matrix-vector products (even if they have the same number of flops).
This is one of the reasons why library calls usually perform better
than hand-coded loops (Blas/Lapack).

\begin{proposition}[Block triangular matrices]
  Let $M \in M(n, m, \R)$ and $B \in M(m, k, \R)$ such that they are \textbf{block triangular}.
  Their product is a block triangular matrix as well. In other words, block triangular matrices are closed under products:
  \[
  M B 
  = \begin{pmatrix}
    A & B\\
    0 & C
  \end{pmatrix}
  \begin{pmatrix}
    D & E\\
    0 & F
  \end{pmatrix}
  = 
  \begin{pmatrix}
    AD & AE + BF\\
    0 & CF
  \end{pmatrix}
\]
\end{proposition}

\begin{proposition}[Properties of block triangular matrices]~\\
    Let M be a block triangular matrix, where all the blocks on the diagonal are square:
   \[
     M 
      = \begin{pmatrix}
        A_{11}  & A_{12} & \cdots & A_{1n}\\
        0       & A_{22} & \cdots & A_{2n}\\
        \vdots  &\ddots & \ddots & \vdots\\
        0       & \cdots & 0 & A_{nn}
      \end{pmatrix}
    \]
    \begin{enumerate}
      \item a block triangular matrix is invertible iff all diagonal blocks $A_{ii}$ are invertible;
      \item the eigenvalues of a block triangular matrix are the union of the eigenvalues of each $A_{ii}$ block;
      \item let $M \in GL(n, \R)$ such that $M= 
        \begin{pmatrix}
          A & B\\
          0 & C\\
        \end{pmatrix}$ 
       the inverse of $M$ is 
        
        $\inv{M} = 
        \begin{pmatrix}
          \inv{A} & -\inv{A}B\inv{C}\\
          0 & \inv{C}
        \end{pmatrix}$;
      \item the product of two block (upper/lower) triangular matrices
(with compatible block sizes) is still block triangular.
    \end{enumerate}
  \end{proposition}

Why are we interested in block triangular matrices? They depict a situation as shown in \Cref{fig:20sett3}.
\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{pics/20sett/1.png}
    \caption{The adjacency matrix of a biparted graph has $0$s in its bottom left part (Matlab syntax \texttt{A[p+1:n; 1:p]=0}), which means that the edges from a connected component and the other are in one direction only.}\label{fig:20sett3}
\end{figure}

\textbf{General principle}: matrix structures matter. Block triangular linear system has a cheaper system solution than a general system as shown in \Cref{exp1:20sett}.

\begin{example}\label{exp1:20sett}
Let us take a $2\times2$ block triangular linear system\\
  \[
\begin{pmatrix}
    A & B\\
    0 & C
\end{pmatrix}
\begin{pmatrix}
    x \\
    y 
\end{pmatrix} = 
\begin{pmatrix}
    e \\
    f 
\end{pmatrix}
\]
(Again, diagonal blocks are square and all dimensions are
compatible.)
  \[
\begin{pmatrix}
    Ax + By\\
    Cy
\end{pmatrix} = 
\begin{pmatrix}
    e \\
    f 
\end{pmatrix}\implies y = \inv{C}f, x = \inv{A}(e - B\inv{C}f)
\]
  \[
\begin{pmatrix}
    A & B\\
    0 & C
\end{pmatrix}^{-1} = 
\begin{pmatrix}
          \inv{A} & -\inv{A}B\inv{C}\\
          0 & \inv{C}
        \end{pmatrix}
      \]
Informal idea: we can start solving from the variables multiplied by C.
\end{example}

\subsection{Orthogonality}

\begin{definition}[Norms]\label{def:20sett_norm}
  Let $x \in \R^n$.
  We ``measure'' its magnitude using so-called ``norms''.
  \begin{description}
    \item[{\sc Euclidean:}] $\norm{x}_2 = \tr{x} x = \sqrt{\sum\limits_{i=1}^n {x_i}^2}$;
    \item[{\sc Norm 1:}] $\norm{x}_1= \sum\limits_{i=1}^n \abs{x_i}$;
    \item[{\sc $p$-Norm:}] $\abs{x}_p = {\bigg(\sum\limits_{i=1}^n \abs{x_i}^p\bigg)}^{1/p}$;
    \item[{\sc $0$-Norm:}] $\norm{x}_0 = \abs{\{i~:~\abs{x_i} > 0\}}$, which accounts for $1- \#$of $0$ entries;
    \item[{\sc $\infty$-Norm:}]$\norm{x}_\infty = \max\limits_{i=1, \ldots, n} \abs{x_i}$.
  \end{description}
\end{definition}

From now on in this part of the course, if not explicitly specified, we will refer to norm-2 only.

\begin{definition}[Scalar product]
  Let $v, w \in \R^n$ we term \textbf{standard scalar product} between $v$ and $w$ the real number $\tr{v} w = \sum\limits_{i=1}^n v_i w_i$.
\end{definition}

\begin{definition}[Orthogonal vectors]
  Let $v, w \in \R^n$. We say that $v$ is \textbf{orthogonal} to $w$ (in symbols $x \bot y$) if their scalar product is zero.
  
  Formally, $\tr{x} v = 0$.
\end{definition}

\begin{definition}[Orthogonal matrix]
  Let $A \in M(n, \R)$ a square matrix.
  We term $A$ \textbf{orthogonal} if $\tr{A} A = A \tr{A} = I_{n}$ where $I_n$ is the identity matrix of size $n$ ($1$ on the diagonal, $0$ elsewhere) or equivalenlty if $\inv{A} = \tr{U}$.

  The set of all orthogonal matrices in $M(n, \R)$ is a group and it called orthogonal group and denotes as $O(n, \R)$.
\end{definition}

\begin{proposition}\label{fact:20sett}
  Let $A \in O(n, \R)$, $\forall x \in \R^n$ we have that $\norm{Ax} = \norm{x}$. 
\end{proposition}

\begin{proof}
  \[
    \norm{Ax} = \sqrt{\tr{(Ax)} Ax} = \sqrt{\tr{x}\tr{A} A x} = \sqrt{\tr{x} I_n x} = \sqrt{\tr{x} x} = \norm{x}
  \]
\end{proof}
\end{document}


