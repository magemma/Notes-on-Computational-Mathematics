%chktex-file 36
%chktex-file 8
%chktex-file 24
\documentclass[computationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{7th of November 2018 --- F.Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\subsection{Least squares problem with SVD}

\recap{
  \textbf{Tall thin SVD:}
  A matrix $A$ can be written as $A = US\tr{V}$, where $U$ is orthogonal, $S$ is a diagonal matrix and $V$ is orthogonal as well. In the case of a tall, thin $A$ the decomposition has the following shape:
  \[
    \begin{pmatrix}
    &&&\\
    &&&\\
    &&&\\
    &&&\\
    U^1 & U^2 & \cdots & U^m\\
    &&&\\
    &&&\\
    &&&\\
    &&&\\
    \end{pmatrix}
    \cdot
    \Sigma \cdot 
    \tr{\begin{pmatrix}
    \\
    V^1 ~ V^2 ~ \cdots ~ V^n\\
    \\
    \end{pmatrix}} 
  \]
where 
\[
    \Sigma = \begin{pmatrix}
        \sigma_{1} & & & & &\\
        & \sigma_{2} & & & &\\
        & & \cdot & & &\\
        & & & \cdot & &\\
        & & & & \cdot &\\
        & & & & & & \sigma_{n}\\
    \end{pmatrix}
\]

  And if we denote $U_1$ the matrix obtained as the first $n$ columns of $U$ we have the tall, thin SVD:~$ A = U_1 \cdot \Sigma \cdot \tr{V}$}.

We would like to see how we can solve a least squares problem through $SVD$:

\begin{equation}
  \begin{aligned}
    \norm{Ax-b} &= \norm{US\tr{V}x - b} ~~~~~~~~~~ \leftarrow~\text{Def.~of SVD}\\
    & = \norm{\tr{U} (US\tr{V}x - b)} ~~~~ \leftarrow \tr{U} \text{is orthogonal}\\
    & = \norm{S\tr{V} x -\tr{U}b} ~~~~~~~~~ \leftarrow \text{Distributivity + orthogonality}\\ 
    & = \norm{Sy - \tr{U}b} ~~~~~~~~~~~~~ \leftarrow y = \tr{V}x\\
    & =\norm{
      \begin{pmatrix}
        \sigma_{1} & & & & &\\
        & \sigma_{2} & & & &\\
        & & \cdot & & &\\
        & & & \cdot & &\\
        & & & & \cdot &\\
        & & & & & & \sigma_{n}\\
        &&&&&\\
        && 0 &&&\\
      \end{pmatrix}
      \begin{pmatrix}
        y_1\\
        y_2\\
        \cdot\\
        \cdot\\
        \cdot\\
        y_n\\
      \end{pmatrix}
      -
       \begin{pmatrix}
        \tr{U^1}b\\
        \tr{U^2}b\\
        \vdots\\
        \tr{U^n}b\\
         \tr{U^{n+1}}b\\
        \vdots\\
        \tr{U^m}b\\
      \end{pmatrix}
    }\\
    &= \norm{
      \begin{pmatrix}
        \sigma_1 y_1 - \tr{U^1}b\\
        \sigma_2 y_2 -\tr{U^2}b\\
        \vdots\\
        \sigma_n y_n -\tr{U^n}b\\
        \sigma_{n+1} y_{n+1} - \tr{U^{n+1}}b\\
        \vdots\\
        \sigma_m y_m -\tr{U^m}b\\
      \end{pmatrix}
    }
  \end{aligned}
\end{equation}
Where the first $n$ rows may be assigned to $0$ iff $y_{i} = -\frac{\tr{U^{i}} b}{\sigma_{1}}$ (if $\sigma_i \neq 0 \forall i$), while the latter $m-n$ do not depend on $y$. This process produces a solution $y$, but the variable change may be inverted, so 
\begin{equation}
  \begin{aligned}
    x &= Vy ~~~~~~~~~~~~~~~~ \leftarrow \text{Orthogonality of} ~V\\
    &= V^1 y_1 + V^2 y_2+ \cdots + V^n y_n\\
    &= V^1 \frac{1}{\sigma_1} \tr{U^1} b + V^2 \frac{1}{\sigma_2} \tr{U^2} b + \cdots + V^n \frac{1}{\sigma_n} \tr{U^n} b\\ 
    &= {V \cdot \begin{pmatrix}
      \frac{1}{\sigma_{1}} & & & & &&&&\\
        & \frac{1}{\sigma_{2}} & & & &&&&\\
        & & \cdot & & & &&&\text{\Huge 0}\\
        & & & \cdot & &&&&\\
        & & & & \cdot &&&&&\\
        & & & & & & \frac{1}{\sigma_{n}}&&&\\
    \end{pmatrix} \cdot \tr{U} \cdot b}\\
    &= {V \cdot \begin{pmatrix}
      \frac{1}{\sigma_{1}} & & & &\\
        & \frac{1}{\sigma_{2}} & & &\\
        & & \cdot & & &\\
        & & & \cdot & &\\
        & & & & \cdot &\\
        & & & & & & \frac{1}{\sigma_{n}}\\
    \end{pmatrix} \cdot \tr{U_1} \cdot b}\\
  \end{aligned}
\end{equation}

Which depends only on the tall, thin~SVD.%
\begin{proposition}
  The $\sigma_{i}$ are different from $0$ iff $A$ has full column rank.
\end{proposition}

\begin{proof}
  \begin{center}
  $A$ has full column rank 
  
  $\Updownarrow$

  $\tr{A}A$ is invertible
  
$\Updownarrow$
  
  $\tr{(US\tr{V})} (US\tr{V}) = V\tr{S}\tr{U}US\tr{V}
    = V \cdot  \begin{pmatrix}
    \sigma_{1}^2 & & & & &\\
    & \sigma_{2}^2 & & & &\\
    & & \cdot & & &\\
    & & & \cdot & &\\
    & & & & \cdot &\\
    & & & & & & \sigma_{n}^2\\ 
  \end{pmatrix} \cdot \tr{V}$
  is invertible 
  
$\Updownarrow$
  
  $\forall i ~ \sigma_i \neq 0$
\end{center}
\end{proof}

\begin{obs}
This lemma also proves that the factorization is also a $QR$ factorization.
\end{obs}
\begin{myframe}{\bf Note on Matlab syntax}
  \texttt{svd(A, 0)} and \texttt{qr(A, 0)} express that we are only interested in the parts of the factorization without zeros, in case of a tall, thin matrix $A$.
\end{myframe}
 We may observe that the computational complexity is $O(15 \cdot n^3)$ for square matrices, while it's $O(m \cdot n^2)$ in the tall, thin case.

 \subsubsection{Behaviour in case of zeros as singular values}
 What happens when there are some zeros as singular values? 
 
 \recap{We may recall that the singular values are ordered on the diagonal in decreasing order (the largerst in top left position). From this assumption, we may say that if there are some $\sigma_{i} = 0$ then they are in the bottom right part of the matrix.
\[
  \sigma_{1} \ge \sigma_{2} \ge \cdots \ge \sigma_{r} > \sigma_{r+1} = \cdots = \sigma_{n} = 0  
\]
}

We also recall the following 
$\norm{Ax-b} = \norm{
      \begin{pmatrix}
        \sigma_1 y_1 - \tr{U^1}b\\
        \sigma_2 y_2 -\tr{U^2}b\\
        \vdots\\
        \sigma_n y_n -\tr{U^n}b\\
        0 \cdot y_{n+1} - \tr{U^{n+1}}b\\
        \vdots\\
        0 \cdot y_m -\tr{U^m}b\\
      \end{pmatrix}
    }$.
    
No matter what value we choose for $y_{r+1} \cdots y_{n}$, the value doesn't change since it's multiplied by $0$.
 Therefore we get infinite solutions of the form $ y =
 \begin{pmatrix}
   -\frac{\tr{U^1} b}{\sigma_1}\\
    -\frac{\tr{U^2} b}{\sigma_2}\\
    \vdots\\
    -\frac{\tr{U^r} b}{\sigma_r}\\
    \\
    \text{\Huge{*}}\\
    \\
  \end{pmatrix}$

We would like to make the solution unique, so we can modify the problem:
  \begin{itemize}
    \item taking the value that minimize the norm:
    \begin{equation}
    	\min\limits_{x \in \arg\min(\norm{Ax-b})} \norm{x} \tag{P2}.
    \end{equation}
Note that $\norm{x} = \norm{y}$, because $x = Vy$. It follows from the expression of $y$ that  the choice that minimizes its norm is $y_{r+1} = \cdots = y_{n} =0$.
    \item 
  \end{itemize}

  The solution of $P2$ is given by
  \[
    Vy = \begin{pmatrix} V^1 & V^2 & \cdots & V^{n} \end{pmatrix} \cdot \begin{pmatrix}
   -\frac{\tr{U^1} b}{\sigma_1}\\
    -\frac{\tr{U^2} b}{\sigma_2}\\
    \vdots\\
    -\frac{\tr{U^r} b}{\sigma_r}\\
    \\
    \text{\Huge{0}}\\
    \\
  \end{pmatrix}
  = \tr{V^1} \frac{1}{\sigma_1} \tr{U^1} b + \cdots + \tr{V^r} \frac{1}{\sigma_r} \tr{U^r} b
\]

  What happens when working with machine precision? Let's make an example where $r = n-1$, so only the last singular value is $0$. If the check of $\sigma_n = 0$ fails, $\frac{1}{\sigma_n}$ becomes very big. A way to circumvent this problem is to find the linear dependencies between the columns, so that the algorithm works correctly.


\subsection{Truncated SVD}
In many real world setups first singular components correspond to the most prominent features of the dataset, while the smallest ones are fine details and noise. Note, though, that in the sum $\sum\limits_{i=1}^n V^i \frac{\tr{U^i} b}{\sigma_i}$ the small singular values may have a large impact, because $\sigma_i$ is in the denominator.

We can modify the solution to cope with real world data problems:

$x = \sum\limits_{i=1}^n V^i \frac{\tr{U^i} b}{\sigma_i} ~~\longrightarrow ~~x_{trunc} = \sum\limits_{i=1}^{k} V^i \frac{\tr{U^i} b}{\sigma_i}$

for a certain $k$, ignoring small singular values.

Another way to modify the problem is the following.

\subsection{Tikhonov regularization / ridge regression}

The Tikhonov regularization is a smoother version of truncated SVD.%

\[
  x_{Tik} = \arg \min_{x \in \R^n} \sqrnorm{Ax-b} + \alpha^2 \sqrnorm{x}
\]

\begin{proposition}
The Tikhonov regularization is equivalent to 
\begin{equation} \label{eq:7novxtik}
  x_{Tik} = \arg \min_{x\in\mathbb{R}^n} \sqrnorm{\begin{pmatrix}
    A\\
    \alpha I
\end{pmatrix} x - \begin{pmatrix}
    b\\0
\end{pmatrix}}
\end{equation}
\end{proposition}
We show that the two objective functions coincide.
\begin{proof}
  \begin{equation}
    \begin{aligned}
      \sqrnorm{\begin{pmatrix}
        A\\
        \alpha I
      \end{pmatrix} x -
      \begin{pmatrix}
        b\\0
      \end{pmatrix}} &= \sqrnorm{\begin{pmatrix}
        Ax\\
        \alpha x
      \end{pmatrix} -
      \begin{pmatrix}
        b\\0
      \end{pmatrix}}\\
      &= \sqrnorm{\begin{pmatrix}
        Ax - b\\
        \alpha x
      \end{pmatrix}}\\
      &= \sqrnorm{Ax-b} + \sqrnorm{\alpha x}\\
      &= \sqrnorm{Ax-b} + \alpha \cdot \sqrnorm{x}\\
    \end{aligned}
  \end{equation}
\end{proof}

\begin{proposition}
The solution of the Tikhonov regularization is given by the formula $x_{Tik}=\inv{(\tr{A}A + {\alpha}^{2} I)}\tr{A} b$.
\end{proposition}

\begin{proof}
We start by writing the explicit solution of~\Cref{eq:7novxtik} using the pseudoinverse
\begin{equation}
  \begin{aligned}
    \begin{pmatrix}
      A\\
      \alpha I
    \end{pmatrix}^{+}
    \begin{pmatrix}
      b\\
      0
    \end{pmatrix} &= \inv{\left(
      \tr{\begin{pmatrix}
        A\\
        \alpha I
      \end{pmatrix}}
      \begin{pmatrix}
        A\\
        \alpha I
      \end{pmatrix}\right)}
      \tr{\begin{pmatrix}
        A\\
        \alpha I
      \end{pmatrix}}
      \begin{pmatrix}
        b\\
        0
      \end{pmatrix} \\
    &=\inv{\left(
      \begin{pmatrix}
        \tr{A} & \alpha I
      \end{pmatrix}
      \begin{pmatrix}
        A\\
        \alpha I
      \end{pmatrix}
      \right)}
      \begin{pmatrix}
        \tr{A} & \alpha I
      \end{pmatrix}
      \begin{pmatrix}
        b\\
        0
      \end{pmatrix}\\
    &= \inv{(\tr{A}A + {\alpha}^{2} I)}\tr{A} b.
  \end{aligned}
\end{equation}
\end{proof}

\begin{proposition}
We can observe that $(\tr{A}A + \alpha^2 I)$ is positive definite.
\end{proposition}

\begin{proof}
  $\tr{z} \cdot (\tr{A}A + \alpha^2 I) \cdot z = \tr{z}\tr{A}Az + \alpha^2 \tr{z} z  \numeq{(1)} \alpha^2 \tr{z} z = \alpha^2 \sqrnorm{x} > 0$. The equality $(1)$ is obtained because $\tr{z}\tr{A}A \ge 0$, since $\tr{A}A$ is positive semidefinite. 
\end{proof}

\begin{exe}
Show using the SVD of $A$ that the Tikhonov / Ridge solution can be written as
\[
  x = \sum\limits_{i=1}^n V^i \frac{\sigma_i}{\sigma_i^2+\alpha^2} \tr{U^i} b.
\]

When $\sigma_i \gg \alpha$, $\frac{\sigma_i}{\sigma_i^2+\alpha^2} \approx \frac{1}{\sigma_i}$: similar to the `true' solution.

When $\sigma_i \ll \alpha$, $\frac{\sigma_i}{\sigma_i^2+\alpha^2} \approx \frac{\sigma_i}{\alpha^2} \approx 0$: approximately ignoring small singular values.
\end{exe}
\textbf{Per casa:} fare esercizio.

\addpic{0.5}{pics/7nov/1.png}{Here is the shape of the formula for the singular values.}{fig:7nov1}
How can we choose $k$? We don't know.
\end{document}
