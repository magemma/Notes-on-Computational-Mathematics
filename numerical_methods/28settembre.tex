%chktex-file 36
%chktex-file 23
%chktex-file 10
%chktex-file 17
%chktex-file 9
%chktex-file 8
%chktex-file 13
\documentclass[computationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\chapter{28th of September 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\section{Singular value decomposition (SVD)}
We are left with the task of reaching a (sort of) ``eigenvalue decomposition'' when the target matrix is not symmetric.\\
There are two ways to generalize the eigenvalue decomposition to a
non-symmetric matrix A (with something that always exists):

\begin{definition}[Schur decomposition]
  Let $A \in M(n, \R)$, $\exists~U \in O(n, \R)$ orthogonal matrix and $T \in T(n, \R)$ triangular matrix such that $A = UT\tr{U}$ and this is called \textbf{Schur decomposition}.
\end{definition}

\noindent We can say more:

\begin{definition}[Singular value decomposition]
  Let $A \in M(n, \R)$, $\exists~U, V \in M(n, \R)$ orthogonal matrices ($V$ not necessary equal to $U$) and $\Sigma \in \text{Diag}(n, \R)$ such that $A = U\Sigma\tr{V}$ and this is called \textbf{Singular Value Decomposition}.

\[
  A = 
  \begin{pmatrix}
    U^1 & U^2 & \cdots & U^n\\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \sigma_1\\
    & \sigma_2\\
    && \ddots\\
    &&&& \sigma_n\\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    V_1\\
    V_2\\
  \vdots\\
    V_n
  \end{pmatrix}
  = \sum\limits_{i=1}^{n} u_{ii} \sigma_i v_{ii} =
\]

  Where $\sigma_i$ are called \textbf{singular values} and they are sorted such that:
  \[
    \sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n \ge 0
  \]
\indent  General fact on singular values:
  \begin{itemize}
      \item Singular values $\neq$ eigenvalues
      \item They are always positive and usually more ‘spread apart’ than the eigenvalues.
        \[
           \sigma_1 \geq \abs{\lambda_1} \;\; \text{and} \;\; \abs{\lambda_m} \geq \sigma_m
         \]
      $\sigma_i$ is larger than the largest eigenvalue of a matrix $A$ and $\sigma_m$ is smaller than the smallest eigenvalue of a matrix $A$.
  \end{itemize}
\end{definition}

\noindent The SVD can be defined also for a rectangular matrix A:

\begin{definition}[Rectangular matrices and SVD]
  Let $A \in M(m, n, \R)$, there exist $U \in O(m, \R)$ orthogonal, $V \in O(n, \R)$ orthogonal and $\Sigma \in D(m, n, \R)$ diagonal in the sense that $\sum_{ij} = 0$ with $i \neq j$ (padded with zeros). 
  Matrix $A$  has a \textbf{SVD factorization} ($A = U \Sigma \tr{V}$), where $\Sigma$ has the following shape:
  \begin{itemize}
      \item case m < n  (e.g $m=3, n=5$)
        \[
          \begin{pmatrix}
            \sigma_1&0&0&0&0\\
            0& \sigma_2&0&0&0\\
            0&0&\sigma_3&0&0
          \end{pmatrix}
        \]
        
        \item  case m > n (e.g $m=5, n=3$)
        \[
          \begin{pmatrix}
            \sigma_1&0&0&\\
            0& \sigma_2&0&\\
            0&0&\sigma_3&\\
            0&0&0\\
            0&0&0
          \end{pmatrix}
        \]
  \end{itemize}

\end{definition}

\begin{definition}[Thin SVD]
  Let $A \in M(m, n, \R)$,   has a \textbf{thin} \textbf{SVD factorization}:  we may restrict to compute only the first min(m, n)
vectors that appear in this sum: thin SVD.
  \[
    A = \sum\limits_{i=1}^{\min(m, n)} u_{ii} \sigma_i v_{ii} 
    = u_{11} \sigma_1 v_{11} + u_{22} \sigma_2 v_{22} + \cdots + u_{\tilde{n}, \tilde{n}} \sigma_{\tilde{n}} v_{\tilde{n}, \tilde{n}}
  \]
\end{definition}

\syntax{In Matlab the SVD decomposition is obtained through the command \texttt{svd(A)}, which return value is made of the three matrices $U, \Sigma, V$.

As an example, \texttt{[U, S, V] = svd(A)}.
Notice that, if \texttt{svd(A)} is assigned to one variable, then such variable is an array of singular values.

The thin SVD can be computed as:\texttt{[U, S, V] = svd(A, 0)}
} 

\noindent \textbf{Computational costs}\\
We are not going into details of algorithms for computing SVD, but we would like to add a consideration about the computational complexity of such an algorithm.
\begin{itemize}
    \item \texttt{[U, S, V] = svd(A, 0)} (thin) costs $O(mn^2)$ ops for $A \in M(m, n, \R)$ or $A \in M(n, m, \R)$ with $m \geq n$
    \item \texttt{[U, S, V] = svd(A)} (non-thin) is more expensive, because it has to store the ‘large’ $m \times m$ factor. (But there are some tricks to store orthogonal matrices compactly, more about it later).
\end{itemize}

\subsection{Properties of SVD}
The SVD reveals rank, image, and kernel of a matrix.
\begin{definition}[Rank]
  Let $A \in M(n, \R)$ we call the \textbf{rank} of $A$ the number of non-zero singular values.
  
  Equivalently, the \textbf{rank} is the size of the column space.
\end{definition}


\begin{property}\label{prop:28sett_rank}
  A matrix $A \in M(n, \R)$ has rank $r$ iff all its eigenvalues starting from the $r+1$-th are $0$, formally iff $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > \sigma_{r+1} = \cdots = \sigma_n = 0$. 
\end{property}

Thanks to \Cref{prop:28sett_rank}, we can somehow talk about an ``even thinner'' SVD, where all the $0$s in the bottom right part of the matrix $\Sigma$, cancel out the latter columns of $U$ and the latter rows of $V$ (aka columns of $\tr{V}$).
A pictorial representation of the shape of $\Sigma$ can be found below.

\[
  \Sigma ~~=~~ \left(\begin{array}{cccccccccc}
    \sigma_1&&&&&& & & & \\
    &\sigma_2&&&&& & & & \\    
    &&\ddots&&&& & & & \\
    &&&&\tikzmark{left}{$\sigma_r$} \tikzmark{right}{~~} && & \\
    &&&&&0 & & & \\
    &&&&&&&\ddots&&\\
    &&&&&&&&0\\
    \midrule
   &&&&\text{\Large\bfseries 0}\\
  \end{array}\right)
  \Highlight[first]
\]
%
\tikz[overlay,remember picture] {
  \node[above right=2 and -0.16 of first](f) {};
  \node[below left=-0.16 and 2.6 of first](i) {};
  \draw[thick] (f.south) -- (first.south east);
  \draw[thick] (i.east) -- (first.south east);
}

This factorization represents $A$ as $\sum\limits_{i=1}^r u_{ii} \sigma_i v_{ii}$. 

\begin{proposition}
	Let $A \in M(n, \R)$ such that has the following SVD-factorization $A = U \Sigma \tr{V}$ with $U \in O(n, \R)$ orthogonal, $V \in O(n, \R)$ orthogonal and $\Sigma \in D(n, \R)$ is a diagonal matrix.
	The image of $A$ is the span of $U_1, U_2, \ldots U_r$, hence $rk(A) = r$.
	Moreover, $\ker(A)=span(V_{r+1}, V_{r+2}, \ldots V_{n})$, since $V$ is orthogonal.
\end{proposition}

\begin{proof}
	\todo[inline]{TODO: plugging in $x=V_j$, where $j>r$}
\end{proof}

\begin{definition}[Matrix norm]
  Let $A \in M(m, n, \R)$. We define the \textbf{matrix norm} of $A$ as
  \[
    \norm{A} := \max\limits_{\vect{x} \neq \vect{0}} \frac{\norm{A\vect{x}}}{\norm{\vect{x}}} = \max\limits_{\vect{z} ~ s.t. \norm{\vect{z}}=1}\norm{A\vect{z}}
  \]
  Where the norm may be any of the ones defined in \Cref{def:20sett_norm} second equality is introduced in order to work in a compact set, the one of normalized vectors $\vect{z}$.
\end{definition}

\begin{property}\label{prop:28sett_norms}
  Let $A$ and $B \in M(n, m, \R)$ and let $\vect{x} \in \R^n$, the following holds, for any norm defined in \Cref{def:20sett_norm}:
  \begin{itemize}
    \item $\norm{A} \ge 0$ (and the equality holds iff $A=0$);
    \item $\norm{\alpha A} = |\alpha| \norm{A}, \forall \alpha \in \R$;
    \item $\norm{A+B} \le \norm{A} + \norm{B}$;
    \item $\norm{AB} \le \norm{A} \norm{B}$;
    \item $\norm{A\vect{x}} \le \norm{A} \norm{\vect{x}}$, $\forall \vect{x} \in \R^n$.
  \end{itemize}
\end{property}

\begin{proposition}\label{fact:28sett_orthogonorm}
  Let $A \in (n, \R)$ and let $U \in O(m, n, \R)$ orthogonal, in the case of $2$-norm $\norm{A}_2 \numeq{(1)} \norm{AU}_2 \numeq{(2)} \norm{UA}_2$.
\end{proposition}

\begin{proof}~\\
	\hangindent=0.7cm$\numeq{(1)}$
	\[
		\begin{split}
			\norm{UA}_2 &= \max\limits_{\vect{x} \in R^n,~\vect{x} \ne \vect{0}} \frac{\norm{UA\vect{x}}_2}{\norm{\vect{x}}_2}\\
			&= \max\limits_{\vect{x} \in R^n,~\vect{x} \ne \vect{0}} \frac{\sqrt{(\tr{UA\vect{x})}(U A \vect{x})}}{\norm{\vect{x}}_2}\\
			&= \max\limits_{\vect{x} \in R^n,~\vect{x} \ne \vect{0}} \frac{\sqrt{\tr{\vect{x}} \tr{A} \cancel{\tr{U}} \cancel{U} A \vect{x}}}{\norm{\vect{x}}_2}\\
			&= \max\limits_{\vect{x} \in R^n,~\vect{x} \ne \vect{0}} \frac{\norm{A\vect{x}}_2}{\norm{\vect{x}}_2} = \norm{A}_2
		\end{split}
	\]
	\hangindent=0.7cm$\numeq{(2)}$
	\[
	\begin{split}
		norm{AU}_2 &= \max\limits_{\vect{x} \in R^n,~\vect{x} \ne \vect{0}} \frac{\norm{AU\vect{x}}_2}{\norm{\vect{x}}_2}\\
		&\numeq{(2)} \max\limits_{\vect{y} \in R^n,~\vect{y} \ne \vect{0}} \frac{\norm{A\vect{y}}_2}{\norm{\vect{y}}_2} = \norm{A}_2
	\end{split}
	\]
	where $\numeq{(2)}$ follows from the substitution $\vect{y}=U\vect{x}$.
\end{proof}

\begin{definition}[Frobenius norm]
  Let $A \in M(n, m, \R)$, we term \textbf{Frobenius norm} of $A$ ${\norm{A}}_F = \sqrt{\sum\limits_{i=1}^n \sum\limits_{j=1}^m {{(A)}_{ij}}^2}$.
\end{definition}

Notice that all the properties enumerated in \Cref{prop:28sett_norms} hold for the Frobenius norm as well.

\begin{proposition}
  Let $A \in M(n, m, \R)$ and let $A = U \Sigma \tr{V}$ be its singular value decomposition.
  The following holds:
  \begin{enumerate}
    \item $\norm{A}_2 = \norm{\Sigma}_2 = \sigma_1$
    \item $\norm{A}_F = \norm{\Sigma}_F = \sum\limits_{i=1}^{\tilde{n}} {\sigma_i}^2$, where $\tilde{n} = \min{n, m}$
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item The first equality follows from \Cref{fact:28sett_orthogonorm}, while the second is proved as follows:
  \begin{equation}
    \begin{split}
      \norm{\Sigma}_2 &=
	    \max\limits_{\vect{x} \in \R^n,~\vect{x} \neq \vect{0}}\frac{\norm{\Sigma \vect{x}}_2}{\norm{\vect{x}}_2} =
	    \max\limits_{\vect{x} \in \R^n,~\vect{x} \neq \vect{0}}\frac{\norm{\begin{pmatrix}
	    \sigma_1\\
	    & \sigma_2\\
	    && \ddots\\
	    &&&& \sigma_n\\
	      &&\text{\Large 0}&&\\
	    \end{pmatrix}
	    \cdot 
	    \begin{pmatrix}
	      x_1\\
	      x_2\\
	      \vdots\\
	      x_n\\
	    \end{pmatrix}}_2}
	      {\norm{\begin{pmatrix}
	      x_1\\
	      x_2\\
	      \vdots\\
	      x_n\\
	    \end{pmatrix}}_2}\\
    &=
      \max\limits_{x \in \R^n,~x \neq 0}\frac{\norm{\begin{pmatrix}
      \sigma_1 x_1\\
      \sigma_2 x_2\\
      \vdots\\
      \sigma_n x_n\\
      0\\
      \vdots\\
      0\\
      \end{pmatrix}}_2}
      {\norm{\begin{pmatrix}
      x_1\\
      x_2\\
      \vdots\\
      x_n\\
      \end{pmatrix}}_2}
    =
    	\frac{\sqrt{{(\sigma_1 x_1)}^2 + {(\sigma_2 x_2)}^2 + \cdots +{(\sigma_n x_n)}^2}}{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}\\
    &\le 
		\frac{\sqrt{{(\sigma_1 x_1)}^2 + {(\sigma_1 x_2)}^2 + \cdots +{(\sigma_1 x_n)}^2}}{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}
	=
    \sqrt{\sigma_1}^2 \cdot \frac{\cancel{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}}{\cancel{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}}
      = \sigma_1
  \end{split}
  \end{equation}

  The equality is achieved if we pick $\vect{x} = \vect{e_1} = \begin{pmatrix}
    1\\
    0\\
    \vdots\\
    0\\
  \end{pmatrix}$.
    \item The proof of this assertion is similar to the other and it is left to the reader.
    \end{enumerate}
\end{proof}

\begin{theorem}[Eckart-Young]
  Let $A \in M(n, m, \R)$ and let $A = U \Sigma \tr{V}$ be its singular value decomposition.

  The solution of $\min\limits_{rk(X) \le k} \norm{A-X}_F$ is given by the \emph{truncated SVD}:
  \[
    X = \begin{pmatrix}
      U^1 & U^2 & \cdots & U^k
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
    \sigma_1\\
    & \sigma_2\\
    && \ddots\\
    &&&& \sigma_k\\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
      V^1\\
      V^2\\
      \vdots\\
      V^k\\
    \end{pmatrix}
  \]
  Where the norm is both $\norm{\cdot}_2$ and $\norm{\cdot}_F$.
\end{theorem}

\begin{proposition}
  Let $A \in M(n, \R)$ and let $A$ be invertible. The following holds:
$  \norm{\inv{A}} = \frac{1}{\sigma_n}$
\end{proposition}

\begin{proof}
  Since $A$ is invertible, none of the $\sigma_i$ is $0$, hence the smaller (namely $\sigma_n$) is not $0$.

  \[
    \inv{A} = \inv{(U \Sigma \tr{V})} \numeq{(1)} \inv{\tr{V}} \inv{\Sigma} \inv{U} = V \begin{pmatrix}
      \frac{1}{\sigma_1}\\
      & \frac{1}{\sigma_2}\\
    && \ddots\\
      &&&& \frac{1}{\sigma_n}\\
    \end{pmatrix}
    \tr{U}
  \]
  Where $\numeq{(1)}$ follows from the orthogonlaity of $V$ and $U$.

  Notice that this is \emph{almost} an SVD, because the values on the diagonal are not sorted in a decreasing order.

Plugging in the norm, we have:

  \[
    \norm{\inv{A}}
    = \norm{V \begin{pmatrix}
      \frac{1}{\sigma_1}\\
      & \frac{1}{\sigma_2}\\
    && \ddots\\
      &&&& \frac{1}{\sigma_n}\\
    \end{pmatrix}
    \tr{U}}
  = \norm{
    \begin{pmatrix}
      \frac{1}{\sigma_1}\\
      & \frac{1}{\sigma_2}\\
    && \ddots\\
      &&&& \frac{1}{\sigma_n}\\
    \end{pmatrix}}
  = \frac{1}{\sigma_n}
  \]
\end{proof}
\end{document}
