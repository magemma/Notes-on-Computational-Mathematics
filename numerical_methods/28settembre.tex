%chktex-file 36
%chktex-file 23
%chktex-file 10
%chktex-file 17
%chktex-file 9
%chktex-file 8
%chktex-file 13
\documentclass[computationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\chapter{28th of September 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%

\section{Singular value decomposition (SVD)}
We are left with the task of reaching a (sort of) ``eigenvalue decomposition'' when the target matix is not symmetric.\\
There are two way to generalize the eigenvalue decomposition to a
nonsymmetric matrix A (with something that always exists):

\begin{definition}[Schur decomposition]
  Let $A \in M(n, \R)$, $\exists~U \in M(n, \R)$ orthogonal matrix and $T \in M(n, \R)$ triangular matrix such that $A = UT\tr{U}$ and this is called \textbf{Schur decomposition}.
\end{definition}

What is really important for us is the \textbf{Singular value decomposition}, every square matrix $A$ can be written with \textbf{SVD} form.\\

\noindent Every square matrix $A$ can be written as SVD.

\begin{definition}[Singular value decomposition]
  Let $A \in M(n, \R)$, $\exists~U, V \in M(n, \R)$ orthogonal matrices ($V$ not necessary equal to $U$) and $\Sigma \in \text{Diag}(n, \R)$ such that $A = U\Sigma\tr{V}$ and this is called \textbf{Singular Value Decomposition}.

\[
  A = 
  \begin{pmatrix}
    u_1 & u_2 & \cdots & u_n\\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \sigma_1\\
    & \sigma_2\\
    && \ddots\\
    &&&& \sigma_n\\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    \tr{v_1}\\
    \tr{v_2}\\
  \vdots\\
    \tr{v_n}
  \end{pmatrix}
  = \sum\limits_{i=1}^{n} u_i \sigma_i \tr{v_i} =
\]

  \[
    = u_1 \sigma_1 \tr{v_1} + u_2 \sigma_2 \tr{v_2} + \cdots + u_n \sigma_n \tr{v_n}
\]

  Where $\sigma_i$ are called \textbf{singular values} and they are sorted such that:
  \[
    \sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_n \ge 0
  \]
\indent  General fact on singular values:
  \begin{itemize}
      \item Singular values $\neq$ eigenvalues
      \item They are always positive and usually more ‘spread apart’ than the eigenvalues.
        \[
           \sigma_1 \geq \abs{\lambda_1} \;\; \text{and} \;\; \abs{\lambda_m} \geq \sigma_m
         \]
      $\lambda_i$ is larger than the largest eigenvalue of a matrix $A$ and $\lambda_m$ is smaller than the smallest eigenvalue of a matrix $A$.
  \end{itemize}
\end{definition}

\noindent The SVD can be defined also for a rectangular matrix A:

\begin{definition}[Rectangular matrices and SVD]
  Let $A \in M(m, n, \R)$, there exist $U \in M(m, \R)$ orthogonal, $V \in M(n, \R)$ orthogonal and $\Sigma(m, n, \R)$ diagonal in the sense that $\sum_{ij} = 0$ with $i \neq j$ (padded with zeros). Matrix $A$  has a \textbf{SVD factorization}, where $\Sigma$ has the following shape:
  \begin{itemize}
      \item case m < n  (e.g $m=3, n=5$)
        \[
          \begin{pmatrix}
            \sigma_1&0&0&0&0\\
            0& \sigma_2&0&0&0\\
            0&0&\sigma_3&0&0
          \end{pmatrix}
        \]
        
        \item  case m > n (e.g $m=5, n=3$)
        \[
          \begin{pmatrix}
            \sigma_1&0&0&\\
            0& \sigma_2&0&\\
            0&0&\sigma_3&\\
            0&0&0\\
            0&0&0
          \end{pmatrix}
        \]
  \end{itemize}

\end{definition}

\begin{definition}[Thin SVD]
  Let $A \in M(m, n, \R)$,   has a \textbf{thin} \textbf{SVD factorization}:  we may restrict to compute only the first min(m, n)
vectors that appear in this sum: thin SVD.
  \[
    A = \sum\limits_{i=1}^{\min(m, n)} u_i \sigma_i \tr{v_i} = u_1 \sigma_1 v_1^T + u_2 \sigma_2 v_2^T + \cdots + u_{\min(m,n)} \sigma_{\min(m,n)} \tr{v_{\min(m,n)}}
  \]
\end{definition}

\syntax{In Matlab the SVD decomposition is obtained through the command \texttt{svd(A)}, which return value is made of the three matrices $U, \Sigma, V$.

As an example, \texttt{[U, S, V] = svd(A)}.
Notice that, if \texttt{svd(A)} is assigned to one variable, then such variable is an array of singular values.

The thin SVD can be compute with:\texttt{[U, S, V] = svd(A, 0)}
} 

\noindent \textbf{Computational costs}\\
We are not going into details of algorithms for computing SVD, but we would like to add a consideration about the computational complexity of such an algorithm.
\begin{itemize}
    \item \texttt{[U, S, V] = svd(A, 0)} (thin) costs $O(mn^2)$ ops for $A \in \mathbb{R}^{m \times n}$ or $A \in \mathbb{R}^{n \times m}$ with $m \geq n$
    \item \texttt{[U, S, V] = svd(A)} (non-thin) more expensive, because it has to store the ‘large’ $m \times m$ factor. (But there are some tricks to store orthogonal matrices compactly, more about it later).
\end{itemize}

\subsection{Properties of SVD}
The SVD reveals rank, image, and kernel of a matrix.
\begin{definition}[Rank]
  Let $A \in M(n, \R)$ we call the \textbf{rank} of $A$ the number of non-zero singular values.
  
  Equivalently, the \textbf{rank} is the size of the column space.
\end{definition}


\begin{property}\label{prop:28sett_rank}
  A matrix $A \in M(n, \R)$ has rank $r$ iff all its eigenvalues starting from the $r+1$-th are $0$, formally iff $\sigma_1 \ge \sigma_2 \ge \cdots \ge \sigma_r > \sigma_{r+1} = \cdots = \sigma_n = 0$. 
\end{property}

Thanks to \Cref{prop:28sett_rank}, we can somehow talk about an ``even thinner'' SVD, where all the $0$s in the bottom right part of the matrix $\Sigma$, cancel out the latter columns of $U$ and the latter rows of $V$ (aka columns of $\tr{V}$).
A pictorial representation of the shape of $\Sigma$ can be found below.

\[
  \Sigma ~~=~~ \left(\begin{array}{cccccccccc}
    \sigma_1&&&&&& & & & \\
    &\sigma_2&&&&& & & & \\    
    &&\ddots&&&& & & & \\
    &&&&\tikzmark{left}{$\sigma_r$} \tikzmark{right}{~~} && & \\
    &&&&&0 & & & \\
    &&&&&&&\ddots&&\\
    &&&&&&&&0\\
    \midrule
   &&&&\text{\Large\bfseries 0}\\
  \end{array}\right)
  \Highlight[first]
\]
%
\tikz[overlay,remember picture] {
  \node[above right=2 and -0.16 of first](f) {};
  \node[below left=-0.16 and 2.6 of first](i) {};
  \draw[thick] (f.south) -- (first.south east);
  \draw[thick] (i.east) -- (first.south east);
}

This factorization represents $A$ as $\sum\limits_{i=1}^r U_i \sigma_i V_i$. 

An attentive reader may notice that $Ax = \sum\limits_{i=1}^r U_i \sigma_i V_i x$, where the last three terms are dimensionally a scalar.
It goes without saying that the image of $A$ is the span of $U_1, U_2, \ldots U_r$, hence $rk(A) = r$.

Moreover, $\ker(A)=span(V_{r+1}, V_{r+2}, \ldots V_{n})$, since $V$ is orthgonal (proof: plugging in $x=V_j$, where $j>r$). 

\begin{definition}[Matrix norm]
  Let $A \in M(m, n, \R)$. We define the \textbf{matrix norm} of $A$ as
  \[
    \norm{A} := \max\limits_{x \neq 0} \frac{\norm{Ax}}{\norm{x}} = \max\limits_{z=1}\norm{Az}
  \]
  Where the norm may be any of the ones defined in \Cref{def:20sett_norm} second equality is introduced in order to work in a compact set, the one of normalized vectors $z$.
\end{definition}

Notice that $\norm{Ax} \le \norm{A} \cdot \norm{x}$.

\begin{property}\label{prop:28sett_norms}
  Let $A$ and $B \in M(n, m, \R)$ and let $x \in \R^n$, the following holds, for any norm defined in \Cref{def:20sett_norm}:
  \begin{itemize}
    \item $\norm{A} \ge 0$ (and the equality holds iff $A=0$);
    \item $\norm{\alpha A} = |\alpha| \norm{A}, \forall \alpha \in \R$;
    \item $\norm{A+B} \le \norm{A} + \norm{B}$;
    \item $\norm{AB} \le \norm{A} \norm{B}$;
    \item $\norm{Ax} \le \norm{A} \norm{x}$.
  \end{itemize}
\end{property}

\begin{proposition}\label{fact:28sett_orthogonorm}
  Let $A \in (n, m, \R)$ and let $U \in M(m, n, \R)$ orthogonal, in the case of $2$-norm $\norm{A}_2 = \norm{AU}_2 = \norm{UA}_2$.
\end{proposition}

\begin{proof}
  $\norm{UA}_2 = \max\limits_{x \in R^n,~x \ne 0} \frac{\norm{UAx}_2}{\norm{x}_2} \numeq{(1)} \max\limits_{x \in R^n,~x \ne 0} \frac{\norm{Ax}_2}{\norm{x}_2} = \norm{A}_2$, where $\numeq{(1)}$ follows from a property of vector norms.

 $\norm{AU}_2 = \max\limits_{x \in R^n,~x \ne 0} \frac{\norm{AUx}_2}{\norm{x}_2} \numeq{(2)} \max\limits_{y \in R^n,~y \ne 0} \frac{\norm{Ay}_2}{\norm{y}_2} = \norm{A}_2$, where $\numeq{(2)}$ follows from the substitution $y=Ux$.
\end{proof}

\begin{definition}[Frobenius norm]
  Let $A \in M(n, m, \R)$, we term \textbf{Frobenius norm} of $A$ ${\norm{A}}_F = \sqrt{\sum\limits_{i=1}^n \sum\limits_{j=1}^m {{(A)}_{ij}}^2}$.
\end{definition}

Notice that all the properties enumerated in \Cref{prop:28sett_norms} hold for the Frobenius norm as well.

\begin{proposition}
  Let $A \in M(n, m, \R)$ and let $A = U \Sigma \tr{V}$ be its singular value decomposition.
  The following hold:
  \begin{enumerate}
    \item $\norm{A}_2 = \norm{\Sigma}_2 = \sigma_1$
    \item $\norm{A}_F = \norm{Sigma}_F = \sum\limits_{i=1}^{\min{n, m}} {\sigma_i}^2$
  \end{enumerate}
\end{proposition}

\begin{proof}
  \begin{enumerate}
    \item The first equality follows from \Cref{fact:28sett_orthogonorm}, while the second is proved as follows:
  \begin{equation}
    \begin{split}
      \norm{\Sigma}_2 &=
    \max\limits_{x \in \R^n,~x \neq 0}\frac{\norm{\Sigma x}_2}{\norm{x}_2} =
    \max\limits_{x \in \R^n,~x \neq 0}\frac{\norm{\begin{pmatrix}
    \sigma_1\\
    & \sigma_2\\
    && \ddots\\
    &&&& \sigma_n\\
      &&\text{\Large 0}&&\\
    \end{pmatrix}
    \cdot 
    \begin{pmatrix}
      x_1\\
      x_2\\
      \vdots\\
      x_n\\
    \end{pmatrix}}_2}
      {\norm{\begin{pmatrix}
      x_1\\
      x_2\\
      \vdots\\
      x_n\\
    \end{pmatrix}}_2}
    =
      \max\limits_{x \in \R^n,~x \neq 0}\frac{\norm{\begin{pmatrix}
      \sigma_1 x_1\\
      \sigma_2 x_2\\
      \vdots\\
      \sigma_n x_n\\
      0\\
      \vdots\\
      0\\
      \end{pmatrix}}_2}
      {\norm{\begin{pmatrix}
      x_1\\
      x_2\\
      \vdots\\
      x_n\\
      \end{pmatrix}}_2}\\
      &=
    \frac{\sqrt{{(\sigma_1 x_1)}^2 + {(\sigma_2 x_2)}^2 + \cdots +{(\sigma_n x_n)}^2}}{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}
      \le 
      \frac{\sqrt{{(\sigma_1 x_1)}^2 + {(\sigma_1 x_2)}^2 + \cdots +{(\sigma_1 x_n)}^2}}{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}\\
      &=
    \sqrt{\sigma_1}^2 \cdot \frac{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}{\sqrt{{x_1}^2 + {x_2}^2 + \cdots +{x_n}^2}}
      = \sigma_1
  \end{split}
  \end{equation}

  The equality is achieved if we pick $x = e_1 = \begin{pmatrix}
    1\\
    0\\
    \vdots\\
    0\\
  \end{pmatrix}$.
    \item The proof of this assertion is similar to the other and it is left to the reader.
    \end{enumerate}
\end{proof}

\begin{theorem}[Eckart-Younger]
  Let $A \in M(n, m, \R)$ and let $A = U \Sigma \tr{V}$ be its singular value decomposition.

  The solution of $\min\limits_{rk(X) \le k} \norm{A}-X$ is given by the \emph{truncated SVD}:
  \[
    X = \begin{pmatrix}
      U^1 & U^2 & \cdots & U^k
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
    \sigma_1\\
    & \sigma_2\\
    && \ddots\\
    &&&& \sigma_k\\
    \end{pmatrix}
    \cdot
    \begin{pmatrix}
      V^1\\
      V^2\\
      \vdots\\
      V^k\\
    \end{pmatrix}
  \]
  Where the norm is both $\norm{\cdot}_2$ and $\norm{\cdot}_F$.
\end{theorem}

\begin{proposition}
  Let $A \in M(n, \R)$ and let $A$ be invertible. The following holds:
$  \norm{\inv{A}} = \frac{1}{\sigma_n}$
\end{proposition}

\begin{proof}
  Since $A$ is invertible, none of the $\sigma_i$ is $0$, hence the smaller (namely $\sigma_n$) is not $0$.

  \[
    \inv{A} = \inv{(U \Sigma \tr{V})} \numeq{(1)} \inv{\tr{V}} \inv{\Sigma} \inv{U} = V \begin{pmatrix}
      \frac{1}{\sigma_1}\\
      & \frac{1}{\sigma_2}\\
    && \ddots\\
      &&&& \frac{1}{\sigma_n}\\
    \end{pmatrix}
    \tr{U}
  \]
  Where $\numeq{(1)}$ follows from the orthogonlaity of $V$ and $U$.

  Notice that this is \emph{almost} an SVD, because the values on the diagonal are not sorted in a decreasing order.

Plugging in the norm, we have:

  \[
    \norm{\inv{A}}
    = \norm{V \begin{pmatrix}
      \frac{1}{\sigma_1}\\
      & \frac{1}{\sigma_2}\\
    && \ddots\\
      &&&& \frac{1}{\sigma_n}\\
    \end{pmatrix}
    \tr{U}}
  = \norm{
    \begin{pmatrix}
      \frac{1}{\sigma_1}\\
      & \frac{1}{\sigma_2}\\
    && \ddots\\
      &&&& \frac{1}{\sigma_n}\\
    \end{pmatrix}}
  = \frac{1}{\sigma_n}
  \]
\end{proof}
\end{document}
