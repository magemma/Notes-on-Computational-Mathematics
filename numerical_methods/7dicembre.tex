%chktex-file 36
%chktex-file 8
%chktex-file 24
%chktex-file 35
%chktex-file 44
%chktex-file 1
\documentclass[ComputationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{7th of December 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
In this lecture we are interested in using Arnoldi method to solve linear systems.

We can use the \emph{sparse eigenvalues funtion} that we saw in last lecture and the \textbf{G}eneralized \textbf{M}inimum \textbf{RES}idual (GMRES).

Our task is to approximate the solution of a large-scale linear system of the form $Ax=b$ and our approach is to look for ``the closest thing to solution'' inside $K_n(A,b)$.

Through Arnoldi of $A$, $b$ and $n$, we obtained $[Q, H]$ and we can approximate the solution $x$ as $Q_1y_1 + Q_2 y_2 + \cdots + Q_n y_n = Qy$, which is a good approximation of the solution inside $K_{n}(A, b)$, formally
\[
  \min\limits_{x \in K_{n}(A, b)} \norm{Ax-b},~ x = Q_n y.
\]
which is equivalent to $\min\limits_{y\in\mathbb{R}^n} \norm{AQ_n y - b}$.

We can perform some more reductions and:
\begin{equation}
  \begin{aligned}
    \norm{AQ_n y - b} &\numeq{(1)} \norm{Q_{n+1}\underline{H}_n y - b}\\
    &\numeq{(2)}
    \norm{
      Q_{n+1} {\underline{H}}_n y - Q_{n+1} \norm{b} e_1}\\
    &=
    \norm{
      Q_{n+1} ({\underline{H}}_n y - \norm{b} e_1)}\\
    &\numeq{(3)}
    \norm{\underline{H}_n y - \norm{b}e_1}.
  \end{aligned}
\end{equation}

where $\numeq{(1)}$ is due to the equivalence $AQ_n = Q_{n+1}H_n$, with $H_n \in M(n+1, n)$, $\numeq{(2)}$ follows from the fact that $q_1 = \frac{b}{\norm{b}}$ and $\numeq{(3)}$ is explained recalling that $Q_{n+1}$ is an orthogonal rectangular matrix in $M(m n+1)$ and $\norm{z} = \norm{Q_{n+1}z}$, since $\tr{z}z = \tr{z}\tr{Q_{n+1}}Q_{n+1}z$.

We got a LS problem of size $(n+1)\times n$ (small), where $y \in \R^n$ and $e_1 \in \R^{n+1}$; moreover $\underline{H}$ has the following shape

\[
  H = \left[
\begin{array}{cccc|ccccc}
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\ast & \cdots & \ast & \ast & \ast & \cdots & \cdots & \ast\\
0 & \ddots & \ast & \ast  & \vdots & \vdots & \vdots & \vdots\\
0 & 0 & \ast & \ast & \ast & \cdots & \cdots & \ast\\
\hline
  &&& \circled{\ast} & \ast & \cdots & \ast & \ast\\
&&&& \ast & \cdots & \ast & \ast\\
  &\text{\Huge 0}&&& 0 & \ddots & \ast & \ast \\
&&&& 0 & 0 & \ast & \ast\\
\end{array}
\right]
\]
hence it is quite sparse.

$qr(H)$ can be computed in $O(n^2)$ using the fact that $H$ is `almost triangular' (Hessenberg matrix), although it is not a big optimization, since $n$ Arnoldi steps need to be computed first.

Notice that instead of doing a QR at the end, we can compute QRs of $\underline{H}_1,\underline{H}_2,\dots$ and update them at each step.
This allows us to compute at each step a residual $\norm{Ax_n-b}$ that we can use as stopping criterion.

\syntax{Matlab has \texttt{gmres(A, b)} (and Python has \texttt{scipy.sparse.linalg.gmres}).}

To estimate the convergence of GMRES we can see $x$ as a plynomial ($x=p(A)b$, such that $p(t) = \alpha_0 + \alpha_1 t + \cdots + \alpha_{n-1} t^{n-1}$ is a polynomial of degree $n-1$).

As far as the residual is concerned $Ax - b = A p(A) b -b = A (\alpha_0 I + \alpha_1 A + \cdots + \alpha_{n-1} A^{n-1}) b - b = q(A) b$, where $q(t) = t p(t) -1$.
If $A=V\Lambda V^{-1}$ diagonalizable, then: 
\[
  A^k = V \begin{bmatrix}
  {\lambda_1}^k\\
    & \ddots \\
    & & {\lambda_m}^k
\end{bmatrix}V^{-1}~and~
q(A) = V\begin{bmatrix}
    q(\lambda_1)\\
    & \ddots \\
    & & q(\lambda_m)
\end{bmatrix}V^{-1}
\]

All this computation was needed to write the residual GMRES in a clearer form:

\begin{equation}
  \begin{aligned}
    \min\limits_{x\in K_n(A,b)}\norm{Ax-b}
    &= \min_{\substack{q(x)=xp(x)-1\\\text{of degree $\leq n$}}}\norm{Ap(A)b-b}\\
    &= \min_{\substack{q(x)=xp(x)-1\\\text{of degree $\leq n$}}}\norm{q(A)b}\\
    &\le (\min\limits_{\cdots}\norm{q(A)}) \norm{b}\\
    &= \min\limits_{\cdots}\norm{V\begin{bmatrix}
    q(\lambda_1)\\
    & \ddots \\
    & & q(\lambda_m)
    \end{bmatrix}V^{-1}}\\
    &\le \min\limits_{\cdots}\norm{V} \norm{\begin{bmatrix}
    q(\lambda_1)\\
    & \ddots \\
    & & q(\lambda_m)
    \end{bmatrix}} \norm{\inv{V}}\\ 
    &\le
    K(V) \norm{\min\limits_{\cdots} \begin{bmatrix}
    q(\lambda_1)\\
    & \ddots \\
    & & q(\lambda_m)
    \end{bmatrix}} 
  \end{aligned}
\end{equation}

If $A$ has very few distinct eigenvalues ($k\leq n$ of them), then we can find $q$ such that $q(\lambda_i)=0$ for all $i$ and $q(0) = -1$, hence $n$ steps of GMRES give us the exact solution.

If $A$ has eigenvalues clustered in $n$ points in the plane, we can find a polynomial $q$ such that $q(\lambda_i)$ is small for all $i$.

\addpic{0.5}{pics/7dic/1.png}{In this picture the eigenvalues are clustered around $P_1, P_2, P_3$ and $P_4$. We can find a polynomial $q$ such that $q(\text{red points}) \approx 0$.}{fig:7diceigen}

Notice that Gauss operations on the rows of any matrix $A$ (e.g.~swapping rows or scalar multiplication of a row) change its eigenvalues, without changing the solution.

More generally, given $P \in M(n, \R)$ we can change the problem $Ax=b$ to $PAx = Pb$. If $P$ is invertible, the two systems have the same solution. However, the spectrum of $PA$ may be much better (in the above sense) than the spectrum of $A$, leading to a faster solution with GMRES.

In particular, this happens if we manage to find $P \approx \inv{A}$. The perfect choice would be $P=\inv{A}$, but, of course, if we knew $\inv{A}$ we would already have a way to solve linear systems: just compute the matrix multiplication $\inv{A} b$.

There are various techniques (often problem-dependent) to build effective \emph{preconditioners} $P$. One comes from approximate LU factorizations of $A$ (in a suitable sense); they are known as \emph{incomplete LU} preconditioners.


\end{document}
