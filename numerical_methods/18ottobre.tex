%chktex-file 36
\documentclass[ComputationalMathematics.tex]{subfiles}

\begin{document}

%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%
\section{18th of October 2018 --- F. Poloni}
%%%%%%%%%%%%%%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~%%%%%%%%%%%%%%%


\subsection{Least squares problem}
\begin{proposition}
  Given $A \in \mathcal{M}(m, n, \mathds{R})$ the following conditions are equivalent:
  \begin{itemize}
    \item ${A}^{T}A$ is positive definite;
    \item $A$ has full column rank;
    \item the columns of $A$ are linear indipendent;
    \item $Ker(A) = \{0\}$.
  \end{itemize}
\end{proposition}

\begin{proposition}
  Given $A \in \mathcal{M}(m, n, \mathds{R})$, if $\tr{A} A$ is \textbf{positive semidefinite} $f(x) = \tr{x}Qx -\tr{q}x +\tr{b}b$ is \textbf{strongly} (or \textbf{strictly}) \textbf{convex}. In other words, $f(x)$ has a \textbf{unique} minimum.
\end{proposition}

We find the minimum by solving $\nabla f(x) = 0$, 

where $f(x) = {x}^{T}{A}^{T}Ax - 2{b}^{T}Ax + {b}^{T}b$

$\nabla  f(x) = 2 {A}^{T}Ax - 2{A}^{T}b$

How should we compute this gradient? 

We know that $f(x+h) = f(x) + {(\nabla f(x))}^{T} \dot h  + o(\|h\|)$

\begin{equation}
  \begin{split}
    f(x+h) & = {(x+h)}^{T} {A}^{T}A(x+h) - 2{b}^{T}A(x+h) +{b}^{T}b\\
    & = {\bf \color{blue} {x}^{T}{A}^{T}Ax} + {\bf \color{green} {x}^{T}{A}^{T}Ah} + {\bf \color{green} {h}^{T}{A}^{T}Ax} + {\bf \color{red} {h}^{T}{A}^{T}Ah} {\bf \color{blue}- 2{b}^{T}Ax} - 2{b}^{T}Ah + {\bf \color{blue} {b}^{T}b}\\
    & = {\bf \color{blue}f(x)} +({\bf \color{green} 2 {x}^{T}{A}^{T}Ah}-2{b}^{T}Ah) + {\bf \color{red} o(\|h\|)}\\
    & = f(x) + {\bf \color{magenta} \tr{(\tr{A}Ax-2\tr{A}b)}h} + o(\|h\|) 
  \end{split}
\end{equation}

So, ${\bf \color{magenta} \nabla  f(x) = {A}^{T}Ax - 2{A}^{T}b}$

We would like to know when the gradient is $0$.

$\nabla  f(x) \questeq 0 \Leftrightarrow {A}^{T}Ax = {A}^{T}b$

Since ${A}^{T}A$ is a {\bf square} matrix and also {\bf non singular} (which means invertible) we may find $x$ by solving a linear system $x={({A}^{T}A)}^{-1}({A}^{T}b)$ via:

\begin{itemize}
  \item Gaussian elimination;
  \item LU factorization;
  \item QR factorization;
  \item Cholesky factorization (specialyzed method for positive definite matrices)
    Idea: ${A}^{T}A$ can be written as ${A}^{T}A={R}^{T}R$, where $R$ is a square, upper triangular matrix.
\end{itemize}

Why do we need factorization method?
Let's compute complexity:
\begin{itemize}
  \item ${A}^{T}A ~ \rightarrow ~ 2m{n}^{2}$, where $m>n$;
  \item ${A}^{T}b ~ \rightarrow ~ 2mn$;
  \item Solving ${A}^{T}Ax={A}^{T}b$ with gaussian elimination has a computational complexity of $\frac{2}{3}{n}^{3}$;
  \item Cholesky factorization ${A}^{T}A={R}^{T}R$ which has a cost of $\frac{1}{3}{n}^{3}$.


\end{itemize}

\subsubsection{Method of normal equations}
This method solves least squares problem and takes his name from the fact that ``normal'' means orthogonal.

The key idea is using symmetry to skip half of the entries of ${A}^{T}A$

If $Ax=b$ can't be solved, since $A$ is tall and thin, we can multiply on both sides by ${A}^{T}$ and try again, since the matrix is square now.

The residual $Ax-b$ is orthogonal to any vector $v \in span(A): {(Av)}^{T} (Ax-b)=0$

Why?

$v^T({A}^{T}Ax - {A}^{T}b) = 0$, see \Cref{fig:18ott1}.

\addpic{0.9}{pics/18ott/1.png}{Geometric idea.}{fig:18ott1}

It's possible to find a close formula for solving this problem: $\min\|Ax-b\|$ is given by $x={({A}^{T}A)}^{-1}{A}^{T}b$.

\begin{definition}[Moore-Penrose pseudoinverse]
  Let $A$ be a matrix in $\mathcal{M}(n, m, \mathds{R})$, the \textbf{Moore-Penrose pseudoinverse} of $A$ with full column rank is ${\bf {A}^{+}} := {({A}^{T}A)}^{-1}{A}^{T}$
\end{definition}

So we can write $x={A}^{+}b$ for the solution of a LS problem.

In particular, the solution of $\|\min Ax - ({b}_{1} + {b}_{2})\|$ is the sum of two solutions of $\min \| A{x}_{1} -b_{1} \|$ and  $\min \| A{x}_{2} -b_{2} \|$.

\textbf{Obs:} $A{A}^{+} = A {(\tr{A}A)}^{-1} \tr{A} = A \inv{A} \inv{\tr{A}} \tr{A} = I_{n \times m}$, but this doesn't hold for ${A}^{+}A = {({A}^{T}A)}^{-1}({A}^{T}A)$, which is not the identity matrix.

\subsection{QR factorization}

\begin{theorem}
  $\forall A \in \mathcal{M}(m, m, \mathds{R}),~ \exists  Q \in \mathcal{O}(m, m, \mathds{R})$ (space of orthogonal matrices of size $m\times m$), $\exists R \in \mathcal{M}(m, m, \mathds{R})$ upper triangular such that $A=QR$
\end{theorem}

QR factorization isn't as powerful as SVD factorization.

Why are we interested in studying factorizations?

\begin{itemize}
  \item They reveal properties: singularity, rank, \dots;
  \item They may be an intermediate step in algorithms.
\end{itemize}

\begin{example}
  We would like to solve $Ax=b$, with $A \in \mathcal{M}(m, m, \mathds{R})$ we may:

\begin{enumerate}
  \item first compute the QR factorization ($A=QR$) and then obtain 
$x={A}^{-1}b={R}^{-1}{Q}^{-1}b$
  \item compute then $c= \tr{Q}b$
  \item and then $x={R}^{-1}c$
\end{enumerate}
\end{example}


What's the computational cost?
\begin{enumerate}
  \item $QR ~ \rightarrow ~ O({m}^{3})$
  \item compute c $\rightarrow ~ O({m}^{2})$
  \item compute x $ \rightarrow ~ O({m}^{2})$
\end{enumerate}

Let's analyze the case in which $A$ is a vector. Given $x \in \mathds{R}^m$, we want to find an orthogonal matrix $Q$ such that $Qx$ has the form $\begin{pmatrix}s\\0\\ \cdot\\ \cdot\\ \cdot\\ 0\end{pmatrix} = se_{1}$, where $s= \pm \|x\|$.
  
  We denote $e_{i}$ the i-th column of the identity matrix.

\begin{definition}[Housholder reflector]
  Let $U$ be a vector in $\mathds{R}^m$. An \textbf{Householder reflector} is a matrix $H$ such that $H = I - \frac{2}{\tr{U}U} U \tr{U}$.
  
  We can observe that $\tr{U}U$ is a scalar.

    Since $\tr{U}U = \|U\|^2$, another way of seeing $H$ may be $H = I - \frac{2}{{\|U\|}^{2}} U \tr{U} = I - 2v \tr{v}$, where $v = \frac{1}{\|U\|}U$

  
  \end{definition}
\begin{lemma}
  Matrices of this form are orthogonal.
\end{lemma}

\begin{proof}
  \begin{equation}
    \begin{split}
      H\tr{H} & = (I-\frac{2}{{\|U\|}^{2}} U \tr{U}) (I-\frac{2}{{\|U\|}^{2}} U \tr{U})\\
      & = I I - \frac{2}{\sqrnorm{U}} U \tr{U} I-I \frac{2}{\sqrnorm{U}} U \tr{U} + \frac{2}{\sqrnorm{U}} U \tr{U} \frac{2}{\sqrnorm{U}} U \tr{U}\\
      & = I - \frac{2}{\sqrnorm{U}} U \tr{U} - \frac{2}{\sqrnorm{U}} U \tr{U} + \frac{4}{{\|U\|}^{4}} U\tr{U} U \tr{U}\\
      & = I - \frac{4}{\sqrnorm{U}} U \tr{U} + \frac{4}{{\|U\|}^{4}} U {\|U\|}^{2} \tr{U}\\
      & = I
    \end{split}
  \end{equation}
\end{proof}

\begin{example}
$Hx = (I - \frac{2}{{\|U\|}^{2}} U \tr{U})x = x - \frac{2}{{\|U\|}^{2}} U (\tr{U} U)$

$y = compute\_product(U,x)$

$a = U' * x$

$b= U' * U$

$y = x \frac{2*a}{b} U$

All these operations are linear operations, so the complexity is $O(m)$, cheaper than generic matrix-vector product ($O(m^2)$).
\end{example}

Can we find a matrix $H$ in this family that maps $x$ to $y$ (equivalently $Hx = y$)? The answer is given by the following lemma

\begin{lemma}
 $\forall x, y ~ s.t ~ \|x\| = \|y\| \exists H ~ s.t. ~ Hx = y$, choosing $u = x - y$.
\end{lemma}

A geometric idea is given by the following:

\addtwopics{0.5}{pics/18ott/2.png}{0.46}{pics/18ott/3.png}{On the left in $\mathds{R}^{2}$, on the right in $\mathds{R}^{3}$.}{fig:18ott2}

What happens if $y = \begin{pmatrix} \|x\|\\ 0\\ \cdot\\ \cdot\\ \cdot\\ 0\end{pmatrix}$?
  
  $y = \tr{H}x$, actually $\tr{H} = \inv{H} = H$
  
  Let's map $x$ to $y$: 
  \[
    U = x - y =
  \begin{pmatrix} {}x_{1}\\ {x}_{2}\\ \cdot\\ \cdot\\ \cdot\\{x}_{n}\end{pmatrix} - \begin{pmatrix} s\\ 0\\ \cdot\\ \cdot\\ \cdot\\ 0 \end{pmatrix} = 
    \begin{pmatrix} {x}_{1}-s\\ {x}_{2}\\ \cdot\\ \cdot\\ \cdot\\ {x}_{n} \end{pmatrix}
  \]
      
      where $s = \|x\|$.

\syntax{
{\tt function[v,s] = householder\_vector(x)}, where v and s are the returned values and x is the argument.
}

\end{document}