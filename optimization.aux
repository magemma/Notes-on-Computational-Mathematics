\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\bbl@cs{beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\@ifundefined{tof@begin}{\global \let \tof@begin\relax \global \let \tof@finish\@empty\global \let \tof@starttags\@gobble\global \let \tof@stoptags\@gobble\global \let \tof@tagthis\@gobble\global \let \tof@untagthis\@gobble}{}}
\@writefile{toc}{\tof@begin}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}19th of September 2018}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction to machine learning problems}{7}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Geometric representation of the input. We are interested in finding a model that fits the input data and allows to predict $\bar {y}$ out of $\bar {x}$.\relax }}{8}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:19sett1}{{1.1}{8}{Geometric representation of the input. We are interested in finding a model that fits the input data and allows to predict $\bar {y}$ out of $\bar {x}$.\relax }{figure.caption.2}{}}
\newlabel{fig:19sett1@cref}{{[figure][1][1]1.1}{[1][8][]8}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Optimization}{9}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Linear estimation}{9}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A linear estimation fitting example\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:19sett_le}{{1.2}{10}{A linear estimation fitting example\relax }{figure.caption.3}{}}
\newlabel{fig:19sett_le@cref}{{[figure][2][1]1.2}{[1][9][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Low-rank approximation}{10}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Support vector machines}{11}{subsection.1.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces There are many possible boundaries that can be chosen as a model using many angular coefficients. Our best guess is the one that maximizes the distance between the line and the nearest points.\relax }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:19sett2}{{1.3}{11}{There are many possible boundaries that can be chosen as a model using many angular coefficients. Our best guess is the one that maximizes the distance between the line and the nearest points.\relax }{figure.caption.4}{}}
\newlabel{fig:19sett2@cref}{{[figure][3][1]1.3}{[1][11][]11}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}21st of September 2018}{13}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Mathematical background for optimization problems}{13}{section.2.1}\protected@file@percent }
\newlabel{def:min_prob}{{2.1.1}{13}{Minimum problem}{definition.2.1.1}{}}
\newlabel{def:min_prob@cref}{{[definition][1][2,1]2.1.1}{[1][13][]13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Multi-objective Optimization}{14}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of Pareto frontier\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig:21sett1}{{2.1}{15}{An example of Pareto frontier\relax }{figure.caption.8}{}}
\newlabel{fig:21sett1@cref}{{[figure][1][2]2.1}{[1][15][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Maximize risk-adjusted return\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:21sett_scalar}{{2.2}{15}{Maximize risk-adjusted return\relax }{figure.caption.10}{}}
\newlabel{fig:21sett_scalar@cref}{{[figure][2][2]2.2}{[1][15][]15}}
\newlabel{subfig:21sett_budget1}{{2.3(a)}{16}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{subfig:21sett_budget1@cref}{{[subfigure][1][2,3]2.3(a)}{[1][16][]16}}
\newlabel{sub@subfig:21sett_budget1}{{(a)}{16}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{subfig:21sett_budget2}{{2.3(b)}{16}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{subfig:21sett_budget2@cref}{{[subfigure][2][2,3]2.3(b)}{[1][16][]16}}
\newlabel{sub@subfig:21sett_budget2}{{(b)}{16}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Budgeting\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:21sett_budgeting}{{2.3}{16}{Budgeting\relax }{figure.caption.11}{}}
\newlabel{fig:21sett_budgeting@cref}{{[figure][3][2]2.3}{[1][16][]16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Maximize return with budget on maximum risk, $\qopname \relax m{min}\{f_{1}(x)~: f_{2}(x) \leq \beta ~:x \in S\}$}}}{16}{figure.caption.11}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Minimize risk with budget on minimum return,, $\qopname \relax m{min}\{f_{2}(x)~: f_{1}(x) \leq \beta ~:x \in S\}$}}}{16}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Infima, suprema and extended reals}{16}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}(Monotone) Sequences in $\mathds  {R}$ and optimization}{17}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Vector spaces and topology}{17}{section.2.4}\protected@file@percent }
\newlabel{21sett_sec:vector_space}{{2.4}{17}{Vector spaces and topology}{section.2.4}{}}
\newlabel{21sett_sec:vector_space@cref}{{[section][4][2]2.4}{[1][17][]17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The shapes of balls centered in the origin of radius $1$ varying the value of $p$-norm.\relax }}{20}{figure.caption.12}\protected@file@percent }
\newlabel{fig:21sett1}{{2.4}{20}{The shapes of balls centered in the origin of radius $1$ varying the value of $p$-norm.\relax }{figure.caption.12}{}}
\newlabel{fig:21sett1@cref}{{[figure][4][2]2.4}{[1][20][]20}}
\@writefile{tdo}{\contentsline {todo}{Aggiungere reference al punto in cui si definisce una matrice definita positiva}{21}{section*.13}\protected@file@percent }
\pgfsyspdfmark {pgfid7}{18318621}{34383107}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Limit of a sequence in $\mathds  {R}^n$}{21}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}27th of September 2018}{23}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Case of non-continuity of the objective function in the border point $(0, 0)$.\relax }}{24}{figure.caption.14}\protected@file@percent }
\newlabel{fig:27set0}{{3.1}{24}{Case of non-continuity of the objective function in the border point $(0, 0)$.\relax }{figure.caption.14}{}}
\newlabel{fig:27set0@cref}{{[figure][1][3]3.1}{[1][24][]24}}
\newlabel{subfig:27sett_graph}{{3.2(a)}{25}{Subfigure 3 3.2(a)}{subfigure.3.2.1}{}}
\newlabel{subfig:27sett_graph@cref}{{[subfigure][1][3,2]3.2(a)}{[1][25][]25}}
\newlabel{sub@subfig:27sett_graph}{{(a)}{25}{Subfigure 3 3.2(a)\relax }{subfigure.3.2.1}{}}
\newlabel{subfig:27sett_epi}{{3.2(b)}{25}{Subfigure 3 3.2(b)}{subfigure.3.2.2}{}}
\newlabel{subfig:27sett_epi@cref}{{[subfigure][2][3,2]3.2(b)}{[1][25][]25}}
\newlabel{sub@subfig:27sett_epi}{{(b)}{25}{Subfigure 3 3.2(b)\relax }{subfigure.3.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Graph and epigraph\relax }}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:27sett1}{{3.2}{25}{Graph and epigraph\relax }{figure.caption.15}{}}
\newlabel{fig:27sett1@cref}{{[figure][2][3]3.2}{[1][25][]25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Graph}}}{25}{figure.caption.15}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Epigraph}}}{25}{figure.caption.15}\protected@file@percent }
\newlabel{subfig:27sett_level}{{3.3(a)}{25}{Subfigure 3 3.3(a)}{subfigure.3.3.1}{}}
\newlabel{subfig:27sett_level@cref}{{[subfigure][1][3,3]3.3(a)}{[1][25][]25}}
\newlabel{sub@subfig:27sett_level}{{(a)}{25}{Subfigure 3 3.3(a)\relax }{subfigure.3.3.1}{}}
\newlabel{subfig:27sett_sub}{{3.3(b)}{25}{Subfigure 3 3.3(b)}{subfigure.3.3.2}{}}
\newlabel{subfig:27sett_sub@cref}{{[subfigure][2][3,3]3.3(b)}{[1][25][]25}}
\newlabel{sub@subfig:27sett_sub}{{(b)}{25}{Subfigure 3 3.3(b)\relax }{subfigure.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Level and sub-level sets\relax }}{25}{figure.caption.16}\protected@file@percent }
\newlabel{fig:27sett2}{{3.3}{25}{Level and sub-level sets\relax }{figure.caption.16}{}}
\newlabel{fig:27sett2@cref}{{[figure][3][3]3.3}{[1][25][]25}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Level set}}}{25}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Sub-level set}}}{25}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Continuity}{26}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Derivatives}{26}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Multivariate differentiability}{27}{subsection.3.2.1}\protected@file@percent }
\newlabel{prop:27set1}{{3.2.7}{28}{}{theorem.3.2.7}{}}
\newlabel{prop:27set1@cref}{{[proposition][7][3,2]3.2.7}{[1][28][]28}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}3rd of October 2018}{31}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:3ott}{{4}{31}{3rd of October 2018}{chapter.4}{}}
\newlabel{chap:3ott@cref}{{[chapter][4][]4}{[1][31][]31}}
\newlabel{fact:3ott_gradient}{{4.0.1}{31}{}{theorem.4.0.1}{}}
\newlabel{fact:3ott_gradient@cref}{{[proposition][1][4,0]4.0.1}{[1][31][]31}}
\newlabel{cor:hessian}{{4.0.3}{33}{}{theorem.4.0.3}{}}
\newlabel{cor:hessian@cref}{{[corollary][3][4,0]4.0.3}{[1][33][]33}}
\newlabel{def:taylor_1st}{{4.0.6}{33}{First order Taylor model}{definition.4.0.6}{}}
\newlabel{def:taylor_1st@cref}{{[definition][6][4,0]4.0.6}{[1][33][]33}}
\newlabel{eq:taylor_1st}{{4.0.1}{33}{First order Taylor model}{equation.4.0.1}{}}
\newlabel{eq:taylor_1st@cref}{{[equation][1][4,0]4.0.1}{[1][33][]33}}
\newlabel{def:taylor_2nd}{{4.0.7}{33}{Second order Taylor model}{definition.4.0.7}{}}
\newlabel{def:taylor_2nd@cref}{{[definition][7][4,0]4.0.7}{[1][33][]33}}
\newlabel{eq:taylor_2nd}{{4.0.2}{33}{Second order Taylor model}{equation.4.0.2}{}}
\newlabel{eq:taylor_2nd@cref}{{[equation][2][4,0]4.0.2}{[1][33][]33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Simple functions}{35}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Linear functions}{35}{subsection.4.1.1}\protected@file@percent }
\newlabel{subfloat:3ott_1}{{4.1(a)}{35}{Subfigure 4 4.1(a)}{subfigure.4.1.1}{}}
\newlabel{subfloat:3ott_1@cref}{{[subfigure][1][4,1]4.1(a)}{[1][35][]35}}
\newlabel{sub@subfloat:3ott_1}{{(a)}{35}{Subfigure 4 4.1(a)\relax }{subfigure.4.1.1}{}}
\newlabel{subfloat:3ott_2}{{4.1(b)}{35}{Subfigure 4 4.1(b)}{subfigure.4.1.2}{}}
\newlabel{subfloat:3ott_2@cref}{{[subfigure][2][4,1]4.1(b)}{[1][35][]35}}
\newlabel{sub@subfloat:3ott_2}{{(b)}{35}{Subfigure 4 4.1(b)\relax }{subfigure.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Graphical example of linear function.\relax }}{35}{figure.caption.17}\protected@file@percent }
\newlabel{fig:3ott_1}{{4.1}{35}{Graphical example of linear function.\relax }{figure.caption.17}{}}
\newlabel{fig:3ott_1@cref}{{[figure][1][4]4.1}{[1][35][]35}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Linear function}}}{35}{figure.caption.17}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Level sets}}}{35}{figure.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Quadratic functions}{35}{subsection.4.1.2}\protected@file@percent }
\newlabel{subfloat:3ott_3}{{4.2(a)}{36}{Subfigure 4 4.2(a)}{subfigure.4.2.1}{}}
\newlabel{subfloat:3ott_3@cref}{{[subfigure][1][4,2]4.2(a)}{[1][35][]36}}
\newlabel{sub@subfloat:3ott_3}{{(a)}{36}{Subfigure 4 4.2(a)\relax }{subfigure.4.2.1}{}}
\newlabel{subfloat:3ott_4}{{4.2(b)}{36}{Subfigure 4 4.2(b)}{subfigure.4.2.2}{}}
\newlabel{subfloat:3ott_4@cref}{{[subfigure][2][4,2]4.2(b)}{[1][35][]36}}
\newlabel{sub@subfloat:3ott_4}{{(b)}{36}{Subfigure 4 4.2(b)\relax }{subfigure.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Graphical example of a quadratic function.\relax }}{36}{figure.caption.18}\protected@file@percent }
\newlabel{fig:3ott_2}{{4.2}{36}{Graphical example of a quadratic function.\relax }{figure.caption.18}{}}
\newlabel{fig:3ott_2@cref}{{[figure][2][4]4.2}{[1][35][]36}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Quadratic function}}}{36}{figure.caption.18}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Level sets}}}{36}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}5th of October 2018}{37}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Unconstrained optimization}{37}{section.5.1}\protected@file@percent }
\newlabel{subfloat:5ott_1}{{5.1(a)}{38}{Subfigure 5 5.1(a)}{subfigure.5.1.1}{}}
\newlabel{subfloat:5ott_1@cref}{{[subfigure][1][5,1]5.1(a)}{[1][37][]38}}
\newlabel{sub@subfloat:5ott_1}{{(a)}{38}{Subfigure 5 5.1(a)\relax }{subfigure.5.1.1}{}}
\newlabel{subfloat:5ott_2}{{5.1(b)}{38}{Subfigure 5 5.1(b)}{subfigure.5.1.2}{}}
\newlabel{subfloat:5ott_2@cref}{{[subfigure][2][5,1]5.1(b)}{[1][37][]38}}
\newlabel{sub@subfloat:5ott_2}{{(b)}{38}{Subfigure 5 5.1(b)\relax }{subfigure.5.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Minima and not minima.\relax }}{38}{figure.caption.19}\protected@file@percent }
\newlabel{fig:5ott_1}{{5.1}{38}{Minima and not minima.\relax }{figure.caption.19}{}}
\newlabel{fig:5ott_1@cref}{{[figure][1][5]5.1}{[1][37][]38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Points where the derivative is not $0$.}}}{38}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Points where the derivative is $0$.}}}{38}{figure.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}First order model}{38}{subsection.5.1.1}\protected@file@percent }
\newlabel{prop:step_dir}{{5.1.2}{38}{}{theorem.5.1.2}{}}
\newlabel{prop:step_dir@cref}{{[proposition][2][5,1]5.1.2}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Second order model}{39}{subsection.5.1.2}\protected@file@percent }
\newlabel{fact:5ott_1}{{5.1.3}{39}{}{theorem.5.1.3}{}}
\newlabel{fact:5ott_1@cref}{{[proposition][3][5,1]5.1.3}{[1][39][]39}}
\@writefile{tdo}{\contentsline {todo}{Aggiungere la ref alla variational characterization nella parte di Numerical Methods}{40}{section*.20}\protected@file@percent }
\pgfsyspdfmark {pgfid10}{21857565}{33334058}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Convexity}{40}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Convex sets}{40}{subsection.5.2.1}\protected@file@percent }
\newlabel{subfloat:5ott_3}{{5.2(a)}{41}{Subfigure 5 5.2(a)}{subfigure.5.2.1}{}}
\newlabel{subfloat:5ott_3@cref}{{[subfigure][1][5,2]5.2(a)}{[1][41][]41}}
\newlabel{sub@subfloat:5ott_3}{{(a)}{41}{Subfigure 5 5.2(a)\relax }{subfigure.5.2.1}{}}
\newlabel{subfloat:5ott_4}{{5.2(b)}{41}{Subfigure 5 5.2(b)}{subfigure.5.2.2}{}}
\newlabel{subfloat:5ott_4@cref}{{[subfigure][2][5,2]5.2(b)}{[1][41][]41}}
\newlabel{sub@subfloat:5ott_4}{{(b)}{41}{Subfigure 5 5.2(b)\relax }{subfigure.5.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Example of unitary simplexes.\relax }}{41}{figure.caption.21}\protected@file@percent }
\newlabel{fig:5ott_2}{{5.2}{41}{Example of unitary simplexes.\relax }{figure.caption.21}{}}
\newlabel{fig:5ott_2@cref}{{[figure][2][5]5.2}{[1][41][]41}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {In $\mathds {R}^2$}}}{41}{figure.caption.21}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {In $\mathds {R}^3$}}}{41}{figure.caption.21}\protected@file@percent }
\newlabel{subfloat:5ott_5}{{5.3(a)}{43}{Subfigure 5 5.3(a)}{subfigure.5.3.1}{}}
\newlabel{subfloat:5ott_5@cref}{{[subfigure][1][5,3]5.3(a)}{[1][42][]43}}
\newlabel{sub@subfloat:5ott_5}{{(a)}{43}{Subfigure 5 5.3(a)\relax }{subfigure.5.3.1}{}}
\newlabel{subfloat:5ott_6}{{5.3(b)}{43}{Subfigure 5 5.3(b)}{subfigure.5.3.2}{}}
\newlabel{subfloat:5ott_6@cref}{{[subfigure][2][5,3]5.3(b)}{[1][42][]43}}
\newlabel{sub@subfloat:5ott_6}{{(b)}{43}{Subfigure 5 5.3(b)\relax }{subfigure.5.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Pictorial examples of slicing and projecting.\relax }}{43}{figure.caption.22}\protected@file@percent }
\newlabel{fig:5ott_3}{{5.3}{43}{Pictorial examples of slicing and projecting.\relax }{figure.caption.22}{}}
\newlabel{fig:5ott_3@cref}{{[figure][3][5]5.3}{[1][42][]43}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Slice on $\bar {{\bf y }}$}}}{43}{figure.caption.22}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Project on the $x$ component}}}{43}{figure.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Convex functions}{43}{subsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces $f(x) = {x}^5 - 8 {x}^3 +10x +15$ in blue and its sublevel graph in orange.\relax }}{44}{figure.caption.23}\protected@file@percent }
\newlabel{fig:5ott_4}{{5.4}{44}{$f(x) = {x}^5 - 8 {x}^3 +10x +15$ in blue and its sublevel graph in orange.\relax }{figure.caption.23}{}}
\newlabel{fig:5ott_4@cref}{{[figure][4][5]5.4}{[1][44][]44}}
\newlabel{prop:5ott_1}{{5.2.4}{44}{}{theorem.5.2.4}{}}
\newlabel{prop:5ott_1@cref}{{[proposition][4][5,2]5.2.4}{[1][44][]44}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}11th of October 2018}{47}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{subfloat:11ott_1}{{6.1(a)}{49}{Subfigure 6 6.1(a)}{subfigure.6.1.1}{}}
\newlabel{subfloat:11ott_1@cref}{{[subfigure][1][6,1]6.1(a)}{[1][48][]49}}
\newlabel{sub@subfloat:11ott_1}{{(a)}{49}{Subfigure 6 6.1(a)\relax }{subfigure.6.1.1}{}}
\newlabel{subfloat:11ott_2}{{6.1(b)}{49}{Subfigure 6 6.1(b)}{subfigure.6.1.2}{}}
\newlabel{subfloat:11ott_2@cref}{{[subfigure][2][6,1]6.1(b)}{[1][48][]49}}
\newlabel{sub@subfloat:11ott_2}{{(b)}{49}{Subfigure 6 6.1(b)\relax }{subfigure.6.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Examples of convexity-preserving operations.\relax }}{49}{figure.caption.24}\protected@file@percent }
\newlabel{fig:11ott_1}{{6.1}{49}{Examples of convexity-preserving operations.\relax }{figure.caption.24}{}}
\newlabel{fig:11ott_1@cref}{{[figure][1][6]6.1}{[1][48][]49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Supremum}}}{49}{figure.caption.24}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Perspective}}}{49}{figure.caption.24}\protected@file@percent }
\newlabel{prop:11ott_1}{{6.0.3}{49}{}{theorem.6.0.3}{}}
\newlabel{prop:11ott_1@cref}{{[proposition][3][6,0]6.0.3}{[1][48][]49}}
\newlabel{subfloat:11ott_2_1}{{6.2(a)}{49}{Subfigure 6 6.2(a)}{subfigure.6.2.1}{}}
\newlabel{subfloat:11ott_2_1@cref}{{[subfigure][1][6,2]6.2(a)}{[1][49][]49}}
\newlabel{sub@subfloat:11ott_2_1}{{(a)}{49}{Subfigure 6 6.2(a)\relax }{subfigure.6.2.1}{}}
\newlabel{subfloat:11ott_2_2}{{6.2(b)}{49}{Subfigure 6 6.2(b)}{subfigure.6.2.2}{}}
\newlabel{subfloat:11ott_2_2@cref}{{[subfigure][2][6,2]6.2(b)}{[1][49][]49}}
\newlabel{sub@subfloat:11ott_2_2}{{(b)}{49}{Subfigure 6 6.2(b)\relax }{subfigure.6.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \relax }}{49}{figure.caption.25}\protected@file@percent }
\newlabel{fig:11ott_2}{{6.2}{49}{\relax }{figure.caption.25}{}}
\newlabel{fig:11ott_2@cref}{{[figure][2][6]6.2}{[1][49][]49}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {On a compact set in the interior of the domain (far from the boundaries) the function is Lipschitz continuous.}}}{49}{figure.caption.25}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {If a function is not Lipschitz on a compact subset it is not convex.}}}{49}{figure.caption.25}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Riscrivere proof}{49}{section*.26}\protected@file@percent }
\pgfsyspdfmark {pgfid14}{18318621}{9666177}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}Non-differentiable functions}{50}{subsection.6.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Pictorial example of subgradients of a non diffrentaible function.\relax }}{51}{figure.caption.27}\protected@file@percent }
\newlabel{fig:11ott_3}{{6.3}{51}{Pictorial example of subgradients of a non diffrentaible function.\relax }{figure.caption.27}{}}
\newlabel{fig:11ott_3@cref}{{[figure][3][6]6.3}{[1][50][]51}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces There are many different subgradients in ${\bf  x }$. We pick the one with minimum norm among the ones which have a negative scalar product with ${\bf  x } -{\bf  x_* }$.\relax }}{52}{figure.caption.28}\protected@file@percent }
\newlabel{fig:11ott_4}{{6.4}{52}{There are many different subgradients in $\vect {x}$. We pick the one with minimum norm among the ones which have a negative scalar product with $\vect {x} -\vect {x_*}$.\relax }{figure.caption.28}{}}
\newlabel{fig:11ott_4@cref}{{[figure][4][6]6.4}{[1][51][]52}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}17th of October 2018}{53}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Optimization algorithms}{53}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Gradient method for quadratic functions}{54}{subsection.7.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {7.1.1}{\ignorespaces Pseudocode for quadratic functions local minimum detection.\relax }}{54}{algorithm.7.1.1}\protected@file@percent }
\newlabel{alg:17ottquad}{{7.1.1}{54}{Pseudocode for quadratic functions local minimum detection.\relax }{algorithm.7.1.1}{}}
\newlabel{alg:17ottquad@cref}{{[algorithm][1][7,1]7.1.1}{[1][54][]54}}
\@writefile{tdo}{\contentsline {todo}{Per me da qui non ha senso...}{55}{section*.29}\protected@file@percent }
\pgfsyspdfmark {pgfid17}{18318621}{17671749}
\@writefile{tdo}{\contentsline {todo}{... Fino a qui}{56}{section*.30}\protected@file@percent }
\pgfsyspdfmark {pgfid18}{21857565}{41629056}
\@writefile{tdo}{\contentsline {todo}{Rispetto alle slide, qui denoto con $\bar {f}(\cdot )$ la funzione che quantifica l'errore, anzichè chiamarla $f_*(\cdot )$, perchè questa seconda notazione si confonde con $f_*$ (senza argomento), che è il valore ottimo della funzione obiettivo.}{57}{section*.31}\protected@file@percent }
\pgfsyspdfmark {pgfid19}{18318621}{27716103}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}19th of October 2018}{59}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.1}Gradient method for quadratic functions (cont.)}{59}{subsection.8.0.1}\protected@file@percent }
\newlabel{prop:direction_orto}{{8.0.2}{59}{}{theorem.8.0.2}{}}
\newlabel{prop:direction_orto@cref}{{[proposition][2][8,0]8.0.2}{[1][59][]59}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Some iterations of the gradient method\relax }}{60}{figure.caption.32}\protected@file@percent }
\newlabel{fig:19ottorto}{{8.1}{60}{Some iterations of the gradient method\relax }{figure.caption.32}{}}
\newlabel{fig:19ottorto@cref}{{[figure][1][8]8.1}{[1][59][]60}}
\@writefile{tdo}{\contentsline {todo}{Fare proof, $\bar {f}({\bf  x^{i+1} }) = 1 - \left ( \genfrac  {}{}{}{}{{\left \lVert {\bf  d_i }\right \rVert }^4}{({{\bf  d^{i} }}^{T} Q {\bf  d^{i} }) ({{\bf  d^{i} }}^{T} {Q}^{-1} {\bf  d^{i} })} \right ) \bar {f}({\bf  x^i })$(hint: for $y^i = x^i - x_*$, $d^i = Q y^i$) }{61}{section*.33}\protected@file@percent }
\pgfsyspdfmark {pgfid21}{18318621}{37800727}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.2}Quadratic gradient descent in Matlab}{62}{subsection.8.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Error}{63}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}24th of October 2018}{65}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Gradient method for non quadratic functions}{65}{section.9.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {9.1.1}{\ignorespaces Pseudocode for non quadratic functions local minimum detection.\relax }}{66}{algorithm.9.1.1}\protected@file@percent }
\newlabel{alg:24ottnonQuad}{{9.1.1}{66}{Pseudocode for non quadratic functions local minimum detection.\relax }{algorithm.9.1.1}{}}
\newlabel{alg:24ottnonQuad@cref}{{[algorithm][1][9,1]9.1.1}{[1][65][]66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Finding the best step size}{66}{subsection.9.1.1}\protected@file@percent }
\newlabel{24ottps}{{9.1.2}{67}{}{theorem.9.1.2}{}}
\newlabel{24ottps@cref}{{[proposition][2][9,1]9.1.2}{[1][67][]67}}
\@writefile{toc}{\contentsline {subsubsection}{First order algorithms}{68}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces First, we restrict from $\mathds  {R}$ to $[x_{1}, x_{2}]$, then double $\alpha $ until the derivative is greater than $0$\relax }}{68}{figure.caption.36}\protected@file@percent }
\newlabel{fig:24ott1}{{9.1}{68}{First, we restrict from $\R $ to $[x_{1}, x_{2}]$, then double $\alpha $ until the derivative is greater than $0$\relax }{figure.caption.36}{}}
\newlabel{fig:24ott1@cref}{{[figure][1][9]9.1}{[1][68][]68}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9.1.2}{\ignorespaces First algorithm for exact line search\relax }}{69}{algorithm.9.1.2}\protected@file@percent }
\newlabel{alg:24ottELS1}{{9.1.2}{69}{First algorithm for exact line search\relax }{algorithm.9.1.2}{}}
\newlabel{alg:24ottELS1@cref}{{[algorithm][2][9,1]9.1.2}{[1][68][]69}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9.1.3}{\ignorespaces Bisection algorithm\relax }}{69}{algorithm.9.1.3}\protected@file@percent }
\newlabel{alg:24ottBS}{{9.1.3}{69}{Bisection algorithm\relax }{algorithm.9.1.3}{}}
\newlabel{alg:24ottBS@cref}{{[algorithm][3][9,1]9.1.3}{[1][69][]69}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces The information we have about function $\varphi $\relax }}{70}{figure.caption.37}\protected@file@percent }
\newlabel{fig:24ott3}{{9.2}{70}{The information we have about function $\varphi $\relax }{figure.caption.37}{}}
\newlabel{fig:24ott3@cref}{{[figure][2][9]9.2}{[1][69][]70}}
\newlabel{fact:24ott3}{{9.1.4}{70}{}{theorem.9.1.4}{}}
\newlabel{fact:24ott3@cref}{{[proposition][4][9,1]9.1.4}{[1][70][]70}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces If the function is not ${\mathcal  {C}}^{3}$ and one derivative is very big, then the range does not shrink much.\relax }}{70}{figure.caption.38}\protected@file@percent }
\newlabel{fig:24ott4}{{9.3}{70}{If the function is not $\C {3}$ and one derivative is very big, then the range does not shrink much.\relax }{figure.caption.38}{}}
\newlabel{fig:24ott4@cref}{{[figure][3][9]9.3}{[1][70][]70}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}25th of October 2018}{71}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{Second order algorithms}{71}{section*.39}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Spiegazione: sono nel punto $\alpha ^k$ e stimo $\varphi '(\alpha )$ come $\varphi '(\alpha ^k) + \varphi ''(\alpha ^k) \text  {[coefficiente angolare]} \cdot (\alpha - \alpha ^k)$}{72}{section*.40}\protected@file@percent }
\pgfsyspdfmark {pgfid26}{21857565}{38380911}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10.0.1}{\ignorespaces Pseudocode for Newton method.\relax }}{72}{algorithm.10.0.1}\protected@file@percent }
\newlabel{alg:25ottNewton}{{10.0.1}{72}{Pseudocode for Newton method.\relax }{algorithm.10.0.1}{}}
\newlabel{alg:25ottNewton@cref}{{[algorithm][1][10,0]10.0.1}{[1][72][]72}}
\@writefile{toc}{\contentsline {subsubsection}{Zero order algorithms}{74}{section*.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces The candidate interval is $[x_1, x_2]$, since $\varphi (x_2) > \varphi (x_1)$ and we are allowed to exclude the interval $[x_3, +\infty )$ since the value in $x_3$ is bigger than $\varphi (x_2)$.\relax }}{74}{figure.caption.42}\protected@file@percent }
\newlabel{fig:25ott1}{{10.1}{74}{The candidate interval is $[x_1, x_2]$, since $\varphi (x_2) > \varphi (x_1)$ and we are allowed to exclude the interval $[x_3, +\infty )$ since the value in $x_3$ is bigger than $\varphi (x_2)$.\relax }{figure.caption.42}{}}
\newlabel{fig:25ott1@cref}{{[figure][1][10]10.1}{[1][74][]74}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces The relationship between $r$ and $1-r$ is $r ~ : ~ 1 = (1-r) ~ : ~ r$.\relax }}{74}{figure.caption.43}\protected@file@percent }
\newlabel{fig:25ott2}{{10.2}{74}{The relationship between $r$ and $1-r$ is $r ~ : ~ 1 = (1-r) ~ : ~ r$.\relax }{figure.caption.43}{}}
\newlabel{fig:25ott2@cref}{{[figure][2][10]10.2}{[1][74][]74}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10.0.2}{\ignorespaces Pseudocode for zero-th order approach for local minimum detection.\relax }}{75}{algorithm.10.0.2}\protected@file@percent }
\newlabel{alg:25ottnonDeriv}{{10.0.2}{75}{Pseudocode for zero-th order approach for local minimum detection.\relax }{algorithm.10.0.2}{}}
\newlabel{alg:25ottnonDeriv@cref}{{[algorithm][2][10,0]10.0.2}{[1][74][]75}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Plot of iterates of zero order algorithm for line search. The lines below the $x$ axis represent the different values of $\alpha _-, \alpha _+, \alpha '_-, \alpha '_+$ across different iterations, while the greed dot marks how the value $\alpha $ changes.\relax }}{75}{figure.caption.44}\protected@file@percent }
\newlabel{fig:25ott_zero_order}{{10.3}{75}{Plot of iterates of zero order algorithm for line search. The lines below the $x$ axis represent the different values of $\alpha _-, \alpha _+, \alpha '_-, \alpha '_+$ across different iterations, while the greed dot marks how the value $\alpha $ changes.\relax }{figure.caption.44}{}}
\newlabel{fig:25ott_zero_order@cref}{{[figure][3][10]10.3}{[1][75][]75}}
\@writefile{toc}{\contentsline {subsubsection}{Inexact line search}{76}{section*.45}\protected@file@percent }
\newlabel{subfloat:armijo_2}{{10.4(a)}{77}{Subfigure 10 10.4(a)}{subfigure.10.4.1}{}}
\newlabel{subfloat:armijo_2@cref}{{[subfigure][1][10,4]10.4(a)}{[1][77][]77}}
\newlabel{sub@subfloat:armijo_2}{{(a)}{77}{Subfigure 10 10.4(a)\relax }{subfigure.10.4.1}{}}
\newlabel{subfloat:armijo_3}{{10.4(b)}{77}{Subfigure 10 10.4(b)}{subfigure.10.4.2}{}}
\newlabel{subfloat:armijo_3@cref}{{[subfigure][2][10,4]10.4(b)}{[1][77][]77}}
\newlabel{sub@subfloat:armijo_3}{{(b)}{77}{Subfigure 10 10.4(b)\relax }{subfigure.10.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces Graphical hint of the functioning of Armijo's condition.\relax }}{77}{figure.caption.46}\protected@file@percent }
\newlabel{fig:armijo}{{10.4}{77}{Graphical hint of the functioning of Armijo's condition.\relax }{figure.caption.46}{}}
\newlabel{fig:armijo@cref}{{[figure][4][10]10.4}{[1][77][]77}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Armijo's condition leads to a line which is less steep than the tangent in $\alpha =0$.}}}{77}{figure.caption.46}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Armijo's condition selects those $\alpha \in \mathds {R}$ such that $\varphi (\alpha ) < y_a(\alpha )$.}}}{77}{figure.caption.46}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {10.0.3}{\ignorespaces Pseudocode for backtracking line search.\relax }}{77}{algorithm.10.0.3}\protected@file@percent }
\newlabel{alg:25ottback}{{10.0.3}{77}{Pseudocode for backtracking line search.\relax }{algorithm.10.0.3}{}}
\newlabel{alg:25ottback@cref}{{[algorithm][3][10,0]10.0.3}{[1][77][]77}}
\newlabel{subfloat:goldstein_1}{{10.5(a)}{78}{Subfigure 10 10.5(a)}{subfigure.10.5.1}{}}
\newlabel{subfloat:goldstein_1@cref}{{[subfigure][1][10,5]10.5(a)}{[1][78][]78}}
\newlabel{sub@subfloat:goldstein_1}{{(a)}{78}{Subfigure 10 10.5(a)\relax }{subfigure.10.5.1}{}}
\newlabel{subfloat:goldstein_2}{{10.5(b)}{78}{Subfigure 10 10.5(b)}{subfigure.10.5.2}{}}
\newlabel{subfloat:goldstein_2@cref}{{[subfigure][2][10,5]10.5(b)}{[1][78][]78}}
\newlabel{sub@subfloat:goldstein_2}{{(b)}{78}{Subfigure 10 10.5(b)\relax }{subfigure.10.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces Conjunction of Armijo's and Goldstein's conditions.\relax }}{78}{figure.caption.47}\protected@file@percent }
\newlabel{fig:goldstein}{{10.5}{78}{Conjunction of Armijo's and Goldstein's conditions.\relax }{figure.caption.47}{}}
\newlabel{fig:goldstein@cref}{{[figure][5][10]10.5}{[1][78][]78}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Goldstein's condition leads to a line which is less steep than the tangent in $\alpha =0$ but steeper than Armijo's line.}}}{78}{figure.caption.47}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Armijo's and Goldstein's conditions select those $\alpha \in \mathds {R}$ such that $\varphi (\alpha ) < y_a(\alpha )$ and $\varphi (\alpha ) > y_g(\alpha )$.}}}{78}{figure.caption.47}\protected@file@percent }
\newlabel{subfloat:wolfe_1}{{10.6(a)}{79}{Subfigure 10 10.6(a)}{subfigure.10.6.1}{}}
\newlabel{subfloat:wolfe_1@cref}{{[subfigure][1][10,6]10.6(a)}{[1][78][]79}}
\newlabel{sub@subfloat:wolfe_1}{{(a)}{79}{Subfigure 10 10.6(a)\relax }{subfigure.10.6.1}{}}
\newlabel{subfloat:wolfe_2}{{10.6(b)}{79}{Subfigure 10 10.6(b)}{subfigure.10.6.2}{}}
\newlabel{subfloat:wolfe_2@cref}{{[subfigure][2][10,6]10.6(b)}{[1][78][]79}}
\newlabel{sub@subfloat:wolfe_2}{{(b)}{79}{Subfigure 10 10.6(b)\relax }{subfigure.10.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces Wolfe's condition.\relax }}{79}{figure.caption.48}\protected@file@percent }
\newlabel{fig:wolfe}{{10.6}{79}{Wolfe's condition.\relax }{figure.caption.48}{}}
\newlabel{fig:wolfe@cref}{{[figure][6][10]10.6}{[1][78][]79}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Wolfe's condition fixes a threshold for the slope of the line tangent to the curve $\varphi $.}}}{79}{figure.caption.48}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Wolfe's rule selects those points where $\varphi '(\alpha )>0^-$.}}}{79}{figure.caption.48}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Strong Wolfe's condition.\relax }}{79}{figure.caption.49}\protected@file@percent }
\newlabel{fig:25ott8}{{10.7}{79}{Strong Wolfe's condition.\relax }{figure.caption.49}{}}
\newlabel{fig:25ott8@cref}{{[figure][7][10]10.7}{[1][79][]79}}
\newlabel{prop:25ott}{{10.0.3}{79}{}{theorem.10.0.3}{}}
\newlabel{prop:25ott@cref}{{[proposition][3][10,0]10.0.3}{[1][79][]79}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces $m_1 \approx 1$ and the intersection between Armijo's and Wolfe's ranges.\relax }}{80}{figure.caption.50}\protected@file@percent }
\newlabel{fig:25ott9}{{10.8}{80}{$m_1 \approx 1$ and the intersection between Armijo's and Wolfe's ranges.\relax }{figure.caption.50}{}}
\newlabel{fig:25ott9@cref}{{[figure][8][10]10.8}{[1][80][]80}}
\newlabel{prop:lipshitz}{{10.0.5}{80}{}{theorem.10.0.5}{}}
\newlabel{prop:lipshitz@cref}{{[proposition][5][10,0]10.0.5}{[1][80][]80}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces If $\varphi $ is not going to $-\infty $ the Armijo's line (blue) and the function will meet in $\bar {\alpha } > 0$.\relax }}{81}{figure.caption.51}\protected@file@percent }
\newlabel{fig:25ott10}{{10.9}{81}{If $\varphi $ is not going to $-\infty $ the Armijo's line (blue) and the function will meet in $\bar {\alpha } > 0$.\relax }{figure.caption.51}{}}
\newlabel{fig:25ott10@cref}{{[figure][9][10]10.9}{[1][80][]81}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}8th of November 2018}{83}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Good practices for the project's design}{83}{section.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {12}14th of November 2018}{85}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {12.1}General descent methods}{85}{section.12.1}\protected@file@percent }
\newlabel{theo:zoutendijk}{{12.1.1}{86}{Zoutendijk's theorem}{theorem.12.1.1}{}}
\newlabel{theo:zoutendijk@cref}{{[theorem][1][12,1]12.1.1}{[1][86][]86}}
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Newton's method}{86}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.1}Convex case}{86}{subsection.12.2.1}\protected@file@percent }
\newlabel{theo:newton_min}{{12.2.1}{87}{}{theorem.12.2.1}{}}
\newlabel{theo:newton_min@cref}{{[theorem][1][12,2]12.2.1}{[1][86][]87}}
\newlabel{theo:newton}{{12.2.2}{88}{}{theorem.12.2.2}{}}
\newlabel{theo:newton@cref}{{[theorem][2][12,2]12.2.2}{[1][87][]88}}
\@writefile{tdo}{\contentsline {todo}{Aggiungere la ref alla variational characterization nella parte di Numerical Methods}{88}{section*.52}\protected@file@percent }
\pgfsyspdfmark {pgfid31}{21857565}{27953764}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.2}Interpretation of Newton's method}{89}{subsection.12.2.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Riscrivere meglio}{89}{section*.53}\protected@file@percent }
\pgfsyspdfmark {pgfid32}{18318621}{17155335}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2.3}Non convex case}{90}{subsection.12.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {13}16th of November 2018}{93}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Quasi-newton methods}{94}{section.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Davidson-Fletcher-Powell}{94}{subsection.13.1.1}\protected@file@percent }
\newlabel{theo:16novsmw}{{13.1.3}{95}{Sherman-Morrison-Woodbury}{theorem.13.1.3}{}}
\newlabel{theo:16novsmw@cref}{{[theorem][3][13,1]13.1.3}{[1][95][]95}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.2}Broyden-Fletcher-Goldfarb-Shanno}{95}{subsection.13.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.3}Poorman's approach}{95}{subsection.13.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.2}Conjugate gradient method}{96}{section.13.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13.1}{\ignorespaces Geometric idea on how the new direction is chosen.\relax }}{96}{figure.caption.54}\protected@file@percent }
\newlabel{fig:16nov1}{{13.1}{96}{Geometric idea on how the new direction is chosen.\relax }{figure.caption.54}{}}
\newlabel{fig:16nov1@cref}{{[figure][1][13]13.1}{[1][96][]96}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {13.2.1}{\ignorespaces Pseudocode for conjugate gradient method for quadratic functions.\relax }}{97}{algorithm.13.2.1}\protected@file@percent }
\newlabel{alg:CGQ1}{{13.2.1}{97}{Pseudocode for conjugate gradient method for quadratic functions.\relax }{algorithm.13.2.1}{}}
\newlabel{alg:CGQ1@cref}{{[algorithm][1][13,2]13.2.1}{[1][96][]97}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {13.2.2}{\ignorespaces Pseudocode for conjugate gradient method for arbitrary functions\relax }}{97}{algorithm.13.2.2}\protected@file@percent }
\newlabel{alg:16novCGA}{{13.2.2}{97}{Pseudocode for conjugate gradient method for arbitrary functions\relax }{algorithm.13.2.2}{}}
\newlabel{alg:16novCGA@cref}{{[algorithm][2][13,2]13.2.2}{[1][97][]97}}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}22nd of November 2018}{99}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Deflected gradient methods}{99}{section.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}Heavy ball gradient method}{99}{subsection.14.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.1}{\ignorespaces On the left side Newton method and on the right side the heavy ball method.\relax }}{100}{figure.caption.55}\protected@file@percent }
\newlabel{fig:22nov1}{{14.1}{100}{On the left side Newton method and on the right side the heavy ball method.\relax }{figure.caption.55}{}}
\newlabel{fig:22nov1@cref}{{[figure][1][14]14.1}{[1][100][]100}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.2}Accelerated gradient}{100}{subsection.14.1.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {14.1.1}{\ignorespaces Pseudocode for accelerated gradient method.\relax }}{101}{algorithm.14.1.1}\protected@file@percent }
\newlabel{alg:22novACCG}{{14.1.1}{101}{Pseudocode for accelerated gradient method.\relax }{algorithm.14.1.1}{}}
\newlabel{alg:22novACCG@cref}{{[algorithm][1][14,1]14.1.1}{[1][100][]101}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.2}{\ignorespaces We build a linear model, which is a lowerbound for the function, then we build a quadratic model, which is above our function.\relax }}{101}{figure.caption.56}\protected@file@percent }
\newlabel{fig:22nov3}{{14.2}{101}{We build a linear model, which is a lowerbound for the function, then we build a quadratic model, which is above our function.\relax }{figure.caption.56}{}}
\newlabel{fig:22nov3@cref}{{[figure][2][14]14.2}{[1][101][]101}}
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Incremental gradient methods}{102}{section.14.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14.3}{\ignorespaces This function has a lot of kinky points.\relax }}{103}{figure.caption.57}\protected@file@percent }
\newlabel{fig:22novlasso}{{14.3}{103}{This function has a lot of kinky points.\relax }{figure.caption.57}{}}
\newlabel{fig:22novlasso@cref}{{[figure][3][14]14.3}{[1][103][]103}}
\@writefile{toc}{\contentsline {section}{\numberline {14.3}Subgradient methods}{103}{section.14.3}\protected@file@percent }
\newlabel{prop:22nov1}{{14.3.1}{103}{}{theorem.14.3.1}{}}
\newlabel{prop:22nov1@cref}{{[property][1][14,3]14.3.1}{[1][103][]103}}
\@writefile{lof}{\contentsline {figure}{\numberline {14.4}{\ignorespaces Since we know that $<g, x^*>$ is negative we get that the angle between $g$ and $x^*$ is larger than $90^{\circ }$. Moreover, we know where the optimum is not (red region), hence we restrict to the green region. At this point we will perform a line search on $g$ direction. Since the function is not smooth, the line search may not succeed since the value of the function along the half line from $x$ in $g$ direction may remain the same, but there may be some points there that are closer to $x^*$.\relax }}{104}{figure.caption.58}\protected@file@percent }
\newlabel{fig:22nov4}{{14.4}{104}{Since we know that $<g, x^*>$ is negative we get that the angle between $g$ and $x^*$ is larger than $90^{\circ }$. Moreover, we know where the optimum is not (red region), hence we restrict to the green region. At this point we will perform a line search on $g$ direction. Since the function is not smooth, the line search may not succeed since the value of the function along the half line from $x$ in $g$ direction may remain the same, but there may be some points there that are closer to $x^*$.\relax }{figure.caption.58}{}}
\newlabel{fig:22nov4@cref}{{[figure][4][14]14.4}{[1][103][]104}}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}28th of November 2018}{105}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Subgradient methods}{105}{section.15.1}\protected@file@percent }
\newlabel{eq:28nov1}{{15.1.1}{105}{Subgradient methods}{equation.15.1.1}{}}
\newlabel{eq:28nov1@cref}{{[equation][1][15,1]15.1.1}{[1][105][]105}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1.1}Target level stepsize}{107}{subsection.15.1.1}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {15.1.1}{\ignorespaces Pseudocode for target level stepsize.\relax }}{108}{algorithm.15.1.1}\protected@file@percent }
\newlabel{alg28nov:nonSGPTL}{{15.1.1}{108}{Pseudocode for target level stepsize.\relax }{algorithm.15.1.1}{}}
\newlabel{alg28nov:nonSGPTL@cref}{{[algorithm][1][15,1]15.1.1}{[1][108][]108}}
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Deflected subgradient}{108}{section.15.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces There are two cases: either the function value is significanlty below the reference (for example $\genfrac  {}{}{}{}{\delta }{2}$ below the reference) or it's not . If we are in the ``happy case'' (we just move the reference to the best value). For what conserns the ``unhappy case'', if we are unhapy for 1 iteration, no problem. Two iterations? no prolem. Many iterations? Problem: after some iterations in which we are not improving it means that we have to decreas the reference values. How? $r$ is updated at each bad step and reset when a good step occurs. When $r$ gets too large we decrease $\delta $ and reset everything.\relax }}{109}{figure.caption.59}\protected@file@percent }
\newlabel{fig:28nov1}{{15.1}{109}{There are two cases: either the function value is significanlty below the reference (for example $\frac {\delta }{2}$ below the reference) or it's not . If we are in the ``happy case'' (we just move the reference to the best value). For what conserns the ``unhappy case'', if we are unhapy for 1 iteration, no problem. Two iterations? no prolem. Many iterations? Problem: after some iterations in which we are not improving it means that we have to decreas the reference values. How? $r$ is updated at each bad step and reset when a good step occurs. When $r$ gets too large we decrease $\delta $ and reset everything.\relax }{figure.caption.59}{}}
\newlabel{fig:28nov1@cref}{{[figure][1][15]15.1}{[1][108][]109}}
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Smoothed gradient methods}{109}{section.15.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces Let us take the absolute value function $f(x) = |x|$. This function would be differentiable, if it wasn't for some nasty points (in this case $0$, were the optimization problem has many optimal solutions). In $0$ there are many subgradients, so it has many optima.\relax }}{110}{figure.caption.60}\protected@file@percent }
\newlabel{fig:28nov2}{{15.2}{110}{Let us take the absolute value function $f(x) = |x|$. This function would be differentiable, if it wasn't for some nasty points (in this case $0$, were the optimization problem has many optimal solutions). In $0$ there are many subgradients, so it has many optima.\relax }{figure.caption.60}{}}
\newlabel{fig:28nov2@cref}{{[figure][2][15]15.2}{[1][109][]110}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.3}{\ignorespaces Geometric intuition of the usage of variable $\mu $.\relax }}{110}{figure.caption.61}\protected@file@percent }
\newlabel{fig:28nov3}{{15.3}{110}{Geometric intuition of the usage of variable $\mu $.\relax }{figure.caption.61}{}}
\newlabel{fig:28nov3@cref}{{[figure][3][15]15.3}{[1][110][]110}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.4}{\ignorespaces At the beginning we eill make a lot of bad steps (the upper gray line). We can improve (pick the black line) changing the $\varepsilon $ value and we obtain something that looks more stable. The more precision we want, the smaller the step we make. At the ending it pays, but at the beginning it is not so.\relax }}{111}{figure.caption.62}\protected@file@percent }
\newlabel{fig:28nov4}{{15.4}{111}{At the beginning we eill make a lot of bad steps (the upper gray line). We can improve (pick the black line) changing the $\eps $ value and we obtain something that looks more stable. The more precision we want, the smaller the step we make. At the ending it pays, but at the beginning it is not so.\relax }{figure.caption.62}{}}
\newlabel{fig:28nov4@cref}{{[figure][4][15]15.4}{[1][111][]111}}
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Cutting-plane algorithm}{111}{section.15.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.5}{\ignorespaces The idea is that we do not compute a subgradient and discard it. We store it, thus resulting in a model\relax }}{112}{figure.caption.63}\protected@file@percent }
\newlabel{fig:28nov5}{{15.5}{112}{The idea is that we do not compute a subgradient and discard it. We store it, thus resulting in a model\relax }{figure.caption.63}{}}
\newlabel{fig:28nov5@cref}{{[figure][5][15]15.5}{[1][111][]112}}
\@writefile{lof}{\contentsline {figure}{\numberline {15.6}{\ignorespaces A geometric representation of how the model (blue line) changes after one iteration.\relax }}{112}{figure.caption.64}\protected@file@percent }
\newlabel{fig:28nov6}{{15.6}{112}{A geometric representation of how the model (blue line) changes after one iteration.\relax }{figure.caption.64}{}}
\newlabel{fig:28nov6@cref}{{[figure][6][15]15.6}{[1][112][]112}}
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Bundle methods}{113}{section.15.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {15.5.1}{\ignorespaces Pseudocode for boundle method.\relax }}{113}{algorithm.15.5.1}\protected@file@percent }
\newlabel{alg:28novPBM}{{15.5.1}{113}{Pseudocode for boundle method.\relax }{algorithm.15.5.1}{}}
\newlabel{alg:28novPBM@cref}{{[algorithm][1][15,5]15.5.1}{[1][113][]113}}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}30th of November 2018}{115}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Constrained optimization}{115}{section.16.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces The red line is level set of the function corresponding to the smallest value that touches the set $X$. The point in the intersection is not a saddle point of the function $f$, altough it is the minimum.\relax }}{115}{figure.caption.65}\protected@file@percent }
\newlabel{fig:30nov1}{{16.1}{115}{The red line is level set of the function corresponding to the smallest value that touches the set $X$. The point in the intersection is not a saddle point of the function $f$, altough it is the minimum.\relax }{figure.caption.65}{}}
\newlabel{fig:30nov1@cref}{{[figure][1][16]16.1}{[1][115][]115}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Linear equality constraints}{116}{subsection.16.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces Linear constraint and a point on the boundary.\relax }}{116}{figure.caption.66}\protected@file@percent }
\newlabel{fig:30nov2}{{16.2}{116}{Linear constraint and a point on the boundary.\relax }{figure.caption.66}{}}
\newlabel{fig:30nov2@cref}{{[figure][2][16]16.2}{[1][116][]116}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.3}{\ignorespaces The gradient is orthogonal to the level set in that point, when the function is smooth. The same holds for matrix $A$.\relax }}{116}{figure.caption.67}\protected@file@percent }
\newlabel{fig:30nov3}{{16.3}{116}{The gradient is orthogonal to the level set in that point, when the function is smooth. The same holds for matrix $A$.\relax }{figure.caption.67}{}}
\newlabel{fig:30nov3@cref}{{[figure][3][16]16.3}{[1][116][]116}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.4}{\ignorespaces If we take any other point in the space it has to be orthogonal to $A$.\relax }}{117}{figure.caption.68}\protected@file@percent }
\newlabel{fig:30nov4}{{16.4}{117}{If we take any other point in the space it has to be orthogonal to $A$.\relax }{figure.caption.68}{}}
\newlabel{fig:30nov4@cref}{{[figure][4][16]16.4}{[1][116][]117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}Background for linear inequality contstraints}{118}{subsection.16.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.5}{\ignorespaces When the boundary has this shape we can move along directions that point inside the constraints. Tangent directions are not allowed.\relax }}{118}{figure.caption.69}\protected@file@percent }
\newlabel{fig:30nov4}{{16.5}{118}{When the boundary has this shape we can move along directions that point inside the constraints. Tangent directions are not allowed.\relax }{figure.caption.69}{}}
\newlabel{fig:30nov4@cref}{{[figure][5][16]16.5}{[1][118][]118}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.6}{\ignorespaces Geometric representation of tangent cone, which is the region of the space where we can pick the directions. The intuition is to zoom in $x$ and the result of this zooming is a cone.\relax }}{118}{figure.caption.70}\protected@file@percent }
\newlabel{fig:30nov5}{{16.6}{118}{Geometric representation of tangent cone, which is the region of the space where we can pick the directions. The intuition is to zoom in $x$ and the result of this zooming is a cone.\relax }{figure.caption.70}{}}
\newlabel{fig:30nov5@cref}{{[figure][6][16]16.6}{[1][118][]118}}
\newlabel{teo:30nov1}{{16.1.4}{118}{}{theorem.16.1.4}{}}
\newlabel{teo:30nov1@cref}{{[theorem][4][16,1]16.1.4}{[1][118][]118}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.7}{\ignorespaces Convex function.\relax }}{119}{figure.caption.71}\protected@file@percent }
\newlabel{fig:30nov5}{{16.7}{119}{Convex function.\relax }{figure.caption.71}{}}
\newlabel{fig:30nov5@cref}{{[figure][7][16]16.7}{[1][119][]119}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.8}{\ignorespaces Let us suppose we pick the direction towards the left part of the function in the saddle point. That direction is promising and actually the value of the function decreases. In this case the problem is that the contraint is not convex.\relax }}{119}{figure.caption.72}\protected@file@percent }
\newlabel{fig:30nov6}{{16.8}{119}{Let us suppose we pick the direction towards the left part of the function in the saddle point. That direction is promising and actually the value of the function decreases. In this case the problem is that the contraint is not convex.\relax }{figure.caption.72}{}}
\newlabel{fig:30nov6@cref}{{[figure][8][16]16.8}{[1][119][]119}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.9}{\ignorespaces The first-order feasible direction cone is made of those directions that are orthogonal to the gradient of the constraints $g_1(x)$ and $g_2(x)$ in $x$.\relax }}{121}{figure.caption.73}\protected@file@percent }
\newlabel{fig:30nov6}{{16.9}{121}{The first-order feasible direction cone is made of those directions that are orthogonal to the gradient of the constraints $g_1(x)$ and $g_2(x)$ in $x$.\relax }{figure.caption.73}{}}
\newlabel{fig:30nov6@cref}{{[figure][9][16]16.9}{[1][121][]121}}
\@writefile{lof}{\contentsline {figure}{\numberline {16.10}{\ignorespaces The circle represents the quadrati constraint, while the semi-plane represents the first degree constraint.\relax }}{122}{figure.caption.74}\protected@file@percent }
\newlabel{fig:30nov7}{{16.10}{122}{The circle represents the quadrati constraint, while the semi-plane represents the first degree constraint.\relax }{figure.caption.74}{}}
\newlabel{fig:30nov7@cref}{{[figure][10][16]16.10}{[1][121][]122}}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}6th of December 2018}{125}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Duality}{125}{section.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Lagrangian duality}{126}{section.17.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Specialized dual}{128}{section.17.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}Linead programs}{128}{subsection.17.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.2}Quadratic programs}{128}{subsection.17.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.3}Conic program}{129}{subsection.17.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces $x^3 \geq \sqrt  {x_1^2 + x_2^2}$ is a convex cone.\relax }}{129}{figure.caption.75}\protected@file@percent }
\newlabel{fig:6dic_1}{{17.1}{129}{$x^3 \ge \sqrt {x_1^2 + x_2^2}$ is a convex cone.\relax }{figure.caption.75}{}}
\newlabel{fig:6dic_1@cref}{{[figure][1][17]17.1}{[1][129][]129}}
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Fenchel's duality}{130}{section.17.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {18}12th of December 2018}{131}{chapter.18}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {18.1}Quadratic problem with linear equality constraints}{131}{section.18.1}\protected@file@percent }
\newlabel{sec:12dic_lec}{{18.1}{131}{Quadratic problem with linear equality constraints}{section.18.1}{}}
\newlabel{sec:12dic_lec@cref}{{[section][1][18]18.1}{[1][131][]131}}
\@writefile{toc}{\contentsline {section}{\numberline {18.2}Inequality constrained problems}{133}{section.18.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.1}Projected gradient method}{133}{subsection.18.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18.1}{\ignorespaces Geometric idea of how to choose the direction in case of lying on the boundary.\relax }}{133}{figure.caption.76}\protected@file@percent }
\newlabel{fig:12dic1}{{18.1}{133}{Geometric idea of how to choose the direction in case of lying on the boundary.\relax }{figure.caption.76}{}}
\newlabel{fig:12dic1@cref}{{[figure][1][18]18.1}{[1][133][]133}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {18.2.1}{\ignorespaces Pseudocode for projected gradient method for quadratic functions.\relax }}{135}{algorithm.18.2.1}\protected@file@percent }
\newlabel{alg:PGM}{{18.2.1}{135}{Pseudocode for projected gradient method for quadratic functions.\relax }{algorithm.18.2.1}{}}
\newlabel{alg:PGM@cref}{{[algorithm][1][18,2]18.2.1}{[1][135][]135}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.2}Projected gradient method with box constraints}{136}{subsection.18.2.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {18.2.2}{\ignorespaces Pseudocode for projected gradient method for quadratic functions in the case of box constraints.\relax }}{137}{algorithm.18.2.2}\protected@file@percent }
\newlabel{alg:PGMBC}{{18.2.2}{137}{Pseudocode for projected gradient method for quadratic functions in the case of box constraints.\relax }{algorithm.18.2.2}{}}
\newlabel{alg:PGMBC@cref}{{[algorithm][2][18,2]18.2.2}{[1][136][]137}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.3}Active-set method for quadratic programs}{137}{subsection.18.2.3}\protected@file@percent }
\newlabel{sec:12dic_active}{{18.2.3}{137}{Active-set method for quadratic programs}{subsection.18.2.3}{}}
\newlabel{sec:12dic_active@cref}{{[subsection][3][18,2]18.2.3}{[1][137][]137}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {18.2.3}{\ignorespaces Pseudocode for active set method for quadratic functions.\relax }}{138}{algorithm.18.2.3}\protected@file@percent }
\newlabel{alg:ASMQP}{{18.2.3}{138}{Pseudocode for active set method for quadratic functions.\relax }{algorithm.18.2.3}{}}
\newlabel{alg:ASMQP@cref}{{[algorithm][3][18,2]18.2.3}{[1][137][]138}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.2.4}Frank-Wolfe method}{138}{subsection.18.2.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {18.2.4}{\ignorespaces Pseudocode for Frank-Wolf method for non linear functions.\relax }}{139}{algorithm.18.2.4}\protected@file@percent }
\newlabel{alg:FWM}{{18.2.4}{139}{Pseudocode for Frank-Wolf method for non linear functions.\relax }{algorithm.18.2.4}{}}
\newlabel{alg:FWM@cref}{{[algorithm][4][18,2]18.2.4}{[1][138][]139}}
\@writefile{toc}{\contentsline {chapter}{\numberline {19}14th of December 2018}{141}{chapter.19}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {19.1}Dual methods for linear constrained optimization}{141}{section.19.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.1.1}Separable problems and partial dual}{142}{subsection.19.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {19.2}Primal/dual methods or barrier methods}{142}{section.19.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {19.2.1}Barrier function and central path}{142}{subsection.19.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19.1}{\ignorespaces The trajectory converges to the optimal solution of the problem, when $\mu \rightarrow 0$.\relax }}{143}{figure.caption.77}\protected@file@percent }
\newlabel{fig:14dic1}{{19.1}{143}{The trajectory converges to the optimal solution of the problem, when $\mu \to 0$.\relax }{figure.caption.77}{}}
\newlabel{fig:14dic1@cref}{{[figure][1][19]19.1}{[1][143][]143}}
\@writefile{lof}{\contentsline {figure}{\numberline {19.2}{\ignorespaces The dotted line represents a region where the Newton method is very efficient. We are starting from a point $x_1$, which belongs to that region and we want to move towards $x_{\mu ^1}$. At next iterate $x_2$ is closer to $x_{\mu ^2}$ than the current iterate. \relax }}{143}{figure.caption.78}\protected@file@percent }
\newlabel{fig:14dic2}{{19.2}{143}{The dotted line represents a region where the Newton method is very efficient. We are starting from a point $x_1$, which belongs to that region and we want to move towards $x_{\mu ^1}$. At next iterate $x_2$ is closer to $x_{\mu ^2}$ than the current iterate. \relax }{figure.caption.78}{}}
\newlabel{fig:14dic2@cref}{{[figure][2][19]19.2}{[1][143][]143}}
\newlabel{eq:14dic1}{{19.2.1}{144}{Barrier function and central path}{equation.19.2.1}{}}
\newlabel{eq:14dic1@cref}{{[equation][1][19,2]19.2.1}{[1][144][]144}}
\@writefile{toc}{\contentsline {section}{\numberline {19.3}Primal-dual interior point method}{145}{section.19.3}\protected@file@percent }
\newlabel{eq:14dic2}{{19.3.1}{145}{Primal-dual interior point method}{equation.19.3.1}{}}
\newlabel{eq:14dic2@cref}{{[equation][1][19,3]19.3.1}{[1][145][]145}}
\newlabel{eq:14dic3}{{19.3.2}{145}{Primal-dual interior point method}{equation.19.3.2}{}}
\newlabel{eq:14dic3@cref}{{[equation][2][19,3]19.3.2}{[1][145][]145}}
\expandafter\gdef\csname eqp@this@M\endcsname{6.38902pt}
\expandafter\gdef\csname eqp@next@M\endcsname{0pt}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  D35715DF6373C968BFDEB90BC99B17BDE2604F6C7487FF7DA7EA82CF26AA5CE8.pygtex}
\@writefile{toc}{\tof@finish}
