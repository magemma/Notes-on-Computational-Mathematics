\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\bbl@cs{beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\@ifundefined{tof@begin}{\global \let \tof@begin\relax \global \let \tof@finish\@empty\global \let \tof@starttags\@gobble\global \let \tof@stoptags\@gobble\global \let \tof@tagthis\@gobble\global \let \tof@untagthis\@gobble}{}}
\@writefile{toc}{\tof@begin}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}19th of September 2018}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Introduction to machine learning problems}{5}{section.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Geometric representation of the input. We are interested in finding a model that fits the input data and allows to predict $\bar {y}$ out of $\bar {x}$.\relax }}{6}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:19sett1}{{1.1}{6}{Geometric representation of the input. We are interested in finding a model that fits the input data and allows to predict $\bar {y}$ out of $\bar {x}$.\relax }{figure.caption.2}{}}
\newlabel{fig:19sett1@cref}{{[figure][1][1]1.1}{[1][6][]6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Optimization}{7}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Linear estimation}{7}{subsection.1.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces A linear estimation fitting example\relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig:19sett_le}{{1.2}{8}{A linear estimation fitting example\relax }{figure.caption.3}{}}
\newlabel{fig:19sett_le@cref}{{[figure][2][1]1.2}{[1][7][]8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Low-rank approximation}{8}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Support vector machines}{9}{subsection.1.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces There are many possible boundaries that can be chosen as a model using many angular coefficients. Our best guess is the one that maximizes the distance between the line and the nearest points.\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:19sett2}{{1.3}{9}{There are many possible boundaries that can be chosen as a model using many angular coefficients. Our best guess is the one that maximizes the distance between the line and the nearest points.\relax }{figure.caption.4}{}}
\newlabel{fig:19sett2@cref}{{[figure][3][1]1.3}{[1][9][]9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}21st of September 2018}{11}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Mathematical background for optimization problems}{11}{section.2.1}\protected@file@percent }
\newlabel{def:min_prob}{{2.1.1}{11}{Minimum problem}{definition.2.1.1}{}}
\newlabel{def:min_prob@cref}{{[definition][1][2,1]2.1.1}{[1][11][]11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Multi-objective Optimization}{12}{subsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of Pareto frontier\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:21sett1}{{2.1}{13}{An example of Pareto frontier\relax }{figure.caption.8}{}}
\newlabel{fig:21sett1@cref}{{[figure][1][2]2.1}{[1][13][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Maximize risk-adjusted return\relax }}{13}{figure.caption.10}\protected@file@percent }
\newlabel{fig:21sett_scalar}{{2.2}{13}{Maximize risk-adjusted return\relax }{figure.caption.10}{}}
\newlabel{fig:21sett_scalar@cref}{{[figure][2][2]2.2}{[1][13][]13}}
\newlabel{subfig:21sett_budget1}{{2.3(a)}{14}{Subfigure 2 2.3(a)}{subfigure.2.3.1}{}}
\newlabel{subfig:21sett_budget1@cref}{{[subfigure][1][2,3]2.3(a)}{[1][14][]14}}
\newlabel{sub@subfig:21sett_budget1}{{(a)}{14}{Subfigure 2 2.3(a)\relax }{subfigure.2.3.1}{}}
\newlabel{subfig:21sett_budget2}{{2.3(b)}{14}{Subfigure 2 2.3(b)}{subfigure.2.3.2}{}}
\newlabel{subfig:21sett_budget2@cref}{{[subfigure][2][2,3]2.3(b)}{[1][14][]14}}
\newlabel{sub@subfig:21sett_budget2}{{(b)}{14}{Subfigure 2 2.3(b)\relax }{subfigure.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Budgeting\relax }}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:21sett_budgeting}{{2.3}{14}{Budgeting\relax }{figure.caption.11}{}}
\newlabel{fig:21sett_budgeting@cref}{{[figure][3][2]2.3}{[1][14][]14}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Maximize return with budget on maximum risk, $\qopname \relax m{min}\{f_{1}(x)~: f_{2}(x) \leq \beta ~:x \in S\}$}}}{14}{figure.caption.11}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Minimize risk with budget on minimum return,, $\qopname \relax m{min}\{f_{2}(x)~: f_{1}(x) \leq \beta ~:x \in S\}$}}}{14}{figure.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Infima, suprema and extended reals}{14}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}(Monotone) Sequences in $\mathds  {R}$ and optimization}{15}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Vector spaces and topology}{15}{section.2.4}\protected@file@percent }
\newlabel{21sett_sec:vector_space}{{2.4}{15}{Vector spaces and topology}{section.2.4}{}}
\newlabel{21sett_sec:vector_space@cref}{{[section][4][2]2.4}{[1][15][]15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The shapes of balls centered in the origin of radius $1$ varying the value of $p$-norm.\relax }}{18}{figure.caption.12}\protected@file@percent }
\newlabel{fig:21sett1}{{2.4}{18}{The shapes of balls centered in the origin of radius $1$ varying the value of $p$-norm.\relax }{figure.caption.12}{}}
\newlabel{fig:21sett1@cref}{{[figure][4][2]2.4}{[1][18][]18}}
\@writefile{tdo}{\contentsline {todo}{Aggiungere reference al punto in cui si definisce una matrice definita positiva}{19}{section*.13}\protected@file@percent }
\pgfsyspdfmark {pgfid7}{18318621}{34383107}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Limit of a sequence in $\mathds  {R}^n$}{19}{section.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}27th of September 2018}{21}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Case of non-continuity of the objective function in the border point $(0, 0)$.\relax }}{22}{figure.caption.14}\protected@file@percent }
\newlabel{fig:27set0}{{3.1}{22}{Case of non-continuity of the objective function in the border point $(0, 0)$.\relax }{figure.caption.14}{}}
\newlabel{fig:27set0@cref}{{[figure][1][3]3.1}{[1][22][]22}}
\newlabel{subfig:27sett_graph}{{3.2(a)}{23}{Subfigure 3 3.2(a)}{subfigure.3.2.1}{}}
\newlabel{subfig:27sett_graph@cref}{{[subfigure][1][3,2]3.2(a)}{[1][23][]23}}
\newlabel{sub@subfig:27sett_graph}{{(a)}{23}{Subfigure 3 3.2(a)\relax }{subfigure.3.2.1}{}}
\newlabel{subfig:27sett_epi}{{3.2(b)}{23}{Subfigure 3 3.2(b)}{subfigure.3.2.2}{}}
\newlabel{subfig:27sett_epi@cref}{{[subfigure][2][3,2]3.2(b)}{[1][23][]23}}
\newlabel{sub@subfig:27sett_epi}{{(b)}{23}{Subfigure 3 3.2(b)\relax }{subfigure.3.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Graph and epigraph\relax }}{23}{figure.caption.15}\protected@file@percent }
\newlabel{fig:27sett1}{{3.2}{23}{Graph and epigraph\relax }{figure.caption.15}{}}
\newlabel{fig:27sett1@cref}{{[figure][2][3]3.2}{[1][23][]23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Graph}}}{23}{figure.caption.15}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Epigraph}}}{23}{figure.caption.15}\protected@file@percent }
\newlabel{subfig:27sett_level}{{3.3(a)}{23}{Subfigure 3 3.3(a)}{subfigure.3.3.1}{}}
\newlabel{subfig:27sett_level@cref}{{[subfigure][1][3,3]3.3(a)}{[1][23][]23}}
\newlabel{sub@subfig:27sett_level}{{(a)}{23}{Subfigure 3 3.3(a)\relax }{subfigure.3.3.1}{}}
\newlabel{subfig:27sett_sub}{{3.3(b)}{23}{Subfigure 3 3.3(b)}{subfigure.3.3.2}{}}
\newlabel{subfig:27sett_sub@cref}{{[subfigure][2][3,3]3.3(b)}{[1][23][]23}}
\newlabel{sub@subfig:27sett_sub}{{(b)}{23}{Subfigure 3 3.3(b)\relax }{subfigure.3.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Level and sub-level sets\relax }}{23}{figure.caption.16}\protected@file@percent }
\newlabel{fig:27sett2}{{3.3}{23}{Level and sub-level sets\relax }{figure.caption.16}{}}
\newlabel{fig:27sett2@cref}{{[figure][3][3]3.3}{[1][23][]23}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Level set}}}{23}{figure.caption.16}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Sub-level set}}}{23}{figure.caption.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Continuity}{24}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Derivatives}{24}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Multivariate differentiability}{25}{subsection.3.2.1}\protected@file@percent }
\newlabel{prop:27set1}{{3.2.7}{26}{}{theorem.3.2.7}{}}
\newlabel{prop:27set1@cref}{{[proposition][7][3,2]3.2.7}{[1][26][]26}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}3rd of October 2018}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{tdo}{\contentsline {todo}{Rivedere prima e seconda componente + parallelo con $e_1, \ldots  , e_n$}{29}{section*.17}\protected@file@percent }
\pgfsyspdfmark {pgfid9}{18318621}{8376221}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Simple functions}{32}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Linear functions}{32}{subsection.4.1.1}\protected@file@percent }
\newlabel{subfloat:3ott_1}{{4.1(a)}{32}{Subfigure 4 4.1(a)}{subfigure.4.1.1}{}}
\newlabel{subfloat:3ott_1@cref}{{[subfigure][1][4,1]4.1(a)}{[1][32][]32}}
\newlabel{sub@subfloat:3ott_1}{{(a)}{32}{Subfigure 4 4.1(a)\relax }{subfigure.4.1.1}{}}
\newlabel{subfloat:3ott_2}{{4.1(b)}{32}{Subfigure 4 4.1(b)}{subfigure.4.1.2}{}}
\newlabel{subfloat:3ott_2@cref}{{[subfigure][2][4,1]4.1(b)}{[1][32][]32}}
\newlabel{sub@subfloat:3ott_2}{{(b)}{32}{Subfigure 4 4.1(b)\relax }{subfigure.4.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Graphical example of linear function.\relax }}{32}{figure.caption.18}\protected@file@percent }
\newlabel{fig:3ott_1}{{4.1}{32}{Graphical example of linear function.\relax }{figure.caption.18}{}}
\newlabel{fig:3ott_1@cref}{{[figure][1][4]4.1}{[1][32][]32}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Linear function}}}{32}{figure.caption.18}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Level sets}}}{32}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Quadratic functions}{33}{subsection.4.1.2}\protected@file@percent }
\newlabel{subfloat:3ott_3}{{4.2(a)}{33}{Subfigure 4 4.2(a)}{subfigure.4.2.1}{}}
\newlabel{subfloat:3ott_3@cref}{{[subfigure][1][4,2]4.2(a)}{[1][33][]33}}
\newlabel{sub@subfloat:3ott_3}{{(a)}{33}{Subfigure 4 4.2(a)\relax }{subfigure.4.2.1}{}}
\newlabel{subfloat:3ott_4}{{4.2(b)}{33}{Subfigure 4 4.2(b)}{subfigure.4.2.2}{}}
\newlabel{subfloat:3ott_4@cref}{{[subfigure][2][4,2]4.2(b)}{[1][33][]33}}
\newlabel{sub@subfloat:3ott_4}{{(b)}{33}{Subfigure 4 4.2(b)\relax }{subfigure.4.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Graphical example of a quadratic function.\relax }}{33}{figure.caption.19}\protected@file@percent }
\newlabel{fig:3ott_2}{{4.2}{33}{Graphical example of a quadratic function.\relax }{figure.caption.19}{}}
\newlabel{fig:3ott_2@cref}{{[figure][2][4]4.2}{[1][33][]33}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Quadratic function}}}{33}{figure.caption.19}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Level sets}}}{33}{figure.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}5th of October 2018 --- A. Frangioni}{34}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Unconstrained optimization}{34}{subsection.4.2.1}\protected@file@percent }
\newlabel{subfloat:5ott_1}{{4.3(a)}{34}{Subfigure 4 4.3(a)}{subfigure.4.3.1}{}}
\newlabel{subfloat:5ott_1@cref}{{[subfigure][1][4,3]4.3(a)}{[1][34][]34}}
\newlabel{sub@subfloat:5ott_1}{{(a)}{34}{Subfigure 4 4.3(a)\relax }{subfigure.4.3.1}{}}
\newlabel{subfloat:5ott_2}{{4.3(b)}{34}{Subfigure 4 4.3(b)}{subfigure.4.3.2}{}}
\newlabel{subfloat:5ott_2@cref}{{[subfigure][2][4,3]4.3(b)}{[1][34][]34}}
\newlabel{sub@subfloat:5ott_2}{{(b)}{34}{Subfigure 4 4.3(b)\relax }{subfigure.4.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces In the leftmost plot, we can see that if the derivatives are non zero the point is not a minimum. Such a condition is satisfied in the right handed plot.\relax }}{34}{figure.caption.20}\protected@file@percent }
\newlabel{fig:5ott_1}{{4.3}{34}{In the leftmost plot, we can see that if the derivatives are non zero the point is not a minimum. Such a condition is satisfied in the right handed plot.\relax }{figure.caption.20}{}}
\newlabel{fig:5ott_1@cref}{{[figure][3][4]4.3}{[1][34][]34}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Not minima}}}{34}{figure.caption.20}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Minima}}}{34}{figure.caption.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{First order model}{34}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Second order model}{35}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Convexity}{36}{subsection.4.2.2}\protected@file@percent }
\newlabel{subfloat:5ott_3}{{4.4(a)}{37}{Subfigure 4 4.4(a)}{subfigure.4.4.1}{}}
\newlabel{subfloat:5ott_3@cref}{{[subfigure][1][4,4]4.4(a)}{[1][37][]37}}
\newlabel{sub@subfloat:5ott_3}{{(a)}{37}{Subfigure 4 4.4(a)\relax }{subfigure.4.4.1}{}}
\newlabel{subfloat:5ott_4}{{4.4(b)}{37}{Subfigure 4 4.4(b)}{subfigure.4.4.2}{}}
\newlabel{subfloat:5ott_4@cref}{{[subfigure][2][4,4]4.4(b)}{[1][37][]37}}
\newlabel{sub@subfloat:5ott_4}{{(b)}{37}{Subfigure 4 4.4(b)\relax }{subfigure.4.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Unitary simplexes.\relax }}{37}{figure.caption.23}\protected@file@percent }
\newlabel{fig:5ott_2}{{4.4}{37}{Unitary simplexes.\relax }{figure.caption.23}{}}
\newlabel{fig:5ott_2@cref}{{[figure][4][4]4.4}{[1][37][]37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {In $\mathds {R}^2$}}}{37}{figure.caption.23}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {In $\mathds {R}^3$}}}{37}{figure.caption.23}\protected@file@percent }
\newlabel{subfloat:5ott_5}{{4.5(a)}{39}{Subfigure 4 4.5(a)}{subfigure.4.5.1}{}}
\newlabel{subfloat:5ott_5@cref}{{[subfigure][1][4,5]4.5(a)}{[1][38][]39}}
\newlabel{sub@subfloat:5ott_5}{{(a)}{39}{Subfigure 4 4.5(a)\relax }{subfigure.4.5.1}{}}
\newlabel{subfloat:5ott_6}{{4.5(b)}{39}{Subfigure 4 4.5(b)}{subfigure.4.5.2}{}}
\newlabel{subfloat:5ott_6@cref}{{[subfigure][2][4,5]4.5(b)}{[1][38][]39}}
\newlabel{sub@subfloat:5ott_6}{{(b)}{39}{Subfigure 4 4.5(b)\relax }{subfigure.4.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Pictorial examples of slicing and projecting.\relax }}{39}{figure.caption.24}\protected@file@percent }
\newlabel{fig:5ott_3}{{4.5}{39}{Pictorial examples of slicing and projecting.\relax }{figure.caption.24}{}}
\newlabel{fig:5ott_3@cref}{{[figure][5][4]4.5}{[1][38][]39}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Slice on $\bar {y}$}}}{39}{figure.caption.24}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Project on the $x$ component}}}{39}{figure.caption.24}\protected@file@percent }
\newlabel{prop:5ott_1}{{4.2.7}{39}{}{theorem.4.2.7}{}}
\newlabel{prop:5ott_1@cref}{{[proposition][7][4,2]4.2.7}{[1][39][]39}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Pictorial example of sublevel graph. Such a graph is drawn in green in the figure.\relax }}{40}{figure.caption.25}\protected@file@percent }
\newlabel{fig:5ott_4}{{4.6}{40}{Pictorial example of sublevel graph. Such a graph is drawn in green in the figure.\relax }{figure.caption.25}{}}
\newlabel{fig:5ott_4@cref}{{[figure][6][4]4.6}{[1][39][]40}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}11th of October 2018 --- A. Frangioni}{41}{section.4.3}\protected@file@percent }
\newlabel{subfloat:11ott_1}{{4.7(a)}{42}{Subfigure 4 4.7(a)}{subfigure.4.7.1}{}}
\newlabel{subfloat:11ott_1@cref}{{[subfigure][1][4,7]4.7(a)}{[1][42][]42}}
\newlabel{sub@subfloat:11ott_1}{{(a)}{42}{Subfigure 4 4.7(a)\relax }{subfigure.4.7.1}{}}
\newlabel{subfloat:11ott_2}{{4.7(b)}{42}{Subfigure 4 4.7(b)}{subfigure.4.7.2}{}}
\newlabel{subfloat:11ott_2@cref}{{[subfigure][2][4,7]4.7(b)}{[1][42][]42}}
\newlabel{sub@subfloat:11ott_2}{{(b)}{42}{Subfigure 4 4.7(b)\relax }{subfigure.4.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Graphic hints.\relax }}{42}{figure.caption.26}\protected@file@percent }
\newlabel{fig:11ott_1}{{4.7}{42}{Graphic hints.\relax }{figure.caption.26}{}}
\newlabel{fig:11ott_1@cref}{{[figure][7][4]4.7}{[1][42][]42}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Suprema --- second property}}}{42}{figure.caption.26}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Perspective --- eightth property}}}{42}{figure.caption.26}\protected@file@percent }
\newlabel{prop:11ott_1}{{4.3.3}{42}{}{theorem.4.3.3}{}}
\newlabel{prop:11ott_1@cref}{{[proposition][3][4,3]4.3.3}{[1][42][]42}}
\newlabel{subfloat:11ott_3}{{4.8(a)}{43}{Subfigure 4 4.8(a)}{subfigure.4.8.1}{}}
\newlabel{subfloat:11ott_3@cref}{{[subfigure][1][4,8]4.8(a)}{[1][42][]43}}
\newlabel{sub@subfloat:11ott_3}{{(a)}{43}{Subfigure 4 4.8(a)\relax }{subfigure.4.8.1}{}}
\newlabel{subfloat:11ott_4}{{4.8(b)}{43}{Subfigure 4 4.8(b)}{subfigure.4.8.2}{}}
\newlabel{subfloat:11ott_4@cref}{{[subfigure][2][4,8]4.8(b)}{[1][42][]43}}
\newlabel{sub@subfloat:11ott_4}{{(b)}{43}{Subfigure 4 4.8(b)\relax }{subfigure.4.8.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces \relax }}{43}{figure.caption.27}\protected@file@percent }
\newlabel{fig:11ott_2}{{4.8}{43}{\relax }{figure.caption.27}{}}
\newlabel{fig:11ott_2@cref}{{[figure][8][4]4.8}{[1][42][]43}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Take a compact set in the interior of the domain (far from the boundaries) there the function is Lipschitz continuous.}}}{43}{figure.caption.27}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {If a function is not Lipschitz on a compact subset it is not convex.}}}{43}{figure.caption.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Subgradients and subdifferentials}{43}{section*.28}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Pictorial example of subgradients of a non diffrentaible function.\relax }}{44}{figure.caption.29}\protected@file@percent }
\newlabel{fig:11ott_3}{{4.9}{44}{Pictorial example of subgradients of a non diffrentaible function.\relax }{figure.caption.29}{}}
\newlabel{fig:11ott_3@cref}{{[figure][9][4]4.9}{[1][43][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces There are many different subgradients in $x$. We pick the one with minimum norm among the ones which have a negative scalar product with $x - x^*$.\relax }}{45}{figure.caption.30}\protected@file@percent }
\newlabel{fig:11ott_3}{{4.10}{45}{There are many different subgradients in $x$. We pick the one with minimum norm among the ones which have a negative scalar product with $x - x^*$.\relax }{figure.caption.30}{}}
\newlabel{fig:11ott_3@cref}{{[figure][10][4]4.10}{[1][44][]45}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}17th of October 2018 --- A. Frangioni}{46}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Optimization algorithms}{46}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Gradient method for quadratic functions}{46}{section*.31}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.4.1}{\ignorespaces Pseudocode for quadratic functions local minimum detection.\relax }}{47}{algorithm.4.4.1}\protected@file@percent }
\newlabel{alg:17ottquad}{{4.4.1}{47}{Pseudocode for quadratic functions local minimum detection.\relax }{algorithm.4.4.1}{}}
\newlabel{alg:17ottquad@cref}{{[algorithm][1][4,4]4.4.1}{[1][46][]47}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}19th of October 2018 --- A. Frangioni}{49}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Gradient method for quadratic functions}{49}{subsection.4.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Some iterations of the gradient method\relax }}{49}{figure.caption.32}\protected@file@percent }
\newlabel{fig:19ottorto}{{4.11}{49}{Some iterations of the gradient method\relax }{figure.caption.32}{}}
\newlabel{fig:19ottorto@cref}{{[figure][11][4]4.11}{[1][49][]49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}MatLab implementation}{51}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Error}{51}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}24th of October 2018 --- A. Frangioni}{53}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Gradient method for non quadratic functions}{53}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How fast does it converge?}{53}{section*.34}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.6.1}{\ignorespaces Pseudocode for non quadratic functions local minimum detection.\relax }}{53}{algorithm.4.6.1}\protected@file@percent }
\newlabel{alg:24ottnonQuad}{{4.6.1}{53}{Pseudocode for non quadratic functions local minimum detection.\relax }{algorithm.4.6.1}{}}
\newlabel{alg:24ottnonQuad@cref}{{[algorithm][1][4,6]4.6.1}{[1][53][]53}}
\@writefile{tdo}{\contentsline {todo}{Chi Ã¨ $x(\alpha )$?}{54}{section*.35}\protected@file@percent }
\pgfsyspdfmark {pgfid17}{10568990}{26402628}
\pgfsyspdfmark {pgfid18}{7967030}{26416735}
\pgfsyspdfmark {pgfid19}{9815325}{26191005}
\newlabel{24ottps}{{4.6.2}{54}{}{theorem.4.6.2}{}}
\newlabel{24ottps@cref}{{[proposition][2][4,6]4.6.2}{[1][54][]54}}
\@writefile{toc}{\contentsline {subsubsection}{Exact line search, first orderd approach}{55}{section*.36}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces First, we restrict from $\mathds  {R}$ to $[x_{1}, x_{2}]$, then double $\alpha $ until the derivative is greater than $0$\relax }}{56}{figure.caption.37}\protected@file@percent }
\newlabel{fig:24ott1}{{4.12}{56}{First, we restrict from $\R $ to $[x_{1}, x_{2}]$, then double $\alpha $ until the derivative is greater than $0$\relax }{figure.caption.37}{}}
\newlabel{fig:24ott1@cref}{{[figure][12][4]4.12}{[1][55][]56}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.6.2}{\ignorespaces First algorithm for exact line search\relax }}{56}{algorithm.4.6.2}\protected@file@percent }
\newlabel{alg:24ottELS1}{{4.6.2}{56}{First algorithm for exact line search\relax }{algorithm.4.6.2}{}}
\newlabel{alg:24ottELS1@cref}{{[algorithm][2][4,6]4.6.2}{[1][55][]56}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.6.3}{\ignorespaces Bisection algorithm\relax }}{56}{algorithm.4.6.3}\protected@file@percent }
\newlabel{alg:24ottBS}{{4.6.3}{56}{Bisection algorithm\relax }{algorithm.4.6.3}{}}
\newlabel{alg:24ottBS@cref}{{[algorithm][3][4,6]4.6.3}{[1][56][]56}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces The information we have about function $\varphi $\relax }}{57}{figure.caption.38}\protected@file@percent }
\newlabel{fig:24ott3}{{4.13}{57}{The information we have about function $\varphi $\relax }{figure.caption.38}{}}
\newlabel{fig:24ott3@cref}{{[figure][13][4]4.13}{[1][56][]57}}
\newlabel{fact:24ott3}{{4.6.4}{57}{}{theorem.4.6.4}{}}
\newlabel{fact:24ott3@cref}{{[proposition][4][4,6]4.6.4}{[1][57][]57}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces If the function isn't $C^3$ and one derivative is very big, then the range doesn't shrink much.\relax }}{57}{figure.caption.39}\protected@file@percent }
\newlabel{fig:24ott4}{{4.14}{57}{If the function isn't $C^3$ and one derivative is very big, then the range doesn't shrink much.\relax }{figure.caption.39}{}}
\newlabel{fig:24ott4@cref}{{[figure][14][4]4.14}{[1][57][]57}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}25th of October 2018 --- A. Frangioni}{58}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Line search: second order approaches}{58}{section*.40}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.7.1}{\ignorespaces Pseudocode for Newton method.\relax }}{58}{algorithm.4.7.1}\protected@file@percent }
\newlabel{alg:25ottNewton}{{4.7.1}{58}{Pseudocode for Newton method.\relax }{algorithm.4.7.1}{}}
\newlabel{alg:25ottNewton@cref}{{[algorithm][1][4,7]4.7.1}{[1][58][]58}}
\@writefile{toc}{\contentsline {subsubsection}{Exact line search: zeroth-order approaches}{59}{section*.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces The interesting interval is $[x_1, x_2]$, since $\varphi (x_2) > \varphi (x_1)$ and we are allowed to exclude the interval $[x_3, +\infty )$ since the value in $x_3$ is bigger than $\varphi (x_2)$.\relax }}{60}{figure.caption.42}\protected@file@percent }
\newlabel{fig:25ott1}{{4.15}{60}{The interesting interval is $[x_1, x_2]$, since $\varphi (x_2) > \varphi (x_1)$ and we are allowed to exclude the interval $[x_3, +\infty )$ since the value in $x_3$ is bigger than $\varphi (x_2)$.\relax }{figure.caption.42}{}}
\newlabel{fig:25ott1@cref}{{[figure][15][4]4.15}{[1][59][]60}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces The relationship between $r$ and $1-r$ is $r ~ : ~ 1 = (1-r) ~ : ~ r$.\relax }}{60}{figure.caption.43}\protected@file@percent }
\newlabel{fig:25ott2}{{4.16}{60}{The relationship between $r$ and $1-r$ is $r ~ : ~ 1 = (1-r) ~ : ~ r$.\relax }{figure.caption.43}{}}
\newlabel{fig:25ott2@cref}{{[figure][16][4]4.16}{[1][60][]60}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.7.2}{\ignorespaces Pseudocode for non differentiable functions for local minimum detection.\relax }}{61}{algorithm.4.7.2}\protected@file@percent }
\newlabel{alg:25ottnonDeriv}{{4.7.2}{61}{Pseudocode for non differentiable functions for local minimum detection.\relax }{algorithm.4.7.2}{}}
\newlabel{alg:25ottnonDeriv@cref}{{[algorithm][2][4,7]4.7.2}{[1][60][]61}}
\@writefile{toc}{\contentsline {subsubsection}{Inexact line search: Armijo-Wolfe}{61}{section*.44}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces The dotted line represents the line which has the derivative as slope.\relax }}{61}{figure.caption.45}\protected@file@percent }
\newlabel{fig:25ott3}{{4.17}{61}{The dotted line represents the line which has the derivative as slope.\relax }{figure.caption.45}{}}
\newlabel{fig:25ott3@cref}{{[figure][17][4]4.17}{[1][61][]61}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces In the left picture Armijo condition chooses a new line which slope is still negative, but less stip than the original one. In the right one, the ranges where to search are highlighted.\relax }}{62}{figure.caption.46}\protected@file@percent }
\newlabel{fig:25ott4}{{4.18}{62}{In the left picture Armijo condition chooses a new line which slope is still negative, but less stip than the original one. In the right one, the ranges where to search are highlighted.\relax }{figure.caption.46}{}}
\newlabel{fig:25ott4@cref}{{[figure][18][4]4.18}{[1][61][]62}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Goldstein condition chooses a new line which slope is still negative, less stip than the original one, but stipper than the one obtained by Armijo.\relax }}{62}{figure.caption.47}\protected@file@percent }
\newlabel{fig:25ott5}{{4.19}{62}{Goldstein condition chooses a new line which slope is still negative, less stip than the original one, but stipper than the one obtained by Armijo.\relax }{figure.caption.47}{}}
\newlabel{fig:25ott5@cref}{{[figure][19][4]4.19}{[1][62][]62}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Here are the intervals that satisfy both Armijo and Goldstein conditions.\relax }}{63}{figure.caption.48}\protected@file@percent }
\newlabel{fig:25ott6}{{4.20}{63}{Here are the intervals that satisfy both Armijo and Goldstein conditions.\relax }{figure.caption.48}{}}
\newlabel{fig:25ott6@cref}{{[figure][20][4]4.20}{[1][62][]63}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces On the left Wolfe condition (which chooses derivatives that are substantially zero). On the right part the intervals selected by Wolfe.\relax }}{63}{figure.caption.49}\protected@file@percent }
\newlabel{fig:25ott7}{{4.21}{63}{On the left Wolfe condition (which chooses derivatives that are substantially zero). On the right part the intervals selected by Wolfe.\relax }{figure.caption.49}{}}
\newlabel{fig:25ott7@cref}{{[figure][21][4]4.21}{[1][62][]63}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces Strong Wolfe condition.\relax }}{63}{figure.caption.50}\protected@file@percent }
\newlabel{fig:25ott8}{{4.22}{63}{Strong Wolfe condition.\relax }{figure.caption.50}{}}
\newlabel{fig:25ott8@cref}{{[figure][22][4]4.22}{[1][62][]63}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces If the line is too close to the original one only a few points will satisfy Armijo condition which means that the intersection between Armijo and Wolfe will almost be empty.\relax }}{64}{figure.caption.51}\protected@file@percent }
\newlabel{fig:25ott9}{{4.23}{64}{If the line is too close to the original one only a few points will satisfy Armijo condition which means that the intersection between Armijo and Wolfe will almost be empty.\relax }{figure.caption.51}{}}
\newlabel{fig:25ott9@cref}{{[figure][23][4]4.23}{[1][63][]64}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces If the function isn't going to $-\infty $ the blue line and the function will meet again and we denote the value along the $x$ axis $\bar {\alpha }$.\relax }}{64}{figure.caption.52}\protected@file@percent }
\newlabel{fig:25ott10}{{4.24}{64}{If the function isn't going to $-\infty $ the blue line and the function will meet again and we denote the value along the $x$ axis $\bar {\alpha }$.\relax }{figure.caption.52}{}}
\newlabel{fig:25ott10@cref}{{[figure][24][4]4.24}{[1][64][]64}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.7.3}{\ignorespaces Pseudocode for backtracking line search.\relax }}{65}{algorithm.4.7.3}\protected@file@percent }
\newlabel{alg:25ottback}{{4.7.3}{65}{Pseudocode for backtracking line search.\relax }{algorithm.4.7.3}{}}
\newlabel{alg:25ottback@cref}{{[algorithm][3][4,7]4.7.3}{[1][64][]65}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}8th of November 2018 --- A. Frangioni}{66}{section.4.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Good practice of designing a project}{66}{subsection.4.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.9}14th of November 2018 --- A. Frangioni}{67}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9.1}Taylor method}{67}{subsection.4.9.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Scrivere meglio la dim}{68}{section*.53}\protected@file@percent }
\pgfsyspdfmark {pgfid23}{10568990}{29295715}
\pgfsyspdfmark {pgfid24}{7967030}{29309822}
\pgfsyspdfmark {pgfid25}{9815325}{29084092}
\@writefile{toc}{\contentsline {subsubsection}{Interpretation of Newton method}{70}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Non convex case}{70}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.10}16th of November 2018 --- A. Frangioni}{72}{section.4.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.1}Quasi-newton methods}{72}{subsection.4.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Davidson-Fletcher-Powell}{73}{section*.56}\protected@file@percent }
\newlabel{theo:16novsmw}{{4.10.3}{73}{Sherman-Morrison-Woodbury}{theorem.4.10.3}{}}
\newlabel{theo:16novsmw@cref}{{[theorem][3][4,10]4.10.3}{[1][73][]73}}
\@writefile{toc}{\contentsline {subsubsection}{Broyden-Fletcher-Goldfarb-Shanno}{74}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Poorman's approach}{74}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10.2}Conjugate gradient method}{74}{subsection.4.10.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces Geometric idea on how the new direction is chosen.\relax }}{75}{figure.caption.59}\protected@file@percent }
\newlabel{fig:16nov1}{{4.25}{75}{Geometric idea on how the new direction is chosen.\relax }{figure.caption.59}{}}
\newlabel{fig:16nov1@cref}{{[figure][25][4]4.25}{[1][74][]75}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.10.1}{\ignorespaces Pseudocode for conjugate gradient method for quadratic functions.\relax }}{75}{algorithm.4.10.1}\protected@file@percent }
\newlabel{alg:CGQ1}{{4.10.1}{75}{Pseudocode for conjugate gradient method for quadratic functions.\relax }{algorithm.4.10.1}{}}
\newlabel{alg:CGQ1@cref}{{[algorithm][1][4,10]4.10.1}{[1][75][]75}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.10.2}{\ignorespaces Pseudocode for conjugate gradient method for arbitrary functions\relax }}{76}{algorithm.4.10.2}\protected@file@percent }
\newlabel{alg:16novCGA}{{4.10.2}{76}{Pseudocode for conjugate gradient method for arbitrary functions\relax }{algorithm.4.10.2}{}}
\newlabel{alg:16novCGA@cref}{{[algorithm][2][4,10]4.10.2}{[1][76][]76}}
\@writefile{toc}{\contentsline {section}{\numberline {4.11}22nd of November 2018 --- A. Frangioni}{77}{section.4.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.1}Deflected gradient methods}{77}{subsection.4.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Heavy ball gradient method}{77}{section*.60}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces On the left side Newton method and on the right side the heavy ball method.\relax }}{78}{figure.caption.61}\protected@file@percent }
\newlabel{fig:22nov1}{{4.26}{78}{On the left side Newton method and on the right side the heavy ball method.\relax }{figure.caption.61}{}}
\newlabel{fig:22nov1@cref}{{[figure][26][4]4.26}{[1][78][]78}}
\@writefile{toc}{\contentsline {subsubsection}{Accelerated gradient}{78}{section*.62}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.11.1}{\ignorespaces Pseudocode for accelerated gradient method.\relax }}{78}{algorithm.4.11.1}\protected@file@percent }
\newlabel{alg:22novACCG}{{4.11.1}{78}{Pseudocode for accelerated gradient method.\relax }{algorithm.4.11.1}{}}
\newlabel{alg:22novACCG@cref}{{[algorithm][1][4,11]4.11.1}{[1][78][]78}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces We build a linear model, which is a lowerbound for the function, then we build a quadratic model, which is above our function.\relax }}{79}{figure.caption.63}\protected@file@percent }
\newlabel{fig:22nov3}{{4.27}{79}{We build a linear model, which is a lowerbound for the function, then we build a quadratic model, which is above our function.\relax }{figure.caption.63}{}}
\newlabel{fig:22nov3@cref}{{[figure][27][4]4.27}{[1][79][]79}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.2}Incremental gradient methods}{79}{subsection.4.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11.3}Subgradient methods}{80}{subsection.4.11.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces This function has a lot of kinky points.\relax }}{81}{figure.caption.64}\protected@file@percent }
\newlabel{fig:22novlasso}{{4.28}{81}{This function has a lot of kinky points.\relax }{figure.caption.64}{}}
\newlabel{fig:22novlasso@cref}{{[figure][28][4]4.28}{[1][80][]81}}
\newlabel{prop:22nov1}{{4.11.2}{81}{}{theorem.4.11.2}{}}
\newlabel{prop:22nov1@cref}{{[property][2][4,11]4.11.2}{[1][80][]81}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Since we know that $<g, x^*>$ is negative we get that the angle between $g$ and $x^*$ is larger than $90^{\circ }$. Moreover, we know where the optimum is not (red region), hence we restrict to the green region. At this point we will perform a line search on $g$ direction. Since the function is not smooth, the line search may not succeed since the value of the function along the half line from $x$ in $g$ direction may remain the same, but there may be some points there that are closer to $x^*$.\relax }}{81}{figure.caption.65}\protected@file@percent }
\newlabel{fig:22nov4}{{4.29}{81}{Since we know that $<g, x^*>$ is negative we get that the angle between $g$ and $x^*$ is larger than $90^{\circ }$. Moreover, we know where the optimum is not (red region), hence we restrict to the green region. At this point we will perform a line search on $g$ direction. Since the function is not smooth, the line search may not succeed since the value of the function along the half line from $x$ in $g$ direction may remain the same, but there may be some points there that are closer to $x^*$.\relax }{figure.caption.65}{}}
\newlabel{fig:22nov4@cref}{{[figure][29][4]4.29}{[1][81][]81}}
\@writefile{toc}{\contentsline {section}{\numberline {4.12}28th of November 2018 --- A. Frangioni}{83}{section.4.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.1}Subgradient methods}{83}{subsection.4.12.1}\protected@file@percent }
\newlabel{eq:28nov1}{{4.12.1}{83}{Subgradient methods}{equation.4.12.1}{}}
\newlabel{eq:28nov1@cref}{{[equation][1][4,12]4.12.1}{[1][83][]83}}
\@writefile{toc}{\contentsline {subsubsection}{Target level stepsize}{85}{section*.66}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.12.1}{\ignorespaces Pseudocode for target level stepsize.\relax }}{86}{algorithm.4.12.1}\protected@file@percent }
\newlabel{alg28nov:nonSGPTL}{{4.12.1}{86}{Pseudocode for target level stepsize.\relax }{algorithm.4.12.1}{}}
\newlabel{alg28nov:nonSGPTL@cref}{{[algorithm][1][4,12]4.12.1}{[1][85][]86}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces There are two cases: either the function value is significanlty below the reference (for example $\genfrac  {}{}{}{}{\delta }{2}$ below the reference) or it's not . If we are in the ``happy case'' (we just move the reference to the best value). For what conserns the ``unhappy case'', if we are unhapy for 1 iteration, no problem. Two iterations? no prolem. Many iterations? Problem: after some iterations in which we are not improving it means that we have to decreas the reference values. How? $r$ is updated at each bad step and reset when a good step occurs. When $r$ gets too large we decrease $\delta $ and reset everything.\relax }}{86}{figure.caption.67}\protected@file@percent }
\newlabel{fig:28nov1}{{4.30}{86}{There are two cases: either the function value is significanlty below the reference (for example $\frac {\delta }{2}$ below the reference) or it's not . If we are in the ``happy case'' (we just move the reference to the best value). For what conserns the ``unhappy case'', if we are unhapy for 1 iteration, no problem. Two iterations? no prolem. Many iterations? Problem: after some iterations in which we are not improving it means that we have to decreas the reference values. How? $r$ is updated at each bad step and reset when a good step occurs. When $r$ gets too large we decrease $\delta $ and reset everything.\relax }{figure.caption.67}{}}
\newlabel{fig:28nov1@cref}{{[figure][30][4]4.30}{[1][86][]86}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.2}Deflected subgradient}{87}{subsection.4.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.3}Smoothed gradient methods}{87}{subsection.4.12.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.31}{\ignorespaces Let us take the absolute value function $f(x) = |x|$. This function would be differentiable, if it wasn't for some nasty points (in this case $0$, were the optimization problem has many optimal solutions). In $0$ there are many subgradients, so it has many optima.\relax }}{88}{figure.caption.68}\protected@file@percent }
\newlabel{fig:28nov2}{{4.31}{88}{Let us take the absolute value function $f(x) = |x|$. This function would be differentiable, if it wasn't for some nasty points (in this case $0$, were the optimization problem has many optimal solutions). In $0$ there are many subgradients, so it has many optima.\relax }{figure.caption.68}{}}
\newlabel{fig:28nov2@cref}{{[figure][31][4]4.31}{[1][87][]88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.32}{\ignorespaces Geometric intuition of the usage of variable $\mu $.\relax }}{88}{figure.caption.69}\protected@file@percent }
\newlabel{fig:28nov3}{{4.32}{88}{Geometric intuition of the usage of variable $\mu $.\relax }{figure.caption.69}{}}
\newlabel{fig:28nov3@cref}{{[figure][32][4]4.32}{[1][87][]88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.33}{\ignorespaces At the beginning we eill make a lot of bad steps (the upper gray line). We can improve (pick the black line) changing the $\varepsilon $ value and we obtain something that looks more stable. The more precision we want, the smaller the step we make. At the ending it pays, but at the beginning it is not so.\relax }}{89}{figure.caption.70}\protected@file@percent }
\newlabel{fig:28nov4}{{4.33}{89}{At the beginning we eill make a lot of bad steps (the upper gray line). We can improve (pick the black line) changing the $\eps $ value and we obtain something that looks more stable. The more precision we want, the smaller the step we make. At the ending it pays, but at the beginning it is not so.\relax }{figure.caption.70}{}}
\newlabel{fig:28nov4@cref}{{[figure][33][4]4.33}{[1][88][]89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.4}Cutting-plane algorithm}{89}{subsection.4.12.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.34}{\ignorespaces The idea is that we do not compute a subgradient and discard it. We store it, thus resulting in a model\relax }}{89}{figure.caption.71}\protected@file@percent }
\newlabel{fig:28nov5}{{4.34}{89}{The idea is that we do not compute a subgradient and discard it. We store it, thus resulting in a model\relax }{figure.caption.71}{}}
\newlabel{fig:28nov5@cref}{{[figure][34][4]4.34}{[1][89][]89}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.35}{\ignorespaces A geometric representation of how the model (blue line) changes after one iteration.\relax }}{90}{figure.caption.72}\protected@file@percent }
\newlabel{fig:28nov6}{{4.35}{90}{A geometric representation of how the model (blue line) changes after one iteration.\relax }{figure.caption.72}{}}
\newlabel{fig:28nov6@cref}{{[figure][35][4]4.35}{[1][90][]90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.12.5}Bundle methods}{90}{subsection.4.12.5}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.12.2}{\ignorespaces Pseudocode for boundle method.\relax }}{91}{algorithm.4.12.2}\protected@file@percent }
\newlabel{alg:28novPBM}{{4.12.2}{91}{Pseudocode for boundle method.\relax }{algorithm.4.12.2}{}}
\newlabel{alg:28novPBM@cref}{{[algorithm][2][4,12]4.12.2}{[1][91][]91}}
\@writefile{toc}{\contentsline {section}{\numberline {4.13}30th of November 2018 --- A. Frangioni}{92}{section.4.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.13.1}Constrained optimization}{92}{subsection.4.13.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.36}{\ignorespaces The red line is level set of the function corresponding to the smallest value that touches the set $X$. The point in the intersection is not a saddle point of the function $f$, altough it is the minimum.\relax }}{92}{figure.caption.73}\protected@file@percent }
\newlabel{fig:30nov1}{{4.36}{92}{The red line is level set of the function corresponding to the smallest value that touches the set $X$. The point in the intersection is not a saddle point of the function $f$, altough it is the minimum.\relax }{figure.caption.73}{}}
\newlabel{fig:30nov1@cref}{{[figure][36][4]4.36}{[1][92][]92}}
\@writefile{toc}{\contentsline {subsubsection}{Linear equality constraints}{92}{section*.74}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.37}{\ignorespaces Linear constraint and a point on the boundary.\relax }}{93}{figure.caption.75}\protected@file@percent }
\newlabel{fig:30nov2}{{4.37}{93}{Linear constraint and a point on the boundary.\relax }{figure.caption.75}{}}
\newlabel{fig:30nov2@cref}{{[figure][37][4]4.37}{[1][92][]93}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.38}{\ignorespaces The gradient is orthogonal to the level set in that point, when the function is smooth. The same holds for matrix $A$.\relax }}{93}{figure.caption.76}\protected@file@percent }
\newlabel{fig:30nov3}{{4.38}{93}{The gradient is orthogonal to the level set in that point, when the function is smooth. The same holds for matrix $A$.\relax }{figure.caption.76}{}}
\newlabel{fig:30nov3@cref}{{[figure][38][4]4.38}{[1][92][]93}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.39}{\ignorespaces If we take any other point in the space it has to be orthogonal to $A$.\relax }}{93}{figure.caption.77}\protected@file@percent }
\newlabel{fig:30nov4}{{4.39}{93}{If we take any other point in the space it has to be orthogonal to $A$.\relax }{figure.caption.77}{}}
\newlabel{fig:30nov4@cref}{{[figure][39][4]4.39}{[1][92][]93}}
\@writefile{toc}{\contentsline {subsubsection}{Background for linear inequality contstraints}{94}{section*.78}\protected@file@percent }
\newlabel{teo:30nov1}{{4.13.4}{94}{}{theorem.4.13.4}{}}
\newlabel{teo:30nov1@cref}{{[theorem][4][4,13]4.13.4}{[1][94][]94}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.40}{\ignorespaces When the boundary has this shape we can move along directions that point inside the constraints. Tangent directions are not allowed.\relax }}{95}{figure.caption.79}\protected@file@percent }
\newlabel{fig:30nov4}{{4.40}{95}{When the boundary has this shape we can move along directions that point inside the constraints. Tangent directions are not allowed.\relax }{figure.caption.79}{}}
\newlabel{fig:30nov4@cref}{{[figure][40][4]4.40}{[1][94][]95}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.41}{\ignorespaces Geometric representation of tangent cone, which is the region of the space where we can pick the directions. The intuition is to zoom in $x$ and the result of this zooming is a cone.\relax }}{95}{figure.caption.80}\protected@file@percent }
\newlabel{fig:30nov5}{{4.41}{95}{Geometric representation of tangent cone, which is the region of the space where we can pick the directions. The intuition is to zoom in $x$ and the result of this zooming is a cone.\relax }{figure.caption.80}{}}
\newlabel{fig:30nov5@cref}{{[figure][41][4]4.41}{[1][94][]95}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.42}{\ignorespaces Convex function.\relax }}{96}{figure.caption.81}\protected@file@percent }
\newlabel{fig:30nov5}{{4.42}{96}{Convex function.\relax }{figure.caption.81}{}}
\newlabel{fig:30nov5@cref}{{[figure][42][4]4.42}{[1][95][]96}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.43}{\ignorespaces Let us suppose we pick the direction towards the left part of the function in the saddle point. That direction is promising and actually the value of the function decreases. In this case the problem is that the contraint is not convex.\relax }}{96}{figure.caption.82}\protected@file@percent }
\newlabel{fig:30nov6}{{4.43}{96}{Let us suppose we pick the direction towards the left part of the function in the saddle point. That direction is promising and actually the value of the function decreases. In this case the problem is that the contraint is not convex.\relax }{figure.caption.82}{}}
\newlabel{fig:30nov6@cref}{{[figure][43][4]4.43}{[1][95][]96}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.44}{\ignorespaces The first-order feasible direction cone is made of those directions that are orthogonal to the gradient of the constraints $g_1(x)$ and $g_2(x)$ in $x$.\relax }}{97}{figure.caption.83}\protected@file@percent }
\newlabel{fig:30nov6}{{4.44}{97}{The first-order feasible direction cone is made of those directions that are orthogonal to the gradient of the constraints $g_1(x)$ and $g_2(x)$ in $x$.\relax }{figure.caption.83}{}}
\newlabel{fig:30nov6@cref}{{[figure][44][4]4.44}{[1][97][]97}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.45}{\ignorespaces The circle represents the quadrati constraint, while the semi-plane represents the first degree constraint.\relax }}{98}{figure.caption.84}\protected@file@percent }
\newlabel{fig:30nov7}{{4.45}{98}{The circle represents the quadrati constraint, while the semi-plane represents the first degree constraint.\relax }{figure.caption.84}{}}
\newlabel{fig:30nov7@cref}{{[figure][45][4]4.45}{[1][98][]98}}
\@writefile{toc}{\contentsline {section}{\numberline {4.14}6th of December 2018 --- A. Frangioni}{100}{section.4.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.1}Duality}{100}{subsection.4.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.2}Lagrangian duality}{101}{subsection.4.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.3}Specialized dual}{103}{subsection.4.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Linead programs}{103}{section*.85}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Quadratic programs}{103}{section*.86}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conic program}{103}{section*.87}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.46}{\ignorespaces $x^3 \geq \sqrt  {x_1^2 + x_2^2}$ is a convex cone.\relax }}{104}{figure.caption.88}\protected@file@percent }
\newlabel{fig:6dic_1}{{4.46}{104}{$x^3 \ge \sqrt {x_1^2 + x_2^2}$ is a convex cone.\relax }{figure.caption.88}{}}
\newlabel{fig:6dic_1@cref}{{[figure][46][4]4.46}{[1][103][]104}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.14.4}Fenchel's duality}{105}{subsection.4.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.15}12th of December 2018 --- A. Frangioni}{106}{section.4.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.15.1}Quadratic problem with linear equality constraints}{106}{subsection.4.15.1}\protected@file@percent }
\newlabel{sec:12dic_lec}{{4.15.1}{106}{Quadratic problem with linear equality constraints}{subsection.4.15.1}{}}
\newlabel{sec:12dic_lec@cref}{{[subsection][1][4,15]4.15.1}{[1][106][]106}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.15.2}Inequality constrained problems}{107}{subsection.4.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Projected gradient method}{108}{section*.89}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.47}{\ignorespaces Geometric idea of how to choose the direction in case of lying on the boundary.\relax }}{108}{figure.caption.90}\protected@file@percent }
\newlabel{fig:12dic1}{{4.47}{108}{Geometric idea of how to choose the direction in case of lying on the boundary.\relax }{figure.caption.90}{}}
\newlabel{fig:12dic1@cref}{{[figure][47][4]4.47}{[1][108][]108}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.15.1}{\ignorespaces Pseudocode for projected gradient method for quadratic functions.\relax }}{110}{algorithm.4.15.1}\protected@file@percent }
\newlabel{alg:PGM}{{4.15.1}{110}{Pseudocode for projected gradient method for quadratic functions.\relax }{algorithm.4.15.1}{}}
\newlabel{alg:PGM@cref}{{[algorithm][1][4,15]4.15.1}{[1][109][]110}}
\@writefile{toc}{\contentsline {subsubsection}{Projected gradient method with box constraints}{111}{section*.91}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.15.2}{\ignorespaces Pseudocode for projected gradient method for quadratic functions in the case of box constraints.\relax }}{111}{algorithm.4.15.2}\protected@file@percent }
\newlabel{alg:PGMBC}{{4.15.2}{111}{Pseudocode for projected gradient method for quadratic functions in the case of box constraints.\relax }{algorithm.4.15.2}{}}
\newlabel{alg:PGMBC@cref}{{[algorithm][2][4,15]4.15.2}{[1][111][]111}}
\@writefile{toc}{\contentsline {subsubsection}{Active-set method for quadratic programs}{112}{section*.92}\protected@file@percent }
\newlabel{sec:12dic_active}{{4.15.2}{112}{Active-set method for quadratic programs}{section*.92}{}}
\newlabel{sec:12dic_active@cref}{{[subsection][2][4,15]4.15.2}{[1][112][]112}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.15.3}{\ignorespaces Pseudocode for active set method for quadratic functions.\relax }}{112}{algorithm.4.15.3}\protected@file@percent }
\newlabel{alg:ASMQP}{{4.15.3}{112}{Pseudocode for active set method for quadratic functions.\relax }{algorithm.4.15.3}{}}
\newlabel{alg:ASMQP@cref}{{[algorithm][3][4,15]4.15.3}{[1][112][]112}}
\@writefile{toc}{\contentsline {subsubsection}{Frank-Wolfe method}{113}{section*.93}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4.15.4}{\ignorespaces Pseudocode for Frank-Wolf method for non linear functions.\relax }}{113}{algorithm.4.15.4}\protected@file@percent }
\newlabel{alg:FWM}{{4.15.4}{113}{Pseudocode for Frank-Wolf method for non linear functions.\relax }{algorithm.4.15.4}{}}
\newlabel{alg:FWM@cref}{{[algorithm][4][4,15]4.15.4}{[1][113][]113}}
\@writefile{toc}{\contentsline {section}{\numberline {4.16}14th of December 2018 --- A. Frangioni}{114}{section.4.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.16.1}Dual methods for linear constrained optimization}{114}{subsection.4.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Separable problems and partial dual}{114}{section*.94}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.16.2}Primal/dual methods or barrier methods}{115}{subsection.4.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Barrier function and central path}{115}{section*.95}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.48}{\ignorespaces The trajectory converges to the optimal solution of the problem, when $\mu \rightarrow 0$.\relax }}{116}{figure.caption.96}\protected@file@percent }
\newlabel{fig:14dic1}{{4.48}{116}{The trajectory converges to the optimal solution of the problem, when $\mu \to 0$.\relax }{figure.caption.96}{}}
\newlabel{fig:14dic1@cref}{{[figure][48][4]4.48}{[1][115][]116}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.49}{\ignorespaces The dotted line represents a region where the Newton method is very efficient. We are starting from a point $x_1$, which belongs to that region and we want to move towards $x_{\mu ^1}$. At next iterate $x_2$ is closer to $x_{\mu ^2}$ than the current iterate. \relax }}{116}{figure.caption.97}\protected@file@percent }
\newlabel{fig:14dic2}{{4.49}{116}{The dotted line represents a region where the Newton method is very efficient. We are starting from a point $x_1$, which belongs to that region and we want to move towards $x_{\mu ^1}$. At next iterate $x_2$ is closer to $x_{\mu ^2}$ than the current iterate. \relax }{figure.caption.97}{}}
\newlabel{fig:14dic2@cref}{{[figure][49][4]4.49}{[1][115][]116}}
\newlabel{eq:14dic1}{{4.16.1}{117}{Barrier function and central path}{equation.4.16.1}{}}
\newlabel{eq:14dic1@cref}{{[equation][1][4,16]4.16.1}{[1][117][]117}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.16.3}Primal-dual interior point method}{117}{subsection.4.16.3}\protected@file@percent }
\newlabel{eq:14dic2}{{4.16.2}{118}{Primal-dual interior point method}{equation.4.16.2}{}}
\newlabel{eq:14dic2@cref}{{[equation][2][4,16]4.16.2}{[1][117][]118}}
\newlabel{eq:14dic3}{{4.16.3}{118}{Primal-dual interior point method}{equation.4.16.3}{}}
\newlabel{eq:14dic3@cref}{{[equation][3][4,16]4.16.3}{[1][118][]118}}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  D35715DF6373C968BFDEB90BC99B17BDE2604F6C7487FF7DA7EA82CF26AA5CE8.pygtex}
\@writefile{toc}{\tof@finish}
